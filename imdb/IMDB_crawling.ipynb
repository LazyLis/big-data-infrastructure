{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1684408552142,
     "user": {
      "displayName": "Peter Shevchenko",
      "userId": "05864957351618494373"
     },
     "user_tz": -180
    },
    "id": "r-iOqYalKXd_"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = 'https://imdb-api.tprojects.workers.dev'\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0'}\n",
    "IMDB_TITLE_PATTERN = r\"/title/(tt\\d+)/\"\n",
    "SAVE_DIRECTORY = Path(r\"C:\\Users\\petro\\Desktop\\IMDB_parsing\")\n",
    "SAVE_FILMS_JSON = SAVE_DIRECTORY.joinpath('films_data_2.json')\n",
    "SAVE_REVIEWS_JSON = SAVE_DIRECTORY.joinpath('reviews_data_2.json')\n",
    "SAVE_FILMS_CSV = SAVE_DIRECTORY.joinpath('films_data_2.csv')\n",
    "SAVE_REVIEWS_CSV = SAVE_DIRECTORY.joinpath('reviews_data_2.csv')\n",
    "PAGES_RANGE = range(6, 16)  # range(1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 779
    },
    "executionInfo": {
     "elapsed": 301773,
     "status": "error",
     "timestamp": 1684409111293,
     "user": {
      "displayName": "Peter Shevchenko",
      "userId": "05864957351618494373"
     },
     "user_tz": -180
    },
    "id": "xAWyW4LoJdRh",
    "outputId": "82b11e3a-f642-4093-e9cd-c8413d78a538",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pages:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "films on page 6:   0%|          | 0/250 [00:00<?, ?it/s]\u001B[A\n",
      "films on page 6:   0%|          | 1/250 [00:16<1:07:18, 16.22s/it]\u001B[A\n",
      "films on page 6:   1%|          | 2/250 [00:28<58:26, 14.14s/it]  \u001B[A\n",
      "films on page 6:   1%|          | 3/250 [00:46<1:05:15, 15.85s/it]\u001B[A\n",
      "films on page 6:   2%|▏         | 4/250 [01:01<1:02:46, 15.31s/it]\u001B[A\n",
      "films on page 6:   2%|▏         | 5/250 [01:13<58:23, 14.30s/it]  \u001B[A\n",
      "films on page 6:   2%|▏         | 6/250 [01:26<55:45, 13.71s/it]\u001B[A\n",
      "films on page 6:   3%|▎         | 7/250 [01:43<59:58, 14.81s/it]\u001B[A\n",
      "films on page 6:   3%|▎         | 8/250 [01:58<1:00:04, 14.89s/it]\u001B[A\n",
      "films on page 6:   4%|▎         | 9/250 [02:18<1:06:49, 16.64s/it]\u001B[A\n",
      "films on page 6:   4%|▍         | 10/250 [02:38<1:09:50, 17.46s/it]\u001B[A\n",
      "films on page 6:   4%|▍         | 11/250 [02:54<1:08:29, 17.20s/it]\u001B[A\n",
      "films on page 6:   5%|▍         | 12/250 [03:13<1:10:32, 17.79s/it]\u001B[A\n",
      "films on page 6:   5%|▌         | 13/250 [03:30<1:09:01, 17.47s/it]\u001B[A\n",
      "films on page 6:   6%|▌         | 14/250 [03:46<1:06:22, 16.87s/it]\u001B[A\n",
      "films on page 6:   6%|▌         | 15/250 [04:01<1:04:27, 16.46s/it]\u001B[A\n",
      "films on page 6:   6%|▋         | 16/250 [04:17<1:03:31, 16.29s/it]\u001B[A\n",
      "films on page 6:   7%|▋         | 17/250 [04:30<58:41, 15.11s/it]  \u001B[A\n",
      "films on page 6:   7%|▋         | 18/250 [04:48<1:02:42, 16.22s/it]\u001B[A\n",
      "films on page 6:   8%|▊         | 19/250 [05:04<1:01:17, 15.92s/it]\u001B[A\n",
      "films on page 6:   8%|▊         | 20/250 [05:23<1:04:51, 16.92s/it]\u001B[A\n",
      "films on page 6:   8%|▊         | 21/250 [05:40<1:04:22, 16.87s/it]\u001B[A\n",
      "films on page 6:   9%|▉         | 22/250 [05:54<1:00:58, 16.05s/it]\u001B[A\n",
      "films on page 6:   9%|▉         | 23/250 [06:11<1:02:22, 16.49s/it]\u001B[A\n",
      "films on page 6:  10%|▉         | 24/250 [06:27<1:01:18, 16.28s/it]\u001B[A\n",
      "films on page 6:  10%|█         | 25/250 [06:46<1:03:50, 17.02s/it]\u001B[A\n",
      "films on page 6:  10%|█         | 26/250 [06:59<59:04, 15.82s/it]  \u001B[A\n",
      "films on page 6:  11%|█         | 27/250 [07:16<59:58, 16.14s/it]\u001B[A\n",
      "films on page 6:  11%|█         | 28/250 [07:30<57:16, 15.48s/it]\u001B[A\n",
      "films on page 6:  12%|█▏        | 29/250 [07:45<57:16, 15.55s/it]\u001B[A\n",
      "films on page 6:  12%|█▏        | 30/250 [08:04<1:00:12, 16.42s/it]\u001B[A\n",
      "films on page 6:  12%|█▏        | 31/250 [08:19<59:08, 16.20s/it]  \u001B[A\n",
      "films on page 6:  13%|█▎        | 32/250 [08:37<1:00:33, 16.67s/it]\u001B[A\n",
      "films on page 6:  13%|█▎        | 33/250 [08:52<58:29, 16.17s/it]  \u001B[A\n",
      "films on page 6:  14%|█▎        | 34/250 [09:13<1:02:49, 17.45s/it]\u001B[A\n",
      "films on page 6:  14%|█▍        | 35/250 [09:27<58:54, 16.44s/it]  \u001B[A\n",
      "films on page 6:  14%|█▍        | 36/250 [09:44<59:54, 16.79s/it]\u001B[A\n",
      "films on page 6:  15%|█▍        | 37/250 [09:59<57:21, 16.16s/it]\u001B[A\n",
      "films on page 6:  15%|█▌        | 38/250 [10:16<58:16, 16.49s/it]\u001B[A\n",
      "films on page 6:  16%|█▌        | 39/250 [10:34<59:36, 16.95s/it]\u001B[A\n",
      "films on page 6:  16%|█▌        | 40/250 [10:49<57:18, 16.38s/it]\u001B[A\n",
      "films on page 6:  16%|█▋        | 41/250 [11:03<54:25, 15.63s/it]\u001B[A\n",
      "films on page 6:  17%|█▋        | 42/250 [11:21<56:47, 16.38s/it]\u001B[A\n",
      "films on page 6:  17%|█▋        | 43/250 [11:43<1:01:46, 17.90s/it]\u001B[A\n",
      "films on page 6:  18%|█▊        | 44/250 [11:57<57:43, 16.81s/it]  \u001B[A\n",
      "films on page 6:  18%|█▊        | 45/250 [12:14<57:18, 16.77s/it]\u001B[A\n",
      "films on page 6:  18%|█▊        | 46/250 [12:30<56:52, 16.73s/it]\u001B[A\n",
      "films on page 6:  19%|█▉        | 47/250 [12:48<57:15, 16.92s/it]\u001B[A\n",
      "films on page 6:  19%|█▉        | 48/250 [13:04<56:19, 16.73s/it]\u001B[A\n",
      "films on page 6:  20%|█▉        | 49/250 [13:22<57:26, 17.15s/it]\u001B[A\n",
      "films on page 6:  20%|██        | 50/250 [13:38<56:04, 16.82s/it]\u001B[A\n",
      "films on page 6:  20%|██        | 51/250 [13:53<53:56, 16.26s/it]\u001B[A\n",
      "films on page 6:  21%|██        | 52/250 [14:11<55:42, 16.88s/it]\u001B[A\n",
      "films on page 6:  21%|██        | 53/250 [14:27<53:55, 16.42s/it]\u001B[A\n",
      "films on page 6:  22%|██▏       | 54/250 [14:40<50:44, 15.54s/it]\u001B[A\n",
      "films on page 6:  22%|██▏       | 55/250 [14:58<52:21, 16.11s/it]\u001B[A\n",
      "films on page 6:  22%|██▏       | 56/250 [15:15<52:44, 16.31s/it]\u001B[A\n",
      "films on page 6:  23%|██▎       | 57/250 [15:32<53:24, 16.60s/it]\u001B[A\n",
      "films on page 6:  23%|██▎       | 58/250 [15:48<52:48, 16.50s/it]\u001B[A\n",
      "films on page 6:  24%|██▎       | 59/250 [16:06<54:06, 17.00s/it]\u001B[A\n",
      "films on page 6:  24%|██▍       | 60/250 [16:22<52:50, 16.69s/it]\u001B[A\n",
      "films on page 6:  24%|██▍       | 61/250 [16:39<53:06, 16.86s/it]\u001B[A\n",
      "films on page 6:  25%|██▍       | 62/250 [16:56<52:30, 16.76s/it]\u001B[A\n",
      "films on page 6:  25%|██▌       | 63/250 [17:15<53:58, 17.32s/it]\u001B[A\n",
      "films on page 6:  26%|██▌       | 64/250 [17:31<52:38, 16.98s/it]\u001B[A\n",
      "films on page 6:  26%|██▌       | 65/250 [17:46<50:27, 16.37s/it]\u001B[A\n",
      "films on page 6:  26%|██▋       | 66/250 [18:00<48:27, 15.80s/it]\u001B[A\n",
      "films on page 6:  27%|██▋       | 67/250 [18:21<52:25, 17.19s/it]\u001B[A\n",
      "films on page 6:  27%|██▋       | 68/250 [18:39<53:09, 17.52s/it]\u001B[A\n",
      "films on page 6:  28%|██▊       | 69/250 [18:56<51:59, 17.23s/it]\u001B[A\n",
      "films on page 6:  28%|██▊       | 70/250 [19:12<50:58, 16.99s/it]\u001B[A\n",
      "films on page 6:  28%|██▊       | 71/250 [19:26<48:30, 16.26s/it]\u001B[A\n",
      "films on page 6:  29%|██▉       | 72/250 [19:44<49:44, 16.77s/it]\u001B[A\n",
      "films on page 6:  29%|██▉       | 73/250 [20:01<48:56, 16.59s/it]\u001B[A\n",
      "films on page 6:  30%|██▉       | 74/250 [20:16<47:48, 16.30s/it]\u001B[A\n",
      "films on page 6:  30%|███       | 75/250 [20:30<45:03, 15.45s/it]\u001B[A\n",
      "films on page 6:  30%|███       | 76/250 [20:45<44:43, 15.42s/it]\u001B[A\n",
      "films on page 6:  31%|███       | 77/250 [21:02<46:03, 15.97s/it]\u001B[A\n",
      "films on page 6:  31%|███       | 78/250 [21:19<46:38, 16.27s/it]\u001B[A\n",
      "films on page 6:  32%|███▏      | 79/250 [21:36<46:43, 16.40s/it]\u001B[A\n",
      "films on page 6:  32%|███▏      | 80/250 [21:55<48:50, 17.24s/it]\u001B[A\n",
      "films on page 6:  32%|███▏      | 81/250 [22:12<48:24, 17.18s/it]\u001B[A\n",
      "films on page 6:  33%|███▎      | 82/250 [22:30<48:40, 17.38s/it]\u001B[A\n",
      "films on page 6:  33%|███▎      | 83/250 [22:47<48:02, 17.26s/it]\u001B[A\n",
      "films on page 6:  34%|███▎      | 84/250 [23:06<48:58, 17.70s/it]\u001B[A\n",
      "films on page 6:  34%|███▍      | 85/250 [23:27<51:26, 18.71s/it]\u001B[A\n",
      "films on page 6:  34%|███▍      | 86/250 [23:43<48:56, 17.90s/it]\u001B[A\n",
      "films on page 6:  35%|███▍      | 87/250 [24:01<48:35, 17.88s/it]\u001B[A\n",
      "films on page 6:  35%|███▌      | 88/250 [24:17<47:16, 17.51s/it]\u001B[A\n",
      "films on page 6:  36%|███▌      | 89/250 [24:34<46:08, 17.20s/it]\u001B[A\n",
      "films on page 6:  36%|███▌      | 90/250 [24:46<42:09, 15.81s/it]\u001B[A\n",
      "films on page 6:  36%|███▋      | 91/250 [25:05<44:26, 16.77s/it]\u001B[A\n",
      "films on page 6:  37%|███▋      | 92/250 [25:21<43:13, 16.42s/it]\u001B[A\n",
      "films on page 6:  37%|███▋      | 93/250 [25:36<42:10, 16.12s/it]\u001B[A\n",
      "films on page 6:  38%|███▊      | 94/250 [25:52<41:52, 16.10s/it]\u001B[A\n",
      "films on page 6:  38%|███▊      | 95/250 [26:10<42:35, 16.49s/it]\u001B[A\n",
      "films on page 6:  38%|███▊      | 96/250 [26:28<43:22, 16.90s/it]\u001B[A\n",
      "films on page 6:  39%|███▉      | 97/250 [26:44<42:21, 16.61s/it]\u001B[A\n",
      "films on page 6:  39%|███▉      | 98/250 [27:01<42:31, 16.78s/it]\u001B[A\n",
      "films on page 6:  40%|███▉      | 99/250 [27:19<43:30, 17.29s/it]\u001B[A\n",
      "films on page 6:  40%|████      | 100/250 [27:34<41:07, 16.45s/it]\u001B[A\n",
      "films on page 6:  40%|████      | 101/250 [27:47<38:16, 15.41s/it]\u001B[A\n",
      "films on page 6:  41%|████      | 102/250 [28:03<38:55, 15.78s/it]\u001B[A\n",
      "films on page 6:  41%|████      | 103/250 [28:20<39:16, 16.03s/it]\u001B[A\n",
      "films on page 6:  42%|████▏     | 104/250 [28:37<39:44, 16.33s/it]\u001B[A\n",
      "films on page 6:  42%|████▏     | 105/250 [28:54<40:08, 16.61s/it]\u001B[A\n",
      "films on page 6:  42%|████▏     | 106/250 [29:09<38:09, 15.90s/it]\u001B[A\n",
      "films on page 6:  43%|████▎     | 107/250 [29:26<38:39, 16.22s/it]\u001B[A\n",
      "films on page 6:  43%|████▎     | 108/250 [29:44<40:11, 16.99s/it]\u001B[A\n",
      "films on page 6:  44%|████▎     | 109/250 [30:00<38:58, 16.58s/it]\u001B[A\n",
      "films on page 6:  44%|████▍     | 110/250 [30:14<36:48, 15.77s/it]\u001B[A\n",
      "films on page 6:  44%|████▍     | 111/250 [30:31<37:25, 16.16s/it]\u001B[A\n",
      "films on page 6:  45%|████▍     | 112/250 [30:47<37:18, 16.22s/it]\u001B[A\n",
      "films on page 6:  45%|████▌     | 113/250 [31:05<37:51, 16.58s/it]\u001B[A\n",
      "films on page 6:  46%|████▌     | 114/250 [31:20<36:30, 16.11s/it]\u001B[A\n",
      "films on page 6:  46%|████▌     | 115/250 [31:36<36:15, 16.11s/it]\u001B[A\n",
      "films on page 6:  46%|████▋     | 116/250 [31:54<37:04, 16.60s/it]\u001B[A\n",
      "films on page 6:  47%|████▋     | 117/250 [32:12<37:56, 17.12s/it]\u001B[A\n",
      "films on page 6:  47%|████▋     | 118/250 [32:28<37:12, 16.91s/it]\u001B[A\n",
      "films on page 6:  48%|████▊     | 119/250 [32:48<38:54, 17.82s/it]\u001B[A\n",
      "films on page 6:  48%|████▊     | 120/250 [33:01<35:33, 16.41s/it]\u001B[A\n",
      "films on page 6:  48%|████▊     | 121/250 [33:20<36:44, 17.09s/it]\u001B[A\n",
      "films on page 6:  49%|████▉     | 122/250 [33:34<34:40, 16.25s/it]\u001B[A\n",
      "films on page 6:  49%|████▉     | 123/250 [33:49<33:11, 15.68s/it]\u001B[A\n",
      "films on page 6:  50%|████▉     | 124/250 [34:06<33:48, 16.10s/it]\u001B[A\n",
      "films on page 6:  50%|█████     | 125/250 [34:22<33:47, 16.22s/it]\u001B[A\n",
      "films on page 6:  50%|█████     | 126/250 [34:40<34:19, 16.61s/it]\u001B[A\n",
      "films on page 6:  51%|█████     | 127/250 [34:53<31:57, 15.59s/it]\u001B[A\n",
      "films on page 6:  51%|█████     | 128/250 [35:11<33:21, 16.41s/it]\u001B[A\n",
      "films on page 6:  52%|█████▏    | 129/250 [35:26<31:59, 15.87s/it]\u001B[A\n",
      "films on page 6:  52%|█████▏    | 130/250 [35:40<30:30, 15.25s/it]\u001B[A\n",
      "films on page 6:  52%|█████▏    | 131/250 [36:00<33:31, 16.90s/it]\u001B[A\n",
      "films on page 6:  53%|█████▎    | 132/250 [36:15<31:41, 16.11s/it]\u001B[A\n",
      "films on page 6:  53%|█████▎    | 133/250 [36:31<31:32, 16.17s/it]\u001B[A\n",
      "films on page 6:  54%|█████▎    | 134/250 [36:49<32:16, 16.69s/it]\u001B[A\n",
      "films on page 6:  54%|█████▍    | 135/250 [37:05<31:40, 16.53s/it]\u001B[A\n",
      "films on page 6:  54%|█████▍    | 136/250 [37:22<31:23, 16.52s/it]\u001B[A\n",
      "films on page 6:  55%|█████▍    | 137/250 [37:38<30:46, 16.34s/it]\u001B[A\n",
      "films on page 6:  55%|█████▌    | 138/250 [37:56<31:32, 16.90s/it]\u001B[A\n",
      "films on page 6:  56%|█████▌    | 139/250 [38:14<31:46, 17.17s/it]\u001B[A\n",
      "films on page 6:  56%|█████▌    | 140/250 [38:28<30:04, 16.41s/it]\u001B[A\n",
      "films on page 6:  56%|█████▋    | 141/250 [38:45<30:09, 16.60s/it]\u001B[A\n",
      "films on page 6:  57%|█████▋    | 142/250 [39:03<30:25, 16.91s/it]\u001B[A\n",
      "films on page 6:  57%|█████▋    | 143/250 [39:21<30:33, 17.14s/it]\u001B[A\n",
      "films on page 6:  58%|█████▊    | 144/250 [39:36<29:39, 16.79s/it]\u001B[A\n",
      "films on page 6:  58%|█████▊    | 145/250 [39:51<28:08, 16.08s/it]\u001B[A\n",
      "films on page 6:  58%|█████▊    | 146/250 [40:09<28:48, 16.62s/it]\u001B[A\n",
      "films on page 6:  59%|█████▉    | 147/250 [40:27<29:17, 17.06s/it]\u001B[A\n",
      "films on page 6:  59%|█████▉    | 148/250 [40:46<29:57, 17.62s/it]\u001B[A\n",
      "films on page 6:  60%|█████▉    | 149/250 [41:03<29:29, 17.52s/it]\u001B[A\n",
      "films on page 6:  60%|██████    | 150/250 [41:23<30:37, 18.38s/it]\u001B[A\n",
      "films on page 6:  60%|██████    | 151/250 [41:40<29:20, 17.78s/it]\u001B[A\n",
      "films on page 6:  61%|██████    | 152/250 [41:54<27:04, 16.58s/it]\u001B[A\n",
      "films on page 6:  61%|██████    | 153/250 [42:11<27:05, 16.76s/it]\u001B[A\n",
      "films on page 6:  62%|██████▏   | 154/250 [42:26<26:09, 16.35s/it]\u001B[A\n",
      "films on page 6:  62%|██████▏   | 155/250 [42:41<25:22, 16.03s/it]\u001B[A\n",
      "films on page 6:  62%|██████▏   | 156/250 [42:58<25:24, 16.22s/it]\u001B[A\n",
      "films on page 6:  63%|██████▎   | 157/250 [43:15<25:20, 16.35s/it]\u001B[A\n",
      "films on page 6:  63%|██████▎   | 158/250 [43:31<25:05, 16.36s/it]\u001B[A\n",
      "films on page 6:  64%|██████▎   | 159/250 [43:48<24:58, 16.47s/it]\u001B[A\n",
      "films on page 6:  64%|██████▍   | 160/250 [44:08<26:07, 17.42s/it]\u001B[A\n",
      "films on page 6:  64%|██████▍   | 161/250 [44:23<25:03, 16.90s/it]\u001B[A\n",
      "films on page 6:  65%|██████▍   | 162/250 [44:38<23:55, 16.31s/it]\u001B[A\n",
      "films on page 6:  65%|██████▌   | 163/250 [44:58<25:09, 17.35s/it]\u001B[A\n",
      "films on page 6:  66%|██████▌   | 164/250 [45:17<25:27, 17.76s/it]\u001B[A\n",
      "films on page 6:  66%|██████▌   | 165/250 [45:32<24:06, 17.02s/it]\u001B[A\n",
      "films on page 6:  66%|██████▋   | 166/250 [45:52<25:00, 17.86s/it]\u001B[A\n",
      "films on page 6:  67%|██████▋   | 167/250 [46:09<24:28, 17.69s/it]\u001B[A\n",
      "films on page 6:  67%|██████▋   | 168/250 [46:23<22:41, 16.60s/it]\u001B[A\n",
      "films on page 6:  68%|██████▊   | 169/250 [46:39<22:13, 16.47s/it]\u001B[A\n",
      "films on page 6:  68%|██████▊   | 170/250 [46:53<21:01, 15.77s/it]\u001B[A\n",
      "films on page 6:  68%|██████▊   | 171/250 [47:07<20:04, 15.25s/it]\u001B[A\n",
      "films on page 6:  69%|██████▉   | 172/250 [47:25<20:47, 16.00s/it]\u001B[A\n",
      "films on page 6:  69%|██████▉   | 173/250 [47:40<19:57, 15.55s/it]\u001B[A\n",
      "films on page 6:  70%|██████▉   | 174/250 [47:54<19:06, 15.09s/it]\u001B[A\n",
      "films on page 6:  70%|███████   | 175/250 [48:10<19:18, 15.45s/it]\u001B[A\n",
      "films on page 6:  70%|███████   | 176/250 [48:24<18:29, 14.99s/it]\u001B[A\n",
      "films on page 6:  71%|███████   | 177/250 [48:40<18:48, 15.46s/it]\u001B[A\n",
      "films on page 6:  71%|███████   | 178/250 [48:57<18:54, 15.76s/it]\u001B[A\n",
      "films on page 6:  72%|███████▏  | 179/250 [49:13<18:43, 15.83s/it]\u001B[A\n",
      "films on page 6:  72%|███████▏  | 180/250 [49:33<19:54, 17.06s/it]\u001B[A\n",
      "films on page 6:  72%|███████▏  | 181/250 [49:50<19:46, 17.19s/it]\u001B[A\n",
      "films on page 6:  73%|███████▎  | 182/250 [50:09<19:56, 17.60s/it]\u001B[A\n",
      "films on page 6:  73%|███████▎  | 183/250 [50:29<20:22, 18.25s/it]\u001B[A\n",
      "films on page 6:  74%|███████▎  | 184/250 [50:47<20:13, 18.39s/it]\u001B[A\n",
      "films on page 6:  74%|███████▍  | 185/250 [51:07<20:23, 18.83s/it]\u001B[A\n",
      "films on page 6:  74%|███████▍  | 186/250 [51:19<17:53, 16.77s/it]\u001B[A\n",
      "films on page 6:  75%|███████▍  | 187/250 [51:34<17:03, 16.25s/it]\u001B[A\n",
      "films on page 6:  75%|███████▌  | 188/250 [51:51<17:04, 16.53s/it]\u001B[A\n",
      "films on page 6:  76%|███████▌  | 189/250 [52:04<15:38, 15.38s/it]\u001B[A\n",
      "films on page 6:  76%|███████▌  | 190/250 [52:16<14:23, 14.39s/it]\u001B[A\n",
      "films on page 6:  76%|███████▋  | 191/250 [52:32<14:25, 14.67s/it]\u001B[A\n",
      "films on page 6:  77%|███████▋  | 192/250 [52:47<14:23, 14.89s/it]\u001B[A\n",
      "films on page 6:  77%|███████▋  | 193/250 [53:06<15:17, 16.10s/it]\u001B[A\n",
      "films on page 6:  78%|███████▊  | 194/250 [53:24<15:38, 16.75s/it]\u001B[A\n",
      "films on page 6:  78%|███████▊  | 195/250 [53:41<15:27, 16.87s/it]\u001B[A\n",
      "films on page 6:  78%|███████▊  | 196/250 [53:57<14:56, 16.61s/it]\u001B[A\n",
      "films on page 6:  79%|███████▉  | 197/250 [54:11<13:52, 15.71s/it]\u001B[A\n",
      "films on page 6:  79%|███████▉  | 198/250 [54:27<13:47, 15.91s/it]\u001B[A\n",
      "films on page 6:  80%|███████▉  | 199/250 [54:43<13:22, 15.73s/it]\u001B[A\n",
      "films on page 6:  80%|████████  | 200/250 [55:03<14:22, 17.24s/it]\u001B[A\n",
      "films on page 6:  80%|████████  | 201/250 [55:15<12:40, 15.52s/it]\u001B[A\n",
      "films on page 6:  81%|████████  | 202/250 [55:32<12:41, 15.86s/it]\u001B[A\n",
      "films on page 6:  81%|████████  | 203/250 [55:50<12:59, 16.59s/it]\u001B[A\n",
      "films on page 6:  82%|████████▏ | 204/250 [56:08<13:00, 16.98s/it]\u001B[A\n",
      "films on page 6:  82%|████████▏ | 205/250 [56:23<12:16, 16.37s/it]\u001B[A\n",
      "films on page 6:  82%|████████▏ | 206/250 [56:40<12:12, 16.66s/it]\u001B[A\n",
      "films on page 6:  83%|████████▎ | 207/250 [56:55<11:32, 16.11s/it]\u001B[A\n",
      "films on page 6:  83%|████████▎ | 208/250 [57:12<11:34, 16.54s/it]\u001B[A\n",
      "films on page 6:  84%|████████▎ | 209/250 [57:27<10:53, 15.94s/it]\u001B[A\n",
      "films on page 6:  84%|████████▍ | 210/250 [57:48<11:39, 17.49s/it]\u001B[A\n",
      "films on page 6:  84%|████████▍ | 211/250 [58:04<11:06, 17.08s/it]\u001B[A\n",
      "films on page 6:  85%|████████▍ | 212/250 [58:22<10:53, 17.19s/it]\u001B[A\n",
      "films on page 6:  85%|████████▌ | 213/250 [58:35<09:51, 15.99s/it]\u001B[A\n",
      "films on page 6:  86%|████████▌ | 214/250 [58:52<09:50, 16.39s/it]\u001B[A\n",
      "films on page 6:  86%|████████▌ | 215/250 [59:13<10:21, 17.75s/it]\u001B[A\n",
      "films on page 6:  86%|████████▋ | 216/250 [59:29<09:45, 17.21s/it]\u001B[A\n",
      "films on page 6:  87%|████████▋ | 217/250 [59:45<09:20, 17.00s/it]\u001B[A\n",
      "films on page 6:  87%|████████▋ | 218/250 [1:00:00<08:41, 16.31s/it]\u001B[A\n",
      "films on page 6:  88%|████████▊ | 219/250 [1:00:16<08:23, 16.25s/it]\u001B[A\n",
      "films on page 6:  88%|████████▊ | 220/250 [1:00:31<07:54, 15.82s/it]\u001B[A\n",
      "films on page 6:  88%|████████▊ | 221/250 [1:00:49<07:57, 16.45s/it]\u001B[A\n",
      "films on page 6:  89%|████████▉ | 222/250 [1:01:06<07:42, 16.52s/it]\u001B[A\n",
      "films on page 6:  89%|████████▉ | 223/250 [1:01:21<07:19, 16.26s/it]\u001B[A\n",
      "films on page 6:  90%|████████▉ | 224/250 [1:01:34<06:33, 15.13s/it]\u001B[A\n",
      "films on page 6:  90%|█████████ | 225/250 [1:01:52<06:40, 16.01s/it]\u001B[A\n",
      "films on page 6:  90%|█████████ | 226/250 [1:02:10<06:37, 16.57s/it]\u001B[A\n",
      "films on page 6:  91%|█████████ | 227/250 [1:02:25<06:09, 16.09s/it]\u001B[A\n",
      "films on page 6:  91%|█████████ | 228/250 [1:02:43<06:10, 16.84s/it]\u001B[A\n",
      "films on page 6:  92%|█████████▏| 229/250 [1:02:59<05:47, 16.56s/it]\u001B[A\n",
      "films on page 6:  92%|█████████▏| 230/250 [1:03:13<05:11, 15.60s/it]\u001B[A\n",
      "films on page 6:  92%|█████████▏| 231/250 [1:03:28<04:55, 15.56s/it]\u001B[A\n",
      "films on page 6:  93%|█████████▎| 232/250 [1:03:43<04:36, 15.35s/it]\u001B[A\n",
      "films on page 6:  93%|█████████▎| 233/250 [1:03:59<04:24, 15.55s/it]\u001B[A\n",
      "films on page 6:  94%|█████████▎| 234/250 [1:04:14<04:06, 15.41s/it]\u001B[A\n",
      "films on page 6:  94%|█████████▍| 235/250 [1:04:29<03:48, 15.23s/it]\u001B[A\n",
      "films on page 6:  94%|█████████▍| 236/250 [1:04:44<03:33, 15.27s/it]\u001B[A\n",
      "films on page 6:  95%|█████████▍| 237/250 [1:05:00<03:19, 15.34s/it]\u001B[A\n",
      "films on page 6:  95%|█████████▌| 238/250 [1:05:13<02:54, 14.58s/it]\u001B[A\n",
      "films on page 6:  96%|█████████▌| 239/250 [1:05:26<02:36, 14.23s/it]\u001B[A\n",
      "films on page 6:  96%|█████████▌| 240/250 [1:05:46<02:39, 15.96s/it]\u001B[A\n",
      "films on page 6:  96%|█████████▋| 241/250 [1:06:02<02:25, 16.13s/it]\u001B[A\n",
      "films on page 6:  97%|█████████▋| 242/250 [1:06:19<02:09, 16.15s/it]\u001B[A\n",
      "films on page 6:  97%|█████████▋| 243/250 [1:06:34<01:50, 15.77s/it]\u001B[A\n",
      "films on page 6:  98%|█████████▊| 244/250 [1:06:47<01:30, 15.08s/it]\u001B[A\n",
      "films on page 6:  98%|█████████▊| 245/250 [1:07:04<01:18, 15.68s/it]\u001B[A\n",
      "films on page 6:  98%|█████████▊| 246/250 [1:07:24<01:07, 16.87s/it]\u001B[A\n",
      "films on page 6:  99%|█████████▉| 247/250 [1:07:38<00:48, 16.05s/it]\u001B[A\n",
      "films on page 6:  99%|█████████▉| 248/250 [1:07:53<00:31, 15.86s/it]\u001B[A\n",
      "films on page 6: 100%|█████████▉| 249/250 [1:08:08<00:15, 15.57s/it]\u001B[A\n",
      "films on page 6: 100%|██████████| 250/250 [1:08:22<00:00, 16.41s/it]\u001B[A\n",
      "pages:  10%|█         | 1/10 [1:08:26<10:15:55, 4106.21s/it]\n",
      "films on page 7:   0%|          | 0/250 [00:00<?, ?it/s]\u001B[A\n",
      "films on page 7:   0%|          | 1/250 [00:15<1:04:00, 15.42s/it]\u001B[A\n",
      "films on page 7:   1%|          | 2/250 [00:29<1:00:20, 14.60s/it]\u001B[A\n",
      "films on page 7:   1%|          | 3/250 [00:44<1:00:17, 14.65s/it]\u001B[A\n",
      "films on page 7:   2%|▏         | 4/250 [01:01<1:03:40, 15.53s/it]\u001B[A\n",
      "films on page 7:   2%|▏         | 5/250 [01:16<1:03:37, 15.58s/it]\u001B[A\n",
      "films on page 7:   2%|▏         | 6/250 [01:33<1:04:55, 15.97s/it]\u001B[A\n",
      "films on page 7:   3%|▎         | 7/250 [01:47<1:02:05, 15.33s/it]\u001B[A\n",
      "films on page 7:   3%|▎         | 8/250 [02:04<1:04:06, 15.89s/it]\u001B[A\n",
      "films on page 7:   4%|▎         | 9/250 [02:17<1:00:15, 15.00s/it]\u001B[A\n",
      "films on page 7:   4%|▍         | 10/250 [02:30<57:56, 14.49s/it] \u001B[A\n",
      "films on page 7:   4%|▍         | 11/250 [02:45<58:09, 14.60s/it]\u001B[A\n",
      "films on page 7:   5%|▍         | 12/250 [03:03<1:02:10, 15.67s/it]\u001B[A\n",
      "films on page 7:   5%|▌         | 13/250 [03:17<58:52, 14.91s/it]  \u001B[A\n",
      "films on page 7:   6%|▌         | 14/250 [03:33<1:00:52, 15.48s/it]\u001B[A\n",
      "films on page 7:   6%|▌         | 15/250 [03:52<1:04:55, 16.58s/it]\u001B[A\n",
      "films on page 7:   6%|▋         | 16/250 [04:12<1:07:38, 17.34s/it]\u001B[A\n",
      "films on page 7:   7%|▋         | 17/250 [04:28<1:06:44, 17.19s/it]\u001B[A\n",
      "films on page 7:   7%|▋         | 18/250 [04:47<1:07:40, 17.50s/it]\u001B[A\n",
      "films on page 7:   8%|▊         | 19/250 [05:02<1:05:16, 16.95s/it]\u001B[A\n",
      "films on page 7:   8%|▊         | 20/250 [05:17<1:01:55, 16.15s/it]\u001B[A\n",
      "films on page 7:   8%|▊         | 21/250 [05:35<1:04:22, 16.87s/it]\u001B[A\n",
      "films on page 7:   9%|▉         | 22/250 [05:52<1:03:33, 16.73s/it]\u001B[A\n",
      "films on page 7:   9%|▉         | 23/250 [06:04<58:52, 15.56s/it]  \u001B[A\n",
      "films on page 7:  10%|▉         | 24/250 [06:24<1:03:28, 16.85s/it]\u001B[A\n",
      "films on page 7:  10%|█         | 25/250 [06:39<1:00:43, 16.19s/it]\u001B[A\n",
      "films on page 7:  10%|█         | 26/250 [06:54<58:49, 15.76s/it]  \u001B[A\n",
      "films on page 7:  11%|█         | 27/250 [07:07<56:07, 15.10s/it]\u001B[A\n",
      "films on page 7:  11%|█         | 28/250 [07:24<57:32, 15.55s/it]\u001B[A\n",
      "films on page 7:  12%|█▏        | 29/250 [07:39<56:24, 15.31s/it]\u001B[A\n",
      "films on page 7:  12%|█▏        | 30/250 [07:53<54:50, 14.96s/it]\u001B[A\n",
      "films on page 7:  12%|█▏        | 31/250 [08:10<57:05, 15.64s/it]\u001B[A\n",
      "films on page 7:  13%|█▎        | 32/250 [08:28<59:12, 16.30s/it]\u001B[A\n",
      "films on page 7:  13%|█▎        | 33/250 [08:43<58:17, 16.12s/it]\u001B[A\n",
      "films on page 7:  14%|█▎        | 34/250 [08:57<55:09, 15.32s/it]\u001B[A\n",
      "films on page 7:  14%|█▍        | 35/250 [09:10<52:12, 14.57s/it]\u001B[A\n",
      "films on page 7:  14%|█▍        | 36/250 [09:26<53:18, 14.95s/it]\u001B[A\n",
      "films on page 7:  15%|█▍        | 37/250 [09:41<53:20, 15.02s/it]\u001B[A\n",
      "films on page 7:  15%|█▌        | 38/250 [09:55<52:42, 14.92s/it]\u001B[A\n",
      "films on page 7:  16%|█▌        | 39/250 [10:14<56:21, 16.03s/it]\u001B[A\n",
      "films on page 7:  16%|█▌        | 40/250 [10:28<54:04, 15.45s/it]\u001B[A\n",
      "films on page 7:  16%|█▋        | 41/250 [10:43<52:52, 15.18s/it]\u001B[A\n",
      "films on page 7:  17%|█▋        | 42/250 [10:57<52:01, 15.01s/it]\u001B[A\n",
      "films on page 7:  17%|█▋        | 43/250 [11:15<54:26, 15.78s/it]\u001B[A\n",
      "films on page 7:  18%|█▊        | 44/250 [11:28<51:40, 15.05s/it]\u001B[A\n",
      "films on page 7:  18%|█▊        | 45/250 [11:43<50:37, 14.82s/it]\u001B[A\n",
      "films on page 7:  18%|█▊        | 46/250 [11:58<51:15, 15.07s/it]\u001B[A\n",
      "films on page 7:  19%|█▉        | 47/250 [12:14<51:35, 15.25s/it]\u001B[A\n",
      "films on page 7:  19%|█▉        | 48/250 [12:31<53:10, 15.79s/it]\u001B[A\n",
      "films on page 7:  20%|█▉        | 49/250 [12:47<53:16, 15.90s/it]\u001B[A\n",
      "films on page 7:  20%|██        | 50/250 [13:06<55:34, 16.67s/it]\u001B[A\n",
      "films on page 7:  20%|██        | 51/250 [13:20<53:21, 16.09s/it]\u001B[A\n",
      "films on page 7:  21%|██        | 52/250 [13:34<51:12, 15.52s/it]\u001B[A\n",
      "films on page 7:  21%|██        | 53/250 [13:48<48:38, 14.81s/it]\u001B[A\n",
      "films on page 7:  22%|██▏       | 54/250 [14:05<50:38, 15.50s/it]\u001B[A\n",
      "films on page 7:  22%|██▏       | 55/250 [14:21<51:00, 15.70s/it]\u001B[A\n",
      "films on page 7:  22%|██▏       | 56/250 [14:35<49:14, 15.23s/it]\u001B[A\n",
      "films on page 7:  23%|██▎       | 57/250 [14:49<48:09, 14.97s/it]\u001B[A\n",
      "films on page 7:  23%|██▎       | 58/250 [15:05<48:11, 15.06s/it]\u001B[A\n",
      "films on page 7:  24%|██▎       | 59/250 [15:19<46:51, 14.72s/it]\u001B[A\n",
      "films on page 7:  24%|██▍       | 60/250 [15:33<46:31, 14.69s/it]\u001B[A\n",
      "films on page 7:  24%|██▍       | 61/250 [15:47<45:26, 14.43s/it]\u001B[A\n",
      "films on page 7:  25%|██▍       | 62/250 [16:02<45:45, 14.60s/it]\u001B[A\n",
      "films on page 7:  25%|██▌       | 63/250 [16:18<47:13, 15.15s/it]\u001B[A\n",
      "films on page 7:  26%|██▌       | 64/250 [16:37<50:04, 16.15s/it]\u001B[A\n",
      "films on page 7:  26%|██▌       | 65/250 [16:57<53:27, 17.34s/it]\u001B[A\n",
      "films on page 7:  26%|██▋       | 66/250 [17:10<49:29, 16.14s/it]\u001B[A\n",
      "films on page 7:  27%|██▋       | 67/250 [17:24<46:37, 15.29s/it]\u001B[A\n",
      "films on page 7:  27%|██▋       | 68/250 [17:41<48:15, 15.91s/it]\u001B[A\n",
      "films on page 7:  28%|██▊       | 69/250 [17:57<48:20, 16.03s/it]\u001B[A\n",
      "films on page 7:  28%|██▊       | 70/250 [18:14<49:04, 16.36s/it]\u001B[A\n",
      "films on page 7:  28%|██▊       | 71/250 [18:29<47:08, 15.80s/it]\u001B[A\n",
      "films on page 7:  29%|██▉       | 72/250 [18:43<45:21, 15.29s/it]\u001B[A\n",
      "films on page 7:  29%|██▉       | 73/250 [18:57<44:19, 15.02s/it]\u001B[A\n",
      "films on page 7:  30%|██▉       | 74/250 [19:17<47:58, 16.36s/it]\u001B[A\n",
      "films on page 7:  30%|███       | 75/250 [19:33<47:30, 16.29s/it]\u001B[A\n",
      "films on page 7:  30%|███       | 76/250 [19:46<44:28, 15.34s/it]\u001B[A\n",
      "films on page 7:  31%|███       | 77/250 [20:05<47:08, 16.35s/it]\u001B[A\n",
      "films on page 7:  31%|███       | 78/250 [20:18<43:56, 15.33s/it]\u001B[A\n",
      "films on page 7:  32%|███▏      | 79/250 [20:35<45:34, 15.99s/it]\u001B[A\n",
      "films on page 7:  32%|███▏      | 80/250 [20:50<44:06, 15.57s/it]\u001B[A\n",
      "films on page 7:  32%|███▏      | 81/250 [21:09<47:08, 16.74s/it]\u001B[A\n",
      "films on page 7:  33%|███▎      | 82/250 [21:26<46:45, 16.70s/it]\u001B[A\n",
      "films on page 7:  33%|███▎      | 83/250 [21:45<48:10, 17.31s/it]\u001B[A\n",
      "films on page 7:  34%|███▎      | 84/250 [21:59<45:00, 16.27s/it]\u001B[A\n",
      "films on page 7:  34%|███▍      | 85/250 [22:13<43:29, 15.82s/it]\u001B[A\n",
      "films on page 7:  34%|███▍      | 86/250 [22:29<42:50, 15.68s/it]\u001B[A\n",
      "films on page 7:  35%|███▍      | 87/250 [22:45<42:49, 15.77s/it]\u001B[A\n",
      "films on page 7:  35%|███▌      | 88/250 [23:02<43:57, 16.28s/it]\u001B[A\n",
      "films on page 7:  36%|███▌      | 89/250 [23:18<43:19, 16.15s/it]\u001B[A\n",
      "films on page 7:  36%|███▌      | 90/250 [23:36<44:17, 16.61s/it]\u001B[A\n",
      "films on page 7:  36%|███▋      | 91/250 [23:47<40:06, 15.14s/it]\u001B[A\n",
      "films on page 7:  37%|███▋      | 92/250 [24:03<40:25, 15.35s/it]\u001B[A\n",
      "films on page 7:  37%|███▋      | 93/250 [24:20<41:04, 15.70s/it]\u001B[A\n",
      "films on page 7:  38%|███▊      | 94/250 [24:34<40:04, 15.41s/it]\u001B[A\n",
      "films on page 7:  38%|███▊      | 95/250 [24:52<41:10, 15.94s/it]\u001B[A\n",
      "films on page 7:  38%|███▊      | 96/250 [25:08<41:26, 16.15s/it]\u001B[A\n",
      "films on page 7:  39%|███▉      | 97/250 [25:25<41:37, 16.32s/it]\u001B[A\n",
      "films on page 7:  39%|███▉      | 98/250 [25:42<41:36, 16.42s/it]\u001B[A\n",
      "films on page 7:  40%|███▉      | 99/250 [25:59<41:41, 16.57s/it]\u001B[A\n",
      "films on page 7:  40%|████      | 100/250 [26:15<41:28, 16.59s/it]\u001B[A\n",
      "films on page 7:  40%|████      | 101/250 [26:31<40:23, 16.27s/it]\u001B[A\n",
      "films on page 7:  41%|████      | 102/250 [26:45<38:50, 15.74s/it]\u001B[A\n",
      "films on page 7:  41%|████      | 103/250 [27:01<38:51, 15.86s/it]\u001B[A\n",
      "films on page 7:  42%|████▏     | 104/250 [27:18<39:11, 16.11s/it]\u001B[A\n",
      "films on page 7:  42%|████▏     | 105/250 [27:32<37:37, 15.57s/it]\u001B[A\n",
      "films on page 7:  42%|████▏     | 106/250 [27:47<36:20, 15.14s/it]\u001B[A\n",
      "films on page 7:  43%|████▎     | 107/250 [28:07<39:31, 16.58s/it]\u001B[A\n",
      "films on page 7:  43%|████▎     | 108/250 [28:20<36:50, 15.57s/it]\u001B[A\n",
      "films on page 7:  44%|████▎     | 109/250 [28:35<36:19, 15.46s/it]\u001B[A\n",
      "films on page 7:  44%|████▍     | 110/250 [28:52<37:10, 15.93s/it]\u001B[A\n",
      "films on page 7:  44%|████▍     | 111/250 [29:06<35:37, 15.38s/it]\u001B[A\n",
      "films on page 7:  45%|████▍     | 112/250 [29:24<37:11, 16.17s/it]\u001B[A\n",
      "films on page 7:  45%|████▌     | 113/250 [29:39<35:47, 15.68s/it]\u001B[A\n",
      "films on page 7:  46%|████▌     | 114/250 [29:54<35:40, 15.74s/it]\u001B[A\n",
      "films on page 7:  46%|████▌     | 115/250 [30:11<36:06, 16.05s/it]\u001B[A\n",
      "films on page 7:  46%|████▋     | 116/250 [30:26<35:08, 15.74s/it]\u001B[A\n",
      "films on page 7:  47%|████▋     | 117/250 [30:44<35:57, 16.22s/it]\u001B[A\n",
      "films on page 7:  47%|████▋     | 118/250 [30:59<34:55, 15.88s/it]\u001B[A\n",
      "films on page 7:  48%|████▊     | 119/250 [31:16<35:50, 16.42s/it]\u001B[A\n",
      "films on page 7:  48%|████▊     | 120/250 [31:32<34:46, 16.05s/it]\u001B[A\n",
      "films on page 7:  48%|████▊     | 121/250 [31:45<32:37, 15.17s/it]\u001B[A\n",
      "films on page 7:  49%|████▉     | 122/250 [32:00<32:32, 15.26s/it]\u001B[A\n",
      "films on page 7:  49%|████▉     | 123/250 [32:19<34:30, 16.30s/it]\u001B[A\n",
      "films on page 7:  50%|████▉     | 124/250 [32:33<32:51, 15.64s/it]\u001B[A\n",
      "films on page 7:  50%|█████     | 125/250 [32:50<33:45, 16.20s/it]\u001B[A\n",
      "films on page 7:  50%|█████     | 126/250 [33:07<33:51, 16.38s/it]\u001B[A\n",
      "films on page 7:  51%|█████     | 127/250 [33:24<33:39, 16.42s/it]\u001B[A\n",
      "films on page 7:  51%|█████     | 128/250 [33:40<33:01, 16.24s/it]\u001B[A\n",
      "films on page 7:  52%|█████▏    | 129/250 [33:56<32:47, 16.26s/it]\u001B[A\n",
      "films on page 7:  52%|█████▏    | 130/250 [34:14<33:37, 16.81s/it]\u001B[A\n",
      "films on page 7:  52%|█████▏    | 131/250 [34:33<34:36, 17.45s/it]\u001B[A\n",
      "films on page 7:  53%|█████▎    | 132/250 [34:49<33:47, 17.18s/it]\u001B[A\n",
      "films on page 7:  53%|█████▎    | 133/250 [35:03<31:24, 16.11s/it]\u001B[A\n",
      "films on page 7:  54%|█████▎    | 134/250 [35:23<33:17, 17.22s/it]\u001B[A\n",
      "films on page 7:  54%|█████▍    | 135/250 [35:38<31:58, 16.68s/it]\u001B[A\n",
      "films on page 7:  54%|█████▍    | 136/250 [35:54<31:13, 16.44s/it]\u001B[A\n",
      "films on page 7:  55%|█████▍    | 137/250 [36:07<29:07, 15.47s/it]\u001B[A\n",
      "films on page 7:  55%|█████▌    | 138/250 [36:22<28:19, 15.17s/it]\u001B[A\n",
      "films on page 7:  56%|█████▌    | 139/250 [36:41<30:00, 16.22s/it]\u001B[A\n",
      "films on page 7:  56%|█████▌    | 140/250 [37:00<31:22, 17.12s/it]\u001B[A\n",
      "films on page 7:  56%|█████▋    | 141/250 [37:16<30:39, 16.87s/it]\u001B[A\n",
      "films on page 7:  57%|█████▋    | 142/250 [37:31<29:04, 16.15s/it]\u001B[A\n",
      "films on page 7:  57%|█████▋    | 143/250 [37:42<26:21, 14.78s/it]\u001B[A\n",
      "films on page 7:  58%|█████▊    | 144/250 [37:58<26:40, 15.10s/it]\u001B[A\n",
      "films on page 7:  58%|█████▊    | 145/250 [38:15<27:17, 15.59s/it]\u001B[A\n",
      "films on page 7:  58%|█████▊    | 146/250 [38:33<28:39, 16.54s/it]\u001B[A\n",
      "films on page 7:  59%|█████▉    | 147/250 [38:51<28:59, 16.89s/it]\u001B[A\n",
      "films on page 7:  59%|█████▉    | 148/250 [39:03<25:59, 15.28s/it]\u001B[A\n",
      "films on page 7:  60%|█████▉    | 149/250 [39:18<25:43, 15.29s/it]\u001B[A\n",
      "films on page 7:  60%|██████    | 150/250 [39:37<27:27, 16.47s/it]\u001B[A\n",
      "films on page 7:  60%|██████    | 151/250 [39:55<27:42, 16.79s/it]\u001B[A\n",
      "films on page 7:  61%|██████    | 152/250 [40:11<26:58, 16.51s/it]\u001B[A\n",
      "films on page 7:  61%|██████    | 153/250 [40:26<26:08, 16.17s/it]\u001B[A\n",
      "films on page 7:  62%|██████▏   | 154/250 [40:39<24:23, 15.24s/it]\u001B[A\n",
      "films on page 7:  62%|██████▏   | 155/250 [40:54<24:01, 15.18s/it]\u001B[A\n",
      "films on page 7:  62%|██████▏   | 156/250 [41:09<23:36, 15.07s/it]\u001B[A\n",
      "films on page 7:  63%|██████▎   | 157/250 [41:27<24:56, 16.09s/it]\u001B[A\n",
      "films on page 7:  63%|██████▎   | 158/250 [41:43<24:30, 15.98s/it]\u001B[A\n",
      "films on page 7:  64%|██████▎   | 159/250 [41:59<24:09, 15.93s/it]\u001B[A\n",
      "films on page 7:  64%|██████▍   | 160/250 [42:15<24:01, 16.02s/it]\u001B[A\n",
      "films on page 7:  64%|██████▍   | 161/250 [42:34<24:57, 16.82s/it]\u001B[A\n",
      "films on page 7:  65%|██████▍   | 162/250 [42:48<23:38, 16.12s/it]\u001B[A\n",
      "films on page 7:  65%|██████▌   | 163/250 [43:03<22:49, 15.74s/it]\u001B[A\n",
      "films on page 7:  66%|██████▌   | 164/250 [43:18<22:19, 15.57s/it]\u001B[A\n",
      "films on page 7:  66%|██████▌   | 165/250 [43:33<21:42, 15.33s/it]\u001B[A\n",
      "films on page 7:  66%|██████▋   | 166/250 [43:48<21:09, 15.12s/it]\u001B[A\n",
      "films on page 7:  67%|██████▋   | 167/250 [44:03<21:08, 15.28s/it]\u001B[A\n",
      "films on page 7:  67%|██████▋   | 168/250 [44:19<20:53, 15.28s/it]\u001B[A\n",
      "films on page 7:  68%|██████▊   | 169/250 [44:34<20:28, 15.17s/it]\u001B[A\n",
      "films on page 7:  68%|██████▊   | 170/250 [44:52<21:32, 16.15s/it]\u001B[A\n",
      "films on page 7:  68%|██████▊   | 171/250 [45:09<21:34, 16.39s/it]\u001B[A\n",
      "films on page 7:  69%|██████▉   | 172/250 [45:31<23:24, 18.00s/it]\u001B[A\n",
      "films on page 7:  69%|██████▉   | 173/250 [45:48<22:39, 17.66s/it]\u001B[A\n",
      "films on page 7:  70%|██████▉   | 174/250 [46:05<22:22, 17.66s/it]\u001B[A\n",
      "films on page 7:  70%|███████   | 175/250 [46:23<22:03, 17.65s/it]\u001B[A\n",
      "films on page 7:  70%|███████   | 176/250 [46:39<21:05, 17.10s/it]\u001B[A\n",
      "films on page 7:  71%|███████   | 177/250 [46:56<20:48, 17.10s/it]\u001B[A\n",
      "films on page 7:  71%|███████   | 178/250 [47:11<19:55, 16.60s/it]\u001B[A\n",
      "films on page 7:  72%|███████▏  | 179/250 [47:27<19:15, 16.27s/it]\u001B[A\n",
      "films on page 7:  72%|███████▏  | 180/250 [47:44<19:13, 16.48s/it]\u001B[A\n",
      "films on page 7:  72%|███████▏  | 181/250 [48:03<19:49, 17.24s/it]\u001B[A\n",
      "films on page 7:  73%|███████▎  | 182/250 [48:23<20:41, 18.26s/it]\u001B[A\n",
      "films on page 7:  73%|███████▎  | 183/250 [48:42<20:39, 18.50s/it]\u001B[A\n",
      "films on page 7:  74%|███████▎  | 184/250 [48:57<19:06, 17.37s/it]\u001B[A\n",
      "films on page 7:  74%|███████▍  | 185/250 [49:15<18:59, 17.53s/it]\u001B[A\n",
      "films on page 7:  74%|███████▍  | 186/250 [49:30<17:48, 16.70s/it]\u001B[A\n",
      "films on page 7:  75%|███████▍  | 187/250 [49:45<16:54, 16.11s/it]\u001B[A\n",
      "films on page 7:  75%|███████▌  | 188/250 [49:56<15:20, 14.85s/it]\u001B[A\n",
      "films on page 7:  76%|███████▌  | 189/250 [50:13<15:32, 15.29s/it]\u001B[A\n",
      "films on page 7:  76%|███████▌  | 190/250 [50:29<15:26, 15.44s/it]\u001B[A\n",
      "films on page 7:  76%|███████▋  | 191/250 [50:45<15:20, 15.61s/it]\u001B[A\n",
      "films on page 7:  77%|███████▋  | 192/250 [51:00<15:05, 15.62s/it]\u001B[A\n",
      "films on page 7:  77%|███████▋  | 193/250 [51:16<14:46, 15.56s/it]\u001B[A\n",
      "films on page 7:  78%|███████▊  | 194/250 [51:32<14:38, 15.69s/it]\u001B[A\n",
      "films on page 7:  78%|███████▊  | 195/250 [51:44<13:36, 14.84s/it]\u001B[A\n",
      "films on page 7:  78%|███████▊  | 196/250 [52:01<13:49, 15.35s/it]\u001B[A\n",
      "films on page 7:  79%|███████▉  | 197/250 [52:18<14:03, 15.91s/it]\u001B[A\n",
      "films on page 7:  79%|███████▉  | 198/250 [52:36<14:21, 16.56s/it]\u001B[A\n",
      "films on page 7:  80%|███████▉  | 199/250 [52:53<14:07, 16.62s/it]\u001B[A\n",
      "films on page 7:  80%|████████  | 200/250 [53:10<13:54, 16.69s/it]\u001B[A\n",
      "films on page 7:  80%|████████  | 201/250 [53:25<13:10, 16.13s/it]\u001B[A\n",
      "films on page 7:  81%|████████  | 202/250 [53:43<13:30, 16.88s/it]\u001B[A\n",
      "films on page 7:  81%|████████  | 203/250 [54:00<13:10, 16.81s/it]\u001B[A\n",
      "films on page 7:  82%|████████▏ | 204/250 [54:16<12:45, 16.64s/it]\u001B[A\n",
      "films on page 7:  82%|████████▏ | 205/250 [54:31<11:59, 15.98s/it]\u001B[A\n",
      "films on page 7:  82%|████████▏ | 206/250 [54:49<12:09, 16.58s/it]\u001B[A\n",
      "films on page 7:  83%|████████▎ | 207/250 [55:05<11:44, 16.38s/it]\u001B[A\n",
      "films on page 7:  83%|████████▎ | 208/250 [55:15<10:15, 14.66s/it]\u001B[A\n",
      "films on page 7:  84%|████████▎ | 209/250 [55:32<10:29, 15.35s/it]\u001B[A\n",
      "films on page 7:  84%|████████▍ | 210/250 [55:45<09:41, 14.54s/it]\u001B[A\n",
      "films on page 7:  84%|████████▍ | 211/250 [56:03<10:12, 15.70s/it]\u001B[A\n",
      "films on page 7:  85%|████████▍ | 212/250 [56:24<10:52, 17.18s/it]\u001B[A\n",
      "films on page 7:  85%|████████▌ | 213/250 [56:41<10:31, 17.06s/it]\u001B[A\n",
      "films on page 7:  86%|████████▌ | 214/250 [56:56<10:00, 16.67s/it]\u001B[A\n",
      "films on page 7:  86%|████████▌ | 215/250 [57:10<09:13, 15.81s/it]\u001B[A\n",
      "films on page 7:  86%|████████▋ | 216/250 [57:29<09:24, 16.60s/it]\u001B[A\n",
      "films on page 7:  87%|████████▋ | 217/250 [57:44<08:56, 16.26s/it]\u001B[A\n",
      "films on page 7:  87%|████████▋ | 218/250 [58:00<08:35, 16.10s/it]\u001B[A\n",
      "films on page 7:  88%|████████▊ | 219/250 [58:15<08:07, 15.71s/it]\u001B[A\n",
      "films on page 7:  88%|████████▊ | 220/250 [58:30<07:48, 15.62s/it]\u001B[A\n",
      "films on page 7:  88%|████████▊ | 221/250 [58:44<07:18, 15.13s/it]\u001B[A\n",
      "films on page 7:  89%|████████▉ | 222/250 [59:01<07:20, 15.75s/it]\u001B[A\n",
      "films on page 7:  89%|████████▉ | 223/250 [59:18<07:16, 16.16s/it]\u001B[A\n",
      "films on page 7:  90%|████████▉ | 224/250 [59:33<06:45, 15.61s/it]\u001B[A\n",
      "films on page 7:  90%|█████████ | 225/250 [59:51<06:47, 16.32s/it]\u001B[A\n",
      "films on page 7:  90%|█████████ | 226/250 [1:00:06<06:22, 15.94s/it]\u001B[A\n",
      "films on page 7:  91%|█████████ | 227/250 [1:00:22<06:06, 15.95s/it]\u001B[A\n",
      "films on page 7:  91%|█████████ | 228/250 [1:00:40<06:09, 16.80s/it]\u001B[A\n",
      "films on page 7:  92%|█████████▏| 229/250 [1:00:58<05:55, 16.92s/it]\u001B[A\n",
      "films on page 7:  92%|█████████▏| 230/250 [1:01:10<05:08, 15.43s/it]\u001B[A\n",
      "films on page 7:  92%|█████████▏| 231/250 [1:01:27<05:06, 16.16s/it]\u001B[A\n",
      "films on page 7:  93%|█████████▎| 232/250 [1:01:40<04:32, 15.14s/it]\u001B[A\n",
      "films on page 7:  93%|█████████▎| 233/250 [1:01:58<04:29, 15.85s/it]\u001B[A\n",
      "films on page 7:  94%|█████████▎| 234/250 [1:02:13<04:12, 15.76s/it]\u001B[A\n",
      "films on page 7:  94%|█████████▍| 235/250 [1:02:27<03:46, 15.09s/it]\u001B[A\n",
      "films on page 7:  94%|█████████▍| 236/250 [1:02:44<03:39, 15.69s/it]\u001B[A\n",
      "films on page 7:  95%|█████████▍| 237/250 [1:02:57<03:13, 14.90s/it]\u001B[A\n",
      "films on page 7:  95%|█████████▌| 238/250 [1:03:15<03:09, 15.80s/it]\u001B[A\n",
      "films on page 7:  96%|█████████▌| 239/250 [1:03:34<03:05, 16.90s/it]\u001B[A\n",
      "films on page 7:  96%|█████████▌| 240/250 [1:03:46<02:33, 15.35s/it]\u001B[A\n",
      "films on page 7:  96%|█████████▋| 241/250 [1:04:04<02:24, 16.01s/it]\u001B[A\n",
      "films on page 7:  97%|█████████▋| 242/250 [1:04:19<02:06, 15.82s/it]\u001B[A\n",
      "films on page 7:  97%|█████████▋| 243/250 [1:04:33<01:47, 15.37s/it]\u001B[A\n",
      "films on page 7:  98%|█████████▊| 244/250 [1:04:50<01:35, 15.88s/it]\u001B[A\n",
      "films on page 7:  98%|█████████▊| 245/250 [1:05:10<01:24, 16.85s/it]\u001B[A\n",
      "films on page 7:  98%|█████████▊| 246/250 [1:05:24<01:05, 16.29s/it]\u001B[A\n",
      "films on page 7:  99%|█████████▉| 247/250 [1:05:40<00:48, 16.03s/it]\u001B[A\n",
      "films on page 7:  99%|█████████▉| 248/250 [1:05:57<00:32, 16.40s/it]\u001B[A\n",
      "films on page 7: 100%|█████████▉| 249/250 [1:06:12<00:15, 16.00s/it]\u001B[A\n",
      "films on page 7: 100%|██████████| 250/250 [1:06:30<00:00, 15.96s/it]\u001B[A\n",
      "pages:  20%|██        | 2/10 [2:15:00<8:58:43, 4040.48s/it] \n",
      "films on page 8:   0%|          | 0/250 [00:00<?, ?it/s]\u001B[A\n",
      "films on page 8:   0%|          | 1/250 [00:15<1:03:59, 15.42s/it]\u001B[A\n",
      "films on page 8:   1%|          | 2/250 [00:33<1:11:16, 17.24s/it]\u001B[A\n",
      "films on page 8:   1%|          | 3/250 [00:54<1:17:18, 18.78s/it]\u001B[A\n",
      "films on page 8:   2%|▏         | 4/250 [01:12<1:15:11, 18.34s/it]\u001B[A\n",
      "films on page 8:   2%|▏         | 5/250 [01:25<1:06:46, 16.35s/it]\u001B[A\n",
      "films on page 8:   2%|▏         | 6/250 [01:40<1:05:54, 16.21s/it]\u001B[A\n",
      "films on page 8:   3%|▎         | 7/250 [01:57<1:06:22, 16.39s/it]\u001B[AC:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py:81: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(f'film {title_id}')\n",
      "WARNING:root:film tt11555492\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D1A9BD00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143D1A9BD00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D1A9BD00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D1A9BD00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 8:   3%|▎         | 8/250 [02:07<58:11, 14.43s/it]  \u001B[A\n",
      "films on page 8:   4%|▎         | 9/250 [02:20<55:48, 13.89s/it]\u001B[A\n",
      "films on page 8:   4%|▍         | 10/250 [02:35<57:08, 14.29s/it]\u001B[A\n",
      "films on page 8:   4%|▍         | 11/250 [02:50<57:32, 14.44s/it]\u001B[A\n",
      "films on page 8:   5%|▍         | 12/250 [03:07<1:00:14, 15.18s/it]\u001B[A\n",
      "films on page 8:   5%|▌         | 13/250 [03:23<1:00:41, 15.37s/it]\u001B[A\n",
      "films on page 8:   6%|▌         | 14/250 [03:40<1:03:10, 16.06s/it]\u001B[A\n",
      "films on page 8:   6%|▌         | 15/250 [03:56<1:02:11, 15.88s/it]\u001B[A\n",
      "films on page 8:   6%|▋         | 16/250 [04:13<1:02:57, 16.14s/it]\u001B[A\n",
      "films on page 8:   7%|▋         | 17/250 [04:27<1:00:06, 15.48s/it]\u001B[A\n",
      "films on page 8:   7%|▋         | 18/250 [04:45<1:02:57, 16.28s/it]\u001B[A\n",
      "films on page 8:   8%|▊         | 19/250 [05:02<1:03:30, 16.50s/it]\u001B[A\n",
      "films on page 8:   8%|▊         | 20/250 [05:16<1:00:57, 15.90s/it]\u001B[A\n",
      "films on page 8:   8%|▊         | 21/250 [05:34<1:02:59, 16.50s/it]\u001B[A\n",
      "films on page 8:   9%|▉         | 22/250 [05:52<1:04:21, 16.94s/it]\u001B[A\n",
      "films on page 8:   9%|▉         | 23/250 [06:07<1:01:59, 16.38s/it]\u001B[A\n",
      "films on page 8:  10%|▉         | 24/250 [06:23<1:00:46, 16.14s/it]\u001B[A\n",
      "films on page 8:  10%|█         | 25/250 [06:40<1:01:31, 16.41s/it]\u001B[A\n",
      "films on page 8:  10%|█         | 26/250 [06:57<1:02:09, 16.65s/it]\u001B[A\n",
      "films on page 8:  11%|█         | 27/250 [07:14<1:02:03, 16.70s/it]\u001B[A\n",
      "films on page 8:  11%|█         | 28/250 [07:30<1:00:58, 16.48s/it]\u001B[A\n",
      "films on page 8:  12%|█▏        | 29/250 [07:45<59:30, 16.15s/it]  \u001B[A\n",
      "films on page 8:  12%|█▏        | 30/250 [08:02<1:00:21, 16.46s/it]\u001B[A\n",
      "films on page 8:  12%|█▏        | 31/250 [08:18<58:49, 16.12s/it]  \u001B[A\n",
      "films on page 8:  13%|█▎        | 32/250 [08:33<57:10, 15.73s/it]\u001B[A\n",
      "films on page 8:  13%|█▎        | 33/250 [08:48<56:13, 15.55s/it]\u001B[A\n",
      "films on page 8:  14%|█▎        | 34/250 [09:04<57:16, 15.91s/it]\u001B[A\n",
      "films on page 8:  14%|█▍        | 35/250 [09:19<55:50, 15.58s/it]\u001B[A\n",
      "films on page 8:  14%|█▍        | 36/250 [09:35<55:59, 15.70s/it]\u001B[A\n",
      "films on page 8:  15%|█▍        | 37/250 [09:49<53:42, 15.13s/it]\u001B[A\n",
      "films on page 8:  15%|█▌        | 38/250 [10:07<56:52, 16.10s/it]\u001B[A\n",
      "films on page 8:  16%|█▌        | 39/250 [10:23<56:36, 16.10s/it]\u001B[A\n",
      "films on page 8:  16%|█▌        | 40/250 [10:40<56:48, 16.23s/it]\u001B[A\n",
      "films on page 8:  16%|█▋        | 41/250 [10:57<57:19, 16.46s/it]\u001B[A\n",
      "films on page 8:  17%|█▋        | 42/250 [11:14<57:15, 16.52s/it]\u001B[A\n",
      "films on page 8:  17%|█▋        | 43/250 [11:32<58:23, 16.93s/it]\u001B[A\n",
      "films on page 8:  18%|█▊        | 44/250 [11:49<58:18, 16.98s/it]\u001B[A\n",
      "films on page 8:  18%|█▊        | 45/250 [12:06<57:57, 16.96s/it]\u001B[A\n",
      "films on page 8:  18%|█▊        | 46/250 [12:19<54:11, 15.94s/it]\u001B[A\n",
      "films on page 8:  19%|█▉        | 47/250 [12:34<52:35, 15.54s/it]\u001B[A\n",
      "films on page 8:  19%|█▉        | 48/250 [12:49<52:32, 15.61s/it]\u001B[A\n",
      "films on page 8:  20%|█▉        | 49/250 [13:06<53:12, 15.88s/it]\u001B[A\n",
      "films on page 8:  20%|██        | 50/250 [13:20<50:54, 15.27s/it]\u001B[A\n",
      "films on page 8:  20%|██        | 51/250 [13:37<52:49, 15.93s/it]\u001B[A\n",
      "films on page 8:  21%|██        | 52/250 [13:55<54:14, 16.44s/it]\u001B[A\n",
      "films on page 8:  21%|██        | 53/250 [14:09<51:43, 15.76s/it]\u001B[A\n",
      "films on page 8:  22%|██▏       | 54/250 [14:24<50:33, 15.48s/it]\u001B[A\n",
      "films on page 8:  22%|██▏       | 55/250 [14:38<49:02, 15.09s/it]\u001B[A\n",
      "films on page 8:  22%|██▏       | 56/250 [14:55<50:15, 15.54s/it]\u001B[A\n",
      "films on page 8:  23%|██▎       | 57/250 [15:14<53:39, 16.68s/it]\u001B[A\n",
      "films on page 8:  23%|██▎       | 58/250 [15:30<52:22, 16.37s/it]\u001B[A\n",
      "films on page 8:  24%|██▎       | 59/250 [15:45<51:02, 16.03s/it]\u001B[A\n",
      "films on page 8:  24%|██▍       | 60/250 [16:00<49:59, 15.79s/it]\u001B[A\n",
      "films on page 8:  24%|██▍       | 61/250 [16:17<51:10, 16.25s/it]\u001B[A\n",
      "films on page 8:  25%|██▍       | 62/250 [16:33<50:05, 15.99s/it]\u001B[A\n",
      "films on page 8:  25%|██▌       | 63/250 [16:50<51:06, 16.40s/it]\u001B[A\n",
      "films on page 8:  26%|██▌       | 64/250 [17:05<49:37, 16.01s/it]\u001B[A\n",
      "films on page 8:  26%|██▌       | 65/250 [17:21<48:42, 15.80s/it]\u001B[A\n",
      "films on page 8:  26%|██▋       | 66/250 [17:34<46:21, 15.12s/it]\u001B[A\n",
      "films on page 8:  27%|██▋       | 67/250 [17:52<48:23, 15.87s/it]\u001B[A\n",
      "films on page 8:  27%|██▋       | 68/250 [18:08<48:36, 16.03s/it]\u001B[A\n",
      "films on page 8:  28%|██▊       | 69/250 [18:25<48:55, 16.22s/it]\u001B[A\n",
      "films on page 8:  28%|██▊       | 70/250 [18:41<48:47, 16.26s/it]\u001B[A\n",
      "films on page 8:  28%|██▊       | 71/250 [18:58<49:14, 16.51s/it]\u001B[A\n",
      "films on page 8:  29%|██▉       | 72/250 [19:16<50:02, 16.87s/it]\u001B[A\n",
      "films on page 8:  29%|██▉       | 73/250 [19:34<50:35, 17.15s/it]\u001B[A\n",
      "films on page 8:  30%|██▉       | 74/250 [19:48<47:31, 16.20s/it]\u001B[A\n",
      "films on page 8:  30%|███       | 75/250 [20:06<48:40, 16.69s/it]\u001B[A\n",
      "films on page 8:  30%|███       | 76/250 [20:19<45:45, 15.78s/it]\u001B[A\n",
      "films on page 8:  31%|███       | 77/250 [20:37<46:47, 16.23s/it]\u001B[A\n",
      "films on page 8:  31%|███       | 78/250 [20:58<50:38, 17.67s/it]\u001B[A\n",
      "films on page 8:  32%|███▏      | 79/250 [21:16<51:22, 18.03s/it]\u001B[A\n",
      "films on page 8:  32%|███▏      | 80/250 [21:35<51:18, 18.11s/it]\u001B[A\n",
      "films on page 8:  32%|███▏      | 81/250 [21:52<50:08, 17.80s/it]\u001B[A\n",
      "films on page 8:  33%|███▎      | 82/250 [22:08<48:33, 17.34s/it]\u001B[A\n",
      "films on page 8:  33%|███▎      | 83/250 [22:22<45:21, 16.29s/it]\u001B[A\n",
      "films on page 8:  34%|███▎      | 84/250 [22:36<43:15, 15.64s/it]\u001B[A\n",
      "films on page 8:  34%|███▍      | 85/250 [22:55<45:55, 16.70s/it]\u001B[A\n",
      "films on page 8:  34%|███▍      | 86/250 [23:11<44:57, 16.45s/it]\u001B[A\n",
      "films on page 8:  35%|███▍      | 87/250 [23:25<42:33, 15.67s/it]\u001B[A\n",
      "films on page 8:  35%|███▌      | 88/250 [23:41<42:13, 15.64s/it]\u001B[A\n",
      "films on page 8:  36%|███▌      | 89/250 [24:01<45:36, 16.99s/it]\u001B[A\n",
      "films on page 8:  36%|███▌      | 90/250 [24:15<43:25, 16.28s/it]\u001B[A\n",
      "films on page 8:  36%|███▋      | 91/250 [24:30<41:39, 15.72s/it]\u001B[A\n",
      "films on page 8:  37%|███▋      | 92/250 [24:41<38:15, 14.53s/it]\u001B[A\n",
      "films on page 8:  37%|███▋      | 93/250 [24:59<40:03, 15.31s/it]\u001B[A\n",
      "films on page 8:  38%|███▊      | 94/250 [25:17<42:36, 16.39s/it]\u001B[A\n",
      "films on page 8:  38%|███▊      | 95/250 [25:36<43:46, 16.94s/it]\u001B[A\n",
      "films on page 8:  38%|███▊      | 96/250 [25:54<44:31, 17.35s/it]\u001B[A\n",
      "films on page 8:  39%|███▉      | 97/250 [26:12<44:51, 17.59s/it]\u001B[A\n",
      "films on page 8:  39%|███▉      | 98/250 [26:28<43:18, 17.10s/it]\u001B[A\n",
      "films on page 8:  40%|███▉      | 99/250 [26:43<41:33, 16.52s/it]\u001B[A\n",
      "films on page 8:  40%|████      | 100/250 [27:01<42:25, 16.97s/it]\u001B[A\n",
      "films on page 8:  40%|████      | 101/250 [27:18<42:13, 17.01s/it]\u001B[A\n",
      "films on page 8:  41%|████      | 102/250 [27:34<41:10, 16.69s/it]\u001B[A\n",
      "films on page 8:  41%|████      | 103/250 [27:49<39:12, 16.00s/it]\u001B[A\n",
      "films on page 8:  42%|████▏     | 104/250 [28:08<41:10, 16.92s/it]\u001B[A\n",
      "films on page 8:  42%|████▏     | 105/250 [28:24<40:10, 16.62s/it]\u001B[A\n",
      "films on page 8:  42%|████▏     | 106/250 [28:41<40:21, 16.81s/it]\u001B[A\n",
      "films on page 8:  43%|████▎     | 107/250 [28:59<41:13, 17.30s/it]\u001B[A\n",
      "films on page 8:  43%|████▎     | 108/250 [29:15<39:32, 16.71s/it]\u001B[A\n",
      "films on page 8:  44%|████▎     | 109/250 [29:33<40:03, 17.04s/it]\u001B[A\n",
      "films on page 8:  44%|████▍     | 110/250 [29:52<41:30, 17.79s/it]\u001B[A\n",
      "films on page 8:  44%|████▍     | 111/250 [30:11<41:58, 18.12s/it]\u001B[A\n",
      "films on page 8:  45%|████▍     | 112/250 [30:28<41:02, 17.84s/it]\u001B[A\n",
      "films on page 8:  45%|████▌     | 113/250 [30:43<38:28, 16.85s/it]\u001B[A\n",
      "films on page 8:  46%|████▌     | 114/250 [30:58<36:52, 16.27s/it]\u001B[A\n",
      "films on page 8:  46%|████▌     | 115/250 [31:14<36:50, 16.37s/it]\u001B[A\n",
      "films on page 8:  46%|████▋     | 116/250 [31:28<34:29, 15.44s/it]\u001B[A\n",
      "films on page 8:  47%|████▋     | 117/250 [31:44<34:59, 15.78s/it]\u001B[A\n",
      "films on page 8:  47%|████▋     | 118/250 [31:57<32:39, 14.85s/it]\u001B[A\n",
      "films on page 8:  48%|████▊     | 119/250 [32:10<31:38, 14.49s/it]\u001B[A\n",
      "films on page 8:  48%|████▊     | 120/250 [32:28<33:37, 15.52s/it]\u001B[A\n",
      "films on page 8:  48%|████▊     | 121/250 [32:43<32:32, 15.13s/it]\u001B[A\n",
      "films on page 8:  49%|████▉     | 122/250 [33:00<34:03, 15.96s/it]\u001B[A\n",
      "films on page 8:  49%|████▉     | 123/250 [33:16<33:18, 15.74s/it]\u001B[A\n",
      "films on page 8:  50%|████▉     | 124/250 [33:30<31:49, 15.16s/it]\u001B[A\n",
      "films on page 8:  50%|█████     | 125/250 [33:46<32:20, 15.52s/it]\u001B[A\n",
      "films on page 8:  50%|█████     | 126/250 [34:04<33:41, 16.30s/it]\u001B[A\n",
      "films on page 8:  51%|█████     | 127/250 [34:19<32:38, 15.93s/it]\u001B[A\n",
      "films on page 8:  51%|█████     | 128/250 [34:32<30:40, 15.09s/it]\u001B[A\n",
      "films on page 8:  52%|█████▏    | 129/250 [34:50<32:18, 16.02s/it]\u001B[A\n",
      "films on page 8:  52%|█████▏    | 130/250 [35:06<31:54, 15.95s/it]\u001B[A\n",
      "films on page 8:  52%|█████▏    | 131/250 [35:21<31:08, 15.70s/it]\u001B[A\n",
      "films on page 8:  53%|█████▎    | 132/250 [35:37<31:09, 15.84s/it]\u001B[A\n",
      "films on page 8:  53%|█████▎    | 133/250 [35:54<31:29, 16.15s/it]\u001B[A\n",
      "films on page 8:  54%|█████▎    | 134/250 [36:08<30:01, 15.53s/it]\u001B[A\n",
      "films on page 8:  54%|█████▍    | 135/250 [36:25<30:21, 15.84s/it]\u001B[A\n",
      "films on page 8:  54%|█████▍    | 136/250 [36:42<30:44, 16.18s/it]\u001B[A\n",
      "films on page 8:  55%|█████▍    | 137/250 [36:58<30:26, 16.17s/it]\u001B[A\n",
      "films on page 8:  55%|█████▌    | 138/250 [37:17<31:54, 17.09s/it]\u001B[A\n",
      "films on page 8:  56%|█████▌    | 139/250 [37:32<30:16, 16.37s/it]\u001B[A\n",
      "films on page 8:  56%|█████▌    | 140/250 [37:46<28:28, 15.53s/it]\u001B[A\n",
      "films on page 8:  56%|█████▋    | 141/250 [38:04<29:38, 16.31s/it]\u001B[A\n",
      "films on page 8:  57%|█████▋    | 142/250 [38:18<28:17, 15.71s/it]\u001B[A\n",
      "films on page 8:  57%|█████▋    | 143/250 [38:34<28:12, 15.82s/it]\u001B[A\n",
      "films on page 8:  58%|█████▊    | 144/250 [38:50<27:44, 15.70s/it]\u001B[A\n",
      "films on page 8:  58%|█████▊    | 145/250 [39:03<26:23, 15.08s/it]\u001B[A\n",
      "films on page 8:  58%|█████▊    | 146/250 [39:19<26:40, 15.39s/it]\u001B[A\n",
      "films on page 8:  59%|█████▉    | 147/250 [39:33<25:43, 14.98s/it]\u001B[A\n",
      "films on page 8:  59%|█████▉    | 148/250 [39:48<25:32, 15.03s/it]\u001B[A\n",
      "films on page 8:  60%|█████▉    | 149/250 [40:06<26:41, 15.86s/it]\u001B[A\n",
      "films on page 8:  60%|██████    | 150/250 [40:23<26:44, 16.04s/it]\u001B[A\n",
      "films on page 8:  60%|██████    | 151/250 [40:38<26:07, 15.83s/it]\u001B[A\n",
      "films on page 8:  61%|██████    | 152/250 [40:52<24:43, 15.14s/it]\u001B[A\n",
      "films on page 8:  61%|██████    | 153/250 [41:06<24:10, 14.95s/it]\u001B[A\n",
      "films on page 8:  62%|██████▏   | 154/250 [41:23<24:45, 15.47s/it]\u001B[A\n",
      "films on page 8:  62%|██████▏   | 155/250 [41:40<25:15, 15.95s/it]\u001B[A\n",
      "films on page 8:  62%|██████▏   | 156/250 [41:55<24:23, 15.57s/it]\u001B[AWARNING:root:film tt1242422\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D5362D90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143D5362D90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D5362D90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D5362D90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 8:  63%|██████▎   | 157/250 [42:07<22:47, 14.70s/it]\u001B[A\n",
      "films on page 8:  63%|██████▎   | 158/250 [42:21<22:01, 14.36s/it]\u001B[A\n",
      "films on page 8:  64%|██████▎   | 159/250 [42:32<20:32, 13.55s/it]\u001B[A\n",
      "films on page 8:  64%|██████▍   | 160/250 [42:47<20:55, 13.95s/it]\u001B[A\n",
      "films on page 8:  64%|██████▍   | 161/250 [43:04<21:49, 14.71s/it]\u001B[A\n",
      "films on page 8:  65%|██████▍   | 162/250 [43:23<23:37, 16.11s/it]\u001B[A\n",
      "films on page 8:  65%|██████▌   | 163/250 [43:38<22:36, 15.60s/it]\u001B[A\n",
      "films on page 8:  66%|██████▌   | 164/250 [43:56<23:33, 16.44s/it]\u001B[A\n",
      "films on page 8:  66%|██████▌   | 165/250 [44:12<23:00, 16.24s/it]\u001B[A\n",
      "films on page 8:  66%|██████▋   | 166/250 [44:27<22:28, 16.06s/it]\u001B[A\n",
      "films on page 8:  67%|██████▋   | 167/250 [44:41<21:24, 15.47s/it]\u001B[A\n",
      "films on page 8:  67%|██████▋   | 168/250 [45:01<22:59, 16.82s/it]\u001B[A\n",
      "films on page 8:  68%|██████▊   | 169/250 [45:16<21:44, 16.10s/it]\u001B[A\n",
      "films on page 8:  68%|██████▊   | 170/250 [45:29<20:15, 15.19s/it]\u001B[A\n",
      "films on page 8:  68%|██████▊   | 171/250 [45:49<21:56, 16.66s/it]\u001B[A\n",
      "films on page 8:  69%|██████▉   | 172/250 [46:05<21:13, 16.33s/it]\u001B[A\n",
      "films on page 8:  69%|██████▉   | 173/250 [46:23<21:41, 16.91s/it]\u001B[A\n",
      "films on page 8:  70%|██████▉   | 174/250 [46:38<20:52, 16.48s/it]\u001B[A\n",
      "films on page 8:  70%|███████   | 175/250 [46:52<19:35, 15.67s/it]\u001B[A\n",
      "films on page 8:  70%|███████   | 176/250 [47:06<18:47, 15.24s/it]\u001B[AWARNING:root:film tt4901306\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D55F1D90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143D55F1D90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D55F1D90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D55F1D90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 8:  71%|███████   | 177/250 [47:19<17:40, 14.52s/it]\u001B[A\n",
      "films on page 8:  71%|███████   | 178/250 [47:34<17:42, 14.76s/it]\u001B[A\n",
      "films on page 8:  72%|███████▏  | 179/250 [47:55<19:40, 16.63s/it]\u001B[A\n",
      "films on page 8:  72%|███████▏  | 180/250 [48:14<20:04, 17.20s/it]\u001B[A\n",
      "films on page 8:  72%|███████▏  | 181/250 [48:28<18:34, 16.15s/it]\u001B[A\n",
      "films on page 8:  73%|███████▎  | 182/250 [48:41<17:28, 15.42s/it]\u001B[A\n",
      "films on page 8:  73%|███████▎  | 183/250 [49:00<18:10, 16.28s/it]\u001B[A\n",
      "films on page 8:  74%|███████▎  | 184/250 [49:15<17:30, 15.92s/it]\u001B[A\n",
      "films on page 8:  74%|███████▍  | 185/250 [49:35<18:33, 17.14s/it]\u001B[A\n",
      "films on page 8:  74%|███████▍  | 186/250 [49:51<18:07, 16.99s/it]\u001B[A\n",
      "films on page 8:  75%|███████▍  | 187/250 [50:05<16:48, 16.00s/it]\u001B[A\n",
      "films on page 8:  75%|███████▌  | 188/250 [50:23<17:06, 16.55s/it]\u001B[A\n",
      "films on page 8:  76%|███████▌  | 189/250 [50:40<16:52, 16.61s/it]\u001B[A\n",
      "films on page 8:  76%|███████▌  | 190/250 [50:57<16:40, 16.68s/it]\u001B[A\n",
      "films on page 8:  76%|███████▋  | 191/250 [51:16<17:06, 17.40s/it]\u001B[A\n",
      "films on page 8:  77%|███████▋  | 192/250 [51:27<15:13, 15.74s/it]\u001B[A\n",
      "films on page 8:  77%|███████▋  | 193/250 [51:45<15:18, 16.12s/it]\u001B[A\n",
      "films on page 8:  78%|███████▊  | 194/250 [52:01<15:16, 16.36s/it]\u001B[A\n",
      "films on page 8:  78%|███████▊  | 195/250 [52:13<13:46, 15.04s/it]\u001B[A\n",
      "films on page 8:  78%|███████▊  | 196/250 [52:28<13:25, 14.92s/it]\u001B[A\n",
      "films on page 8:  79%|███████▉  | 197/250 [52:44<13:25, 15.19s/it]\u001B[A\n",
      "films on page 8:  79%|███████▉  | 198/250 [53:00<13:23, 15.46s/it]\u001B[A\n",
      "films on page 8:  80%|███████▉  | 199/250 [53:15<13:05, 15.41s/it]\u001B[A\n",
      "films on page 8:  80%|████████  | 200/250 [53:34<13:40, 16.41s/it]\u001B[A\n",
      "films on page 8:  80%|████████  | 201/250 [53:48<12:44, 15.60s/it]\u001B[A\n",
      "films on page 8:  81%|████████  | 202/250 [54:02<12:17, 15.36s/it]\u001B[A\n",
      "films on page 8:  81%|████████  | 203/250 [54:22<13:03, 16.66s/it]\u001B[A\n",
      "films on page 8:  82%|████████▏ | 204/250 [54:40<13:07, 17.12s/it]\u001B[A\n",
      "films on page 8:  82%|████████▏ | 205/250 [54:55<12:14, 16.32s/it]\u001B[A\n",
      "films on page 8:  82%|████████▏ | 206/250 [55:13<12:27, 16.98s/it]\u001B[A\n",
      "films on page 8:  83%|████████▎ | 207/250 [55:31<12:16, 17.12s/it]\u001B[A\n",
      "films on page 8:  83%|████████▎ | 208/250 [55:45<11:27, 16.37s/it]\u001B[A\n",
      "films on page 8:  84%|████████▎ | 209/250 [56:00<10:44, 15.71s/it]\u001B[A\n",
      "films on page 8:  84%|████████▍ | 210/250 [56:12<09:45, 14.65s/it]\u001B[A\n",
      "films on page 8:  84%|████████▍ | 211/250 [56:28<09:48, 15.08s/it]\u001B[A\n",
      "films on page 8:  85%|████████▍ | 212/250 [56:43<09:30, 15.02s/it]\u001B[A\n",
      "films on page 8:  85%|████████▌ | 213/250 [56:57<09:10, 14.89s/it]\u001B[A\n",
      "films on page 8:  86%|████████▌ | 214/250 [57:12<08:50, 14.73s/it]\u001B[A\n",
      "films on page 8:  86%|████████▌ | 215/250 [57:28<08:48, 15.11s/it]\u001B[A\n",
      "films on page 8:  86%|████████▋ | 216/250 [57:43<08:39, 15.28s/it]\u001B[A\n",
      "films on page 8:  87%|████████▋ | 217/250 [58:03<09:05, 16.54s/it]\u001B[A\n",
      "films on page 8:  87%|████████▋ | 218/250 [58:20<08:59, 16.86s/it]\u001B[A\n",
      "films on page 8:  88%|████████▊ | 219/250 [58:36<08:29, 16.44s/it]\u001B[A\n",
      "films on page 8:  88%|████████▊ | 220/250 [58:51<08:00, 16.02s/it]\u001B[A\n",
      "films on page 8:  88%|████████▊ | 221/250 [59:05<07:25, 15.36s/it]\u001B[A\n",
      "films on page 8:  89%|████████▉ | 222/250 [59:21<07:19, 15.69s/it]\u001B[A\n",
      "films on page 8:  89%|████████▉ | 223/250 [59:36<06:54, 15.36s/it]\u001B[A\n",
      "films on page 8:  90%|████████▉ | 224/250 [59:54<07:03, 16.31s/it]\u001B[A\n",
      "films on page 8:  90%|█████████ | 225/250 [1:00:10<06:40, 16.01s/it]\u001B[A\n",
      "films on page 8:  90%|█████████ | 226/250 [1:00:22<05:59, 14.97s/it]\u001B[A\n",
      "films on page 8:  91%|█████████ | 227/250 [1:00:35<05:27, 14.25s/it]\u001B[A\n",
      "films on page 8:  91%|█████████ | 228/250 [1:00:52<05:31, 15.05s/it]\u001B[A\n",
      "films on page 8:  92%|█████████▏| 229/250 [1:01:07<05:17, 15.12s/it]\u001B[A\n",
      "films on page 8:  92%|█████████▏| 230/250 [1:01:23<05:09, 15.45s/it]\u001B[A\n",
      "films on page 8:  92%|█████████▏| 231/250 [1:01:39<04:54, 15.48s/it]\u001B[A\n",
      "films on page 8:  93%|█████████▎| 232/250 [1:01:54<04:35, 15.32s/it]\u001B[A\n",
      "films on page 8:  93%|█████████▎| 233/250 [1:02:09<04:18, 15.18s/it]\u001B[A\n",
      "films on page 8:  94%|█████████▎| 234/250 [1:02:26<04:16, 16.00s/it]\u001B[A\n",
      "films on page 8:  94%|█████████▍| 235/250 [1:02:41<03:53, 15.58s/it]\u001B[A\n",
      "films on page 8:  94%|█████████▍| 236/250 [1:02:56<03:33, 15.27s/it]\u001B[A\n",
      "films on page 8:  95%|█████████▍| 237/250 [1:03:10<03:16, 15.15s/it]\u001B[A\n",
      "films on page 8:  95%|█████████▌| 238/250 [1:03:26<03:03, 15.32s/it]\u001B[A\n",
      "films on page 8:  96%|█████████▌| 239/250 [1:03:42<02:48, 15.35s/it]\u001B[A\n",
      "films on page 8:  96%|█████████▌| 240/250 [1:03:56<02:29, 14.94s/it]\u001B[A\n",
      "films on page 8:  96%|█████████▋| 241/250 [1:04:11<02:16, 15.17s/it]\u001B[A\n",
      "films on page 8:  97%|█████████▋| 242/250 [1:04:26<02:00, 15.08s/it]\u001B[A\n",
      "films on page 8:  97%|█████████▋| 243/250 [1:04:41<01:45, 15.03s/it]\u001B[A\n",
      "films on page 8:  98%|█████████▊| 244/250 [1:04:56<01:30, 15.10s/it]\u001B[A\n",
      "films on page 8:  98%|█████████▊| 245/250 [1:05:12<01:15, 15.16s/it]\u001B[A\n",
      "films on page 8:  98%|█████████▊| 246/250 [1:05:27<01:01, 15.29s/it]\u001B[A\n",
      "films on page 8:  99%|█████████▉| 247/250 [1:05:44<00:47, 15.75s/it]\u001B[A\n",
      "films on page 8:  99%|█████████▉| 248/250 [1:06:02<00:32, 16.39s/it]\u001B[A\n",
      "films on page 8: 100%|█████████▉| 249/250 [1:06:18<00:16, 16.39s/it]\u001B[A\n",
      "films on page 8: 100%|██████████| 250/250 [1:06:33<00:00, 15.97s/it]\u001B[A\n",
      "pages:  30%|███       | 3/10 [3:21:37<7:49:04, 4020.65s/it]\n",
      "films on page 9:   0%|          | 0/250 [00:00<?, ?it/s]\u001B[A\n",
      "films on page 9:   0%|          | 1/250 [00:13<56:49, 13.69s/it]\u001B[A\n",
      "films on page 9:   1%|          | 2/250 [00:27<57:01, 13.80s/it]\u001B[A\n",
      "films on page 9:   1%|          | 3/250 [00:44<1:03:04, 15.32s/it]\u001B[A\n",
      "films on page 9:   2%|▏         | 4/250 [01:00<1:03:21, 15.45s/it]\u001B[A\n",
      "films on page 9:   2%|▏         | 5/250 [01:16<1:04:36, 15.82s/it]\u001B[A\n",
      "films on page 9:   2%|▏         | 6/250 [01:35<1:07:47, 16.67s/it]\u001B[A\n",
      "films on page 9:   3%|▎         | 7/250 [01:47<1:02:03, 15.32s/it]\u001B[A\n",
      "films on page 9:   3%|▎         | 8/250 [02:04<1:03:09, 15.66s/it]\u001B[A\n",
      "films on page 9:   4%|▎         | 9/250 [02:25<1:09:56, 17.41s/it]\u001B[A\n",
      "films on page 9:   4%|▍         | 10/250 [02:43<1:10:43, 17.68s/it]\u001B[A\n",
      "films on page 9:   4%|▍         | 11/250 [03:01<1:11:14, 17.88s/it]\u001B[A\n",
      "films on page 9:   5%|▍         | 12/250 [03:15<1:05:56, 16.62s/it]\u001B[A\n",
      "films on page 9:   5%|▌         | 13/250 [03:32<1:05:25, 16.56s/it]\u001B[A\n",
      "films on page 9:   6%|▌         | 14/250 [03:45<1:01:36, 15.66s/it]\u001B[A\n",
      "films on page 9:   6%|▌         | 15/250 [03:59<59:29, 15.19s/it]  \u001B[AWARNING:root:film tt0416508\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.dev', port=443): Max retries exceeded with url: /reviews/tt0416508?option=helpfulness&sortOrder=desc&nextKey=g4w6ddbmqyzdo6ic4oxwjnbqrhrmqab334otd7pob7e74vl5pjt6udc4oezvjmzpb4dxtogtn66kk2j5c6bzjfys27cm2 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143BA125580>: Failed to establish a new connection: [WinError 10060] Попытка установить соединение была безуспешной, т.к. от другого компьютера за требуемое время не получен нужный отклик, или было разорвано уже установленное соединение из-за неверного отклика уже подключенного компьютера'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 95, in create_connection\n",
      "    raise err\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\n",
      "    sock.connect(sa)\n",
      "TimeoutError: [WinError 10060] Попытка установить соединение была безуспешной, т.к. от другого компьютера за требуемое время не получен нужный отклик, или было разорвано уже установленное соединение из-за неверного отклика уже подключенного компьютера\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143BA125580>: Failed to establish a new connection: [WinError 10060] Попытка установить соединение была безуспешной, т.к. от другого компьютера за требуемое время не получен нужный отклик, или было разорвано уже установленное соединение из-за неверного отклика уже подключенного компьютера\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.dev', port=443): Max retries exceeded with url: /reviews/tt0416508?option=helpfulness&sortOrder=desc&nextKey=g4w6ddbmqyzdo6ic4oxwjnbqrhrmqab334otd7pob7e74vl5pjt6udc4oezvjmzpb4dxtogtn66kk2j5c6bzjfys27cm2 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143BA125580>: Failed to establish a new connection: [WinError 10060] Попытка установить соединение была безуспешной, т.к. от другого компьютера за требуемое время не получен нужный отклик, или было разорвано уже установленное соединение из-за неверного отклика уже подключенного компьютера'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.dev', port=443): Max retries exceeded with url: /reviews/tt0416508?option=helpfulness&sortOrder=desc&nextKey=g4w6ddbmqyzdo6ic4oxwjnbqrhrmqab334otd7pob7e74vl5pjt6udc4oezvjmzpb4dxtogtn66kk2j5c6bzjfys27cm2 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143BA125580>: Failed to establish a new connection: [WinError 10060] Попытка установить соединение была безуспешной, т.к. от другого компьютера за требуемое время не получен нужный отклик, или было разорвано уже установленное соединение из-за неверного отклика уже подключенного компьютера'))\n",
      "\n",
      "films on page 9:   6%|▋         | 16/250 [04:50<1:40:23, 25.74s/it]\u001B[AWARNING:root:film tt16419074\n",
      "ERROR:root:('Connection aborted.', ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 419, in connect\n",
      "    self.sock = ssl_wrap_socket(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 449, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 493, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 501, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 1041, in _create\n",
      "    self.do_handshake()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 1310, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ConnectionResetError: [WinError 10054] Удаленный хост принудительно разорвал существующее подключение\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\packages\\six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 419, in connect\n",
      "    self.sock = ssl_wrap_socket(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 449, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 493, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 501, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 1041, in _create\n",
      "    self.do_handshake()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 1310, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 67, in <module>\n",
      "    result_film = requests.get(API_URL + f'/title/{title_id}')\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 502, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None))\n",
      "\n",
      "films on page 9:   7%|▋         | 17/250 [05:30<1:57:34, 30.28s/it]\u001B[A\n",
      "films on page 9:   7%|▋         | 18/250 [05:56<1:51:31, 28.84s/it]\u001B[A\n",
      "films on page 9:   8%|▊         | 19/250 [06:13<1:37:14, 25.26s/it]\u001B[A\n",
      "films on page 9:   8%|▊         | 20/250 [06:26<1:23:22, 21.75s/it]\u001B[A\n",
      "films on page 9:   8%|▊         | 21/250 [06:43<1:17:17, 20.25s/it]\u001B[A\n",
      "films on page 9:   9%|▉         | 22/250 [07:00<1:13:39, 19.38s/it]\u001B[AWARNING:root:film tt1270262\n",
      "ERROR:root:('Connection aborted.', TimeoutError(10060, 'Попытка установить соединение была безуспешной, т.к. от другого компьютера за требуемое время не получен нужный отклик, или было разорвано уже установленное соединение из-за неверного отклика уже подключенного компьютера', None, 10060, None))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\http\\client.py\", line 1377, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\http\\client.py\", line 320, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\http\\client.py\", line 281, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 1242, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 1100, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "TimeoutError: [WinError 10060] Попытка установить соединение была безуспешной, т.к. от другого компьютера за требуемое время не получен нужный отклик, или было разорвано уже установленное соединение из-за неверного отклика уже подключенного компьютера\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\packages\\six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\http\\client.py\", line 1377, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\http\\client.py\", line 320, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\http\\client.py\", line 281, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 1242, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 1100, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', TimeoutError(10060, 'Попытка установить соединение была безуспешной, т.к. от другого компьютера за требуемое время не получен нужный отклик, или было разорвано уже установленное соединение из-за неверного отклика уже подключенного компьютера', None, 10060, None))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 723, in send\n",
      "    history = [resp for resp in gen]\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 723, in <listcomp>\n",
      "    history = [resp for resp in gen]\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 266, in resolve_redirects\n",
      "    resp = self.send(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 502, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', TimeoutError(10060, 'Попытка установить соединение была безуспешной, т.к. от другого компьютера за требуемое время не получен нужный отклик, или было разорвано уже установленное соединение из-за неверного отклика уже подключенного компьютера', None, 10060, None))\n",
      "\n",
      "films on page 9:   9%|▉         | 23/250 [07:46<1:42:46, 27.17s/it]\u001B[AWARNING:root:film tt0313443\n",
      "ERROR:root:('Connection aborted.', ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 419, in connect\n",
      "    self.sock = ssl_wrap_socket(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 449, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 493, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 501, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 1041, in _create\n",
      "    self.do_handshake()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 1310, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ConnectionResetError: [WinError 10054] Удаленный хост принудительно разорвал существующее подключение\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\packages\\six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 419, in connect\n",
      "    self.sock = ssl_wrap_socket(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 449, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 493, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 501, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 1041, in _create\n",
      "    self.do_handshake()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 1310, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 67, in <module>\n",
      "    result_film = requests.get(API_URL + f'/title/{title_id}')\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 502, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None))\n",
      "\n",
      "films on page 9:  10%|▉         | 24/250 [08:28<1:58:50, 31.55s/it]\u001B[A\n",
      "films on page 9:  10%|█         | 25/250 [08:44<1:41:28, 27.06s/it]\u001B[A\n",
      "films on page 9:  10%|█         | 26/250 [09:04<1:32:33, 24.79s/it]\u001B[A\n",
      "films on page 9:  11%|█         | 27/250 [09:20<1:22:10, 22.11s/it]\u001B[A\n",
      "films on page 9:  11%|█         | 28/250 [09:40<1:19:48, 21.57s/it]\u001B[A\n",
      "films on page 9:  12%|█▏        | 29/250 [09:54<1:11:46, 19.49s/it]\u001B[A\n",
      "films on page 9:  12%|█▏        | 30/250 [10:09<1:05:28, 17.86s/it]\u001B[A\n",
      "films on page 9:  12%|█▏        | 31/250 [10:26<1:05:15, 17.88s/it]\u001B[A\n",
      "films on page 9:  13%|█▎        | 32/250 [10:41<1:01:35, 16.95s/it]\u001B[A\n",
      "films on page 9:  13%|█▎        | 33/250 [10:59<1:01:47, 17.09s/it]\u001B[A\n",
      "films on page 9:  14%|█▎        | 34/250 [11:13<58:32, 16.26s/it]  \u001B[A\n",
      "films on page 9:  14%|█▍        | 35/250 [11:27<55:21, 15.45s/it]\u001B[A\n",
      "films on page 9:  14%|█▍        | 36/250 [11:44<57:22, 16.09s/it]\u001B[A\n",
      "films on page 9:  15%|█▍        | 37/250 [12:02<58:30, 16.48s/it]\u001B[A\n",
      "films on page 9:  15%|█▌        | 38/250 [12:16<56:17, 15.93s/it]\u001B[A\n",
      "films on page 9:  16%|█▌        | 39/250 [12:30<53:32, 15.23s/it]\u001B[A\n",
      "films on page 9:  16%|█▌        | 40/250 [12:44<52:34, 15.02s/it]\u001B[A\n",
      "films on page 9:  16%|█▋        | 41/250 [12:59<51:58, 14.92s/it]\u001B[A\n",
      "films on page 9:  17%|█▋        | 42/250 [13:15<52:49, 15.24s/it]\u001B[A\n",
      "films on page 9:  17%|█▋        | 43/250 [13:31<53:13, 15.43s/it]\u001B[A\n",
      "films on page 9:  18%|█▊        | 44/250 [13:50<56:32, 16.47s/it]\u001B[A\n",
      "films on page 9:  18%|█▊        | 45/250 [14:07<57:18, 16.77s/it]\u001B[A\n",
      "films on page 9:  18%|█▊        | 46/250 [14:22<55:21, 16.28s/it]\u001B[A\n",
      "films on page 9:  19%|█▉        | 47/250 [14:37<53:12, 15.73s/it]\u001B[A\n",
      "films on page 9:  19%|█▉        | 48/250 [14:53<53:19, 15.84s/it]\u001B[A\n",
      "films on page 9:  20%|█▉        | 49/250 [15:11<55:08, 16.46s/it]\u001B[A\n",
      "films on page 9:  20%|██        | 50/250 [15:31<58:16, 17.48s/it]\u001B[A\n",
      "films on page 9:  20%|██        | 51/250 [15:49<58:56, 17.77s/it]\u001B[A\n",
      "films on page 9:  21%|██        | 52/250 [16:04<56:01, 16.98s/it]\u001B[A\n",
      "films on page 9:  21%|██        | 53/250 [16:20<54:21, 16.56s/it]\u001B[A\n",
      "films on page 9:  22%|██▏       | 54/250 [16:36<53:57, 16.52s/it]\u001B[A\n",
      "films on page 9:  22%|██▏       | 55/250 [16:50<50:36, 15.57s/it]\u001B[A\n",
      "films on page 9:  22%|██▏       | 56/250 [17:06<51:30, 15.93s/it]\u001B[A\n",
      "films on page 9:  23%|██▎       | 57/250 [17:23<51:28, 16.00s/it]\u001B[A\n",
      "films on page 9:  23%|██▎       | 58/250 [17:37<49:30, 15.47s/it]\u001B[A\n",
      "films on page 9:  24%|██▎       | 59/250 [17:51<48:21, 15.19s/it]\u001B[A\n",
      "films on page 9:  24%|██▍       | 60/250 [18:08<49:28, 15.62s/it]\u001B[A\n",
      "films on page 9:  24%|██▍       | 61/250 [18:24<50:00, 15.87s/it]\u001B[A\n",
      "films on page 9:  25%|██▍       | 62/250 [18:41<49:58, 15.95s/it]\u001B[A\n",
      "films on page 9:  25%|██▌       | 63/250 [18:53<46:26, 14.90s/it]\u001B[A\n",
      "films on page 9:  26%|██▌       | 64/250 [19:08<45:57, 14.82s/it]\u001B[A\n",
      "films on page 9:  26%|██▌       | 65/250 [19:26<48:51, 15.85s/it]\u001B[A\n",
      "films on page 9:  26%|██▋       | 66/250 [19:42<48:58, 15.97s/it]\u001B[A\n",
      "films on page 9:  27%|██▋       | 67/250 [20:00<50:12, 16.46s/it]\u001B[A\n",
      "films on page 9:  27%|██▋       | 68/250 [20:16<49:56, 16.46s/it]\u001B[A\n",
      "films on page 9:  28%|██▊       | 69/250 [20:33<49:50, 16.52s/it]\u001B[A\n",
      "films on page 9:  28%|██▊       | 70/250 [20:51<50:53, 16.96s/it]\u001B[A\n",
      "films on page 9:  28%|██▊       | 71/250 [21:05<48:14, 16.17s/it]\u001B[A\n",
      "films on page 9:  29%|██▉       | 72/250 [21:21<47:42, 16.08s/it]\u001B[A\n",
      "films on page 9:  29%|██▉       | 73/250 [21:35<45:23, 15.39s/it]\u001B[A\n",
      "films on page 9:  30%|██▉       | 74/250 [21:50<45:23, 15.48s/it]\u001B[A\n",
      "films on page 9:  30%|███       | 75/250 [22:05<44:03, 15.10s/it]\u001B[A\n",
      "films on page 9:  30%|███       | 76/250 [22:21<45:08, 15.57s/it]\u001B[A\n",
      "films on page 9:  31%|███       | 77/250 [22:36<43:57, 15.25s/it]\u001B[A\n",
      "films on page 9:  31%|███       | 78/250 [22:54<46:35, 16.25s/it]\u001B[A\n",
      "films on page 9:  32%|███▏      | 79/250 [23:05<41:50, 14.68s/it]\u001B[A\n",
      "films on page 9:  32%|███▏      | 80/250 [23:23<43:59, 15.52s/it]\u001B[A\n",
      "films on page 9:  32%|███▏      | 81/250 [23:44<48:19, 17.15s/it]\u001B[A\n",
      "films on page 9:  33%|███▎      | 82/250 [23:57<44:50, 16.01s/it]\u001B[A\n",
      "films on page 9:  33%|███▎      | 83/250 [24:15<46:08, 16.58s/it]\u001B[A\n",
      "films on page 9:  34%|███▎      | 84/250 [24:35<48:35, 17.57s/it]\u001B[A\n",
      "films on page 9:  34%|███▍      | 85/250 [24:50<46:01, 16.74s/it]\u001B[A\n",
      "films on page 9:  34%|███▍      | 86/250 [25:06<45:05, 16.49s/it]\u001B[A\n",
      "films on page 9:  35%|███▍      | 87/250 [25:23<45:17, 16.67s/it]\u001B[A\n",
      "films on page 9:  35%|███▌      | 88/250 [25:36<42:28, 15.73s/it]\u001B[A\n",
      "films on page 9:  36%|███▌      | 89/250 [25:54<43:49, 16.33s/it]\u001B[A\n",
      "films on page 9:  36%|███▌      | 90/250 [26:11<43:43, 16.40s/it]\u001B[A\n",
      "films on page 9:  36%|███▋      | 91/250 [26:25<42:06, 15.89s/it]\u001B[A\n",
      "films on page 9:  37%|███▋      | 92/250 [26:42<42:41, 16.21s/it]\u001B[A\n",
      "films on page 9:  37%|███▋      | 93/250 [26:58<42:10, 16.12s/it]\u001B[A\n",
      "films on page 9:  38%|███▊      | 94/250 [27:13<41:12, 15.85s/it]\u001B[A\n",
      "films on page 9:  38%|███▊      | 95/250 [27:27<39:21, 15.23s/it]\u001B[A\n",
      "films on page 9:  38%|███▊      | 96/250 [27:44<40:34, 15.81s/it]\u001B[A\n",
      "films on page 9:  39%|███▉      | 97/250 [28:03<42:30, 16.67s/it]\u001B[A\n",
      "films on page 9:  39%|███▉      | 98/250 [28:16<39:25, 15.56s/it]\u001B[A\n",
      "films on page 9:  40%|███▉      | 99/250 [28:32<39:06, 15.54s/it]\u001B[A\n",
      "films on page 9:  40%|████      | 100/250 [28:42<35:19, 14.13s/it]\u001B[A\n",
      "films on page 9:  40%|████      | 101/250 [29:00<37:22, 15.05s/it]\u001B[A\n",
      "films on page 9:  41%|████      | 102/250 [29:14<36:41, 14.87s/it]\u001B[A\n",
      "films on page 9:  41%|████      | 103/250 [29:32<38:38, 15.77s/it]\u001B[A\n",
      "films on page 9:  42%|████▏     | 104/250 [29:51<40:50, 16.79s/it]\u001B[A\n",
      "films on page 9:  42%|████▏     | 105/250 [30:07<40:02, 16.57s/it]\u001B[A\n",
      "films on page 9:  42%|████▏     | 106/250 [30:24<40:02, 16.69s/it]\u001B[A\n",
      "films on page 9:  43%|████▎     | 107/250 [30:43<41:06, 17.25s/it]\u001B[A\n",
      "films on page 9:  43%|████▎     | 108/250 [30:59<40:13, 17.00s/it]\u001B[A\n",
      "films on page 9:  44%|████▎     | 109/250 [31:17<40:47, 17.36s/it]\u001B[A\n",
      "films on page 9:  44%|████▍     | 110/250 [31:33<39:41, 17.01s/it]\u001B[A\n",
      "films on page 9:  44%|████▍     | 111/250 [31:51<40:00, 17.27s/it]\u001B[A\n",
      "films on page 9:  45%|████▍     | 112/250 [32:07<38:42, 16.83s/it]\u001B[A\n",
      "films on page 9:  45%|████▌     | 113/250 [32:23<37:40, 16.50s/it]\u001B[A\n",
      "films on page 9:  46%|████▌     | 114/250 [32:38<36:44, 16.21s/it]\u001B[A\n",
      "films on page 9:  46%|████▌     | 115/250 [32:55<36:43, 16.32s/it]\u001B[A\n",
      "films on page 9:  46%|████▋     | 116/250 [33:12<36:59, 16.56s/it]\u001B[A\n",
      "films on page 9:  47%|████▋     | 117/250 [33:26<35:07, 15.85s/it]\u001B[A\n",
      "films on page 9:  47%|████▋     | 118/250 [33:42<34:28, 15.67s/it]\u001B[A\n",
      "films on page 9:  48%|████▊     | 119/250 [33:56<33:11, 15.20s/it]\u001B[A\n",
      "films on page 9:  48%|████▊     | 120/250 [34:12<33:41, 15.55s/it]\u001B[A\n",
      "films on page 9:  48%|████▊     | 121/250 [34:27<32:49, 15.27s/it]\u001B[A\n",
      "films on page 9:  49%|████▉     | 122/250 [34:39<30:53, 14.48s/it]\u001B[A\n",
      "films on page 9:  49%|████▉     | 123/250 [34:58<33:13, 15.70s/it]\u001B[A\n",
      "films on page 9:  50%|████▉     | 124/250 [35:16<34:27, 16.41s/it]\u001B[A\n",
      "films on page 9:  50%|█████     | 125/250 [35:36<36:24, 17.47s/it]\u001B[A\n",
      "films on page 9:  50%|█████     | 126/250 [35:50<34:11, 16.55s/it]\u001B[A\n",
      "films on page 9:  51%|█████     | 127/250 [36:09<35:27, 17.30s/it]\u001B[A\n",
      "films on page 9:  51%|█████     | 128/250 [36:28<36:07, 17.77s/it]\u001B[A\n",
      "films on page 9:  52%|█████▏    | 129/250 [36:44<34:51, 17.29s/it]\u001B[A\n",
      "films on page 9:  52%|█████▏    | 130/250 [37:00<33:23, 16.70s/it]\u001B[A\n",
      "films on page 9:  52%|█████▏    | 131/250 [37:16<32:38, 16.46s/it]\u001B[A\n",
      "films on page 9:  53%|█████▎    | 132/250 [37:30<31:24, 15.97s/it]\u001B[A\n",
      "films on page 9:  53%|█████▎    | 133/250 [37:45<30:37, 15.71s/it]\u001B[A\n",
      "films on page 9:  54%|█████▎    | 134/250 [38:01<30:08, 15.59s/it]\u001B[A\n",
      "films on page 9:  54%|█████▍    | 135/250 [38:18<30:53, 16.11s/it]\u001B[A\n",
      "films on page 9:  54%|█████▍    | 136/250 [38:35<30:48, 16.21s/it]\u001B[A\n",
      "films on page 9:  55%|█████▍    | 137/250 [38:54<32:19, 17.17s/it]\u001B[A\n",
      "films on page 9:  55%|█████▌    | 138/250 [39:14<33:25, 17.91s/it]\u001B[A\n",
      "films on page 9:  56%|█████▌    | 139/250 [39:27<30:44, 16.62s/it]\u001B[A\n",
      "films on page 9:  56%|█████▌    | 140/250 [39:40<28:35, 15.59s/it]\u001B[A\n",
      "films on page 9:  56%|█████▋    | 141/250 [39:59<29:50, 16.43s/it]\u001B[A\n",
      "films on page 9:  57%|█████▋    | 142/250 [40:16<29:46, 16.55s/it]\u001B[A\n",
      "films on page 9:  57%|█████▋    | 143/250 [40:33<29:50, 16.73s/it]\u001B[A\n",
      "films on page 9:  58%|█████▊    | 144/250 [40:50<29:57, 16.96s/it]\u001B[A\n",
      "films on page 9:  58%|█████▊    | 145/250 [41:06<28:54, 16.52s/it]\u001B[A\n",
      "films on page 9:  58%|█████▊    | 146/250 [41:19<27:07, 15.65s/it]\u001B[A\n",
      "films on page 9:  59%|█████▉    | 147/250 [41:33<25:58, 15.13s/it]\u001B[A\n",
      "films on page 9:  59%|█████▉    | 148/250 [41:49<25:55, 15.25s/it]\u001B[A\n",
      "films on page 9:  60%|█████▉    | 149/250 [42:05<26:12, 15.57s/it]\u001B[A\n",
      "films on page 9:  60%|██████    | 150/250 [42:27<28:54, 17.34s/it]\u001B[A\n",
      "films on page 9:  60%|██████    | 151/250 [42:42<27:54, 16.91s/it]\u001B[AWARNING:root:film tt0183869\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D936BBB0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143D936BBB0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D936BBB0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D936BBB0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 9:  61%|██████    | 152/250 [42:58<26:58, 16.52s/it]\u001B[A\n",
      "films on page 9:  61%|██████    | 153/250 [43:14<26:28, 16.38s/it]\u001B[A\n",
      "films on page 9:  62%|██████▏   | 154/250 [43:28<25:10, 15.73s/it]\u001B[A\n",
      "films on page 9:  62%|██████▏   | 155/250 [43:44<25:02, 15.82s/it]\u001B[A\n",
      "films on page 9:  62%|██████▏   | 156/250 [44:04<26:40, 17.03s/it]\u001B[A\n",
      "films on page 9:  63%|██████▎   | 157/250 [44:19<25:33, 16.49s/it]\u001B[A\n",
      "films on page 9:  63%|██████▎   | 158/250 [44:37<25:47, 16.82s/it]\u001B[A\n",
      "films on page 9:  64%|██████▎   | 159/250 [44:54<25:32, 16.84s/it]\u001B[A\n",
      "films on page 9:  64%|██████▍   | 160/250 [45:10<24:49, 16.55s/it]\u001B[A\n",
      "films on page 9:  64%|██████▍   | 161/250 [45:23<23:11, 15.64s/it]\u001B[A\n",
      "films on page 9:  65%|██████▍   | 162/250 [45:41<23:52, 16.28s/it]\u001B[A\n",
      "films on page 9:  65%|██████▌   | 163/250 [46:00<24:56, 17.20s/it]\u001B[A\n",
      "films on page 9:  66%|██████▌   | 164/250 [46:16<23:57, 16.71s/it]\u001B[A\n",
      "films on page 9:  66%|██████▌   | 165/250 [46:32<23:33, 16.63s/it]\u001B[A\n",
      "films on page 9:  66%|██████▋   | 166/250 [46:45<21:25, 15.31s/it]\u001B[A\n",
      "films on page 9:  67%|██████▋   | 167/250 [46:58<20:10, 14.58s/it]\u001B[A\n",
      "films on page 9:  67%|██████▋   | 168/250 [47:13<20:24, 14.93s/it]\u001B[A\n",
      "films on page 9:  68%|██████▊   | 169/250 [47:27<19:36, 14.52s/it]\u001B[A\n",
      "films on page 9:  68%|██████▊   | 170/250 [47:44<20:14, 15.18s/it]\u001B[A\n",
      "films on page 9:  68%|██████▊   | 171/250 [47:58<19:49, 15.06s/it]\u001B[A\n",
      "films on page 9:  69%|██████▉   | 172/250 [48:13<19:26, 14.96s/it]\u001B[A\n",
      "films on page 9:  69%|██████▉   | 173/250 [48:28<19:11, 14.95s/it]\u001B[A\n",
      "films on page 9:  70%|██████▉   | 174/250 [48:42<18:33, 14.65s/it]\u001B[A\n",
      "films on page 9:  70%|███████   | 175/250 [48:58<18:39, 14.93s/it]\u001B[A\n",
      "films on page 9:  70%|███████   | 176/250 [49:13<18:31, 15.01s/it]\u001B[A\n",
      "films on page 9:  71%|███████   | 177/250 [49:28<18:14, 14.99s/it]\u001B[A\n",
      "films on page 9:  71%|███████   | 178/250 [49:43<17:55, 14.94s/it]\u001B[A\n",
      "films on page 9:  72%|███████▏  | 179/250 [49:58<18:01, 15.24s/it]\u001B[A\n",
      "films on page 9:  72%|███████▏  | 180/250 [50:14<17:59, 15.42s/it]\u001B[A\n",
      "films on page 9:  72%|███████▏  | 181/250 [50:32<18:22, 15.98s/it]\u001B[A\n",
      "films on page 9:  73%|███████▎  | 182/250 [50:52<19:32, 17.24s/it]\u001B[A\n",
      "films on page 9:  73%|███████▎  | 183/250 [51:05<17:52, 16.01s/it]\u001B[A\n",
      "films on page 9:  74%|███████▎  | 184/250 [51:21<17:47, 16.17s/it]\u001B[A\n",
      "films on page 9:  74%|███████▍  | 185/250 [51:38<17:31, 16.18s/it]\u001B[A\n",
      "films on page 9:  74%|███████▍  | 186/250 [51:54<17:10, 16.10s/it]\u001B[A\n",
      "films on page 9:  75%|███████▍  | 187/250 [52:11<17:26, 16.61s/it]\u001B[A\n",
      "films on page 9:  75%|███████▌  | 188/250 [52:26<16:40, 16.13s/it]\u001B[A\n",
      "films on page 9:  76%|███████▌  | 189/250 [52:42<16:14, 15.97s/it]\u001B[A\n",
      "films on page 9:  76%|███████▌  | 190/250 [52:55<15:11, 15.19s/it]\u001B[AWARNING:root:film tt7825208\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D9889BB0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143D9889BB0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D9889BB0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D9889BB0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 9:  76%|███████▋  | 191/250 [53:09<14:35, 14.85s/it]\u001B[A\n",
      "films on page 9:  77%|███████▋  | 192/250 [53:27<15:08, 15.67s/it]\u001B[A\n",
      "films on page 9:  77%|███████▋  | 193/250 [53:45<15:36, 16.44s/it]\u001B[A\n",
      "films on page 9:  78%|███████▊  | 194/250 [54:01<15:11, 16.27s/it]\u001B[A\n",
      "films on page 9:  78%|███████▊  | 195/250 [54:17<14:44, 16.08s/it]\u001B[A\n",
      "films on page 9:  78%|███████▊  | 196/250 [54:36<15:25, 17.14s/it]\u001B[A\n",
      "films on page 9:  79%|███████▉  | 197/250 [54:50<14:20, 16.24s/it]\u001B[A\n",
      "films on page 9:  79%|███████▉  | 198/250 [55:06<13:59, 16.14s/it]\u001B[A\n",
      "films on page 9:  80%|███████▉  | 199/250 [55:23<13:46, 16.21s/it]\u001B[A\n",
      "films on page 9:  80%|████████  | 200/250 [55:37<13:02, 15.65s/it]\u001B[A\n",
      "films on page 9:  80%|████████  | 201/250 [55:52<12:42, 15.56s/it]\u001B[A\n",
      "films on page 9:  81%|████████  | 202/250 [56:07<12:18, 15.38s/it]\u001B[A\n",
      "films on page 9:  81%|████████  | 203/250 [56:20<11:23, 14.54s/it]\u001B[A\n",
      "films on page 9:  82%|████████▏ | 204/250 [56:36<11:35, 15.12s/it]\u001B[A\n",
      "films on page 9:  82%|████████▏ | 205/250 [56:54<11:59, 15.98s/it]\u001B[A\n",
      "films on page 9:  82%|████████▏ | 206/250 [57:12<11:59, 16.35s/it]\u001B[A\n",
      "films on page 9:  83%|████████▎ | 207/250 [57:26<11:21, 15.85s/it]\u001B[A\n",
      "films on page 9:  83%|████████▎ | 208/250 [57:41<10:51, 15.52s/it]\u001B[A\n",
      "films on page 9:  84%|████████▎ | 209/250 [57:58<10:55, 15.99s/it]\u001B[A\n",
      "films on page 9:  84%|████████▍ | 210/250 [58:12<10:12, 15.31s/it]\u001B[A\n",
      "films on page 9:  84%|████████▍ | 211/250 [58:30<10:32, 16.23s/it]\u001B[A\n",
      "films on page 9:  85%|████████▍ | 212/250 [58:45<10:00, 15.81s/it]\u001B[A\n",
      "films on page 9:  85%|████████▌ | 213/250 [59:00<09:35, 15.55s/it]\u001B[A\n",
      "films on page 9:  86%|████████▌ | 214/250 [59:18<09:44, 16.23s/it]\u001B[A\n",
      "films on page 9:  86%|████████▌ | 215/250 [59:34<09:31, 16.32s/it]\u001B[A\n",
      "films on page 9:  86%|████████▋ | 216/250 [59:50<09:08, 16.13s/it]\u001B[A\n",
      "films on page 9:  87%|████████▋ | 217/250 [1:00:04<08:31, 15.50s/it]\u001B[A\n",
      "films on page 9:  87%|████████▋ | 218/250 [1:00:21<08:32, 16.02s/it]\u001B[A\n",
      "films on page 9:  88%|████████▊ | 219/250 [1:00:38<08:19, 16.10s/it]\u001B[A\n",
      "films on page 9:  88%|████████▊ | 220/250 [1:00:55<08:11, 16.37s/it]\u001B[A\n",
      "films on page 9:  88%|████████▊ | 221/250 [1:01:11<07:52, 16.29s/it]\u001B[A\n",
      "films on page 9:  89%|████████▉ | 222/250 [1:01:30<07:59, 17.12s/it]\u001B[A\n",
      "films on page 9:  89%|████████▉ | 223/250 [1:01:47<07:46, 17.28s/it]\u001B[A\n",
      "films on page 9:  90%|████████▉ | 224/250 [1:02:02<07:11, 16.61s/it]\u001B[A\n",
      "films on page 9:  90%|█████████ | 225/250 [1:02:17<06:42, 16.10s/it]\u001B[A\n",
      "films on page 9:  90%|█████████ | 226/250 [1:02:32<06:17, 15.71s/it]\u001B[A\n",
      "films on page 9:  91%|█████████ | 227/250 [1:02:43<05:25, 14.13s/it]\u001B[A\n",
      "films on page 9:  91%|█████████ | 228/250 [1:02:56<05:06, 13.92s/it]\u001B[A\n",
      "films on page 9:  92%|█████████▏| 229/250 [1:03:08<04:38, 13.24s/it]\u001B[A\n",
      "films on page 9:  92%|█████████▏| 230/250 [1:03:23<04:34, 13.73s/it]\u001B[A\n",
      "films on page 9:  92%|█████████▏| 231/250 [1:03:40<04:44, 14.96s/it]\u001B[A\n",
      "films on page 9:  93%|█████████▎| 232/250 [1:03:55<04:29, 14.97s/it]\u001B[A\n",
      "films on page 9:  93%|█████████▎| 233/250 [1:04:11<04:15, 15.03s/it]\u001B[A\n",
      "films on page 9:  94%|█████████▎| 234/250 [1:04:23<03:46, 14.13s/it]\u001B[A\n",
      "films on page 9:  94%|█████████▍| 235/250 [1:04:40<03:45, 15.05s/it]\u001B[A\n",
      "films on page 9:  94%|█████████▍| 236/250 [1:04:58<03:43, 15.97s/it]\u001B[A\n",
      "films on page 9:  95%|█████████▍| 237/250 [1:05:14<03:26, 15.91s/it]\u001B[A\n",
      "films on page 9:  95%|█████████▌| 238/250 [1:05:29<03:08, 15.74s/it]\u001B[A\n",
      "films on page 9:  96%|█████████▌| 239/250 [1:05:47<02:59, 16.30s/it]\u001B[A\n",
      "films on page 9:  96%|█████████▌| 240/250 [1:06:01<02:37, 15.71s/it]\u001B[A\n",
      "films on page 9:  96%|█████████▋| 241/250 [1:06:17<02:22, 15.80s/it]\u001B[A\n",
      "films on page 9:  97%|█████████▋| 242/250 [1:06:34<02:09, 16.24s/it]\u001B[A\n",
      "films on page 9:  97%|█████████▋| 243/250 [1:06:51<01:55, 16.44s/it]\u001B[A\n",
      "films on page 9:  98%|█████████▊| 244/250 [1:07:06<01:35, 15.95s/it]\u001B[A\n",
      "films on page 9:  98%|█████████▊| 245/250 [1:07:21<01:18, 15.62s/it]\u001B[A\n",
      "films on page 9:  98%|█████████▊| 246/250 [1:07:39<01:05, 16.31s/it]\u001B[A\n",
      "films on page 9:  99%|█████████▉| 247/250 [1:07:54<00:47, 15.87s/it]\u001B[A\n",
      "films on page 9:  99%|█████████▉| 248/250 [1:08:11<00:32, 16.34s/it]\u001B[A\n",
      "films on page 9: 100%|█████████▉| 249/250 [1:08:27<00:16, 16.26s/it]\u001B[A\n",
      "films on page 9: 100%|██████████| 250/250 [1:08:52<00:00, 16.53s/it]\u001B[A\n",
      "pages:  40%|████      | 4/10 [4:30:37<6:46:45, 4067.52s/it]\n",
      "films on page 10:   0%|          | 0/250 [00:00<?, ?it/s]\u001B[A\n",
      "films on page 10:   0%|          | 1/250 [00:14<58:50, 14.18s/it]\u001B[A\n",
      "films on page 10:   1%|          | 2/250 [00:30<1:02:59, 15.24s/it]\u001B[A\n",
      "films on page 10:   1%|          | 3/250 [00:47<1:06:35, 16.18s/it]\u001B[A\n",
      "films on page 10:   2%|▏         | 4/250 [01:05<1:08:39, 16.74s/it]\u001B[A\n",
      "films on page 10:   2%|▏         | 5/250 [01:19<1:04:27, 15.79s/it]\u001B[A\n",
      "films on page 10:   2%|▏         | 6/250 [01:32<1:01:15, 15.06s/it]\u001B[A\n",
      "films on page 10:   3%|▎         | 7/250 [01:47<1:01:01, 15.07s/it]\u001B[A\n",
      "films on page 10:   3%|▎         | 8/250 [02:03<1:00:56, 15.11s/it]\u001B[A\n",
      "films on page 10:   4%|▎         | 9/250 [02:20<1:03:03, 15.70s/it]\u001B[A\n",
      "films on page 10:   4%|▍         | 10/250 [02:36<1:03:59, 16.00s/it]\u001B[A\n",
      "films on page 10:   4%|▍         | 11/250 [02:50<1:01:06, 15.34s/it]\u001B[A\n",
      "films on page 10:   5%|▍         | 12/250 [03:08<1:04:15, 16.20s/it]\u001B[A\n",
      "films on page 10:   5%|▌         | 13/250 [03:21<59:37, 15.10s/it]  \u001B[A\n",
      "films on page 10:   6%|▌         | 14/250 [03:36<59:40, 15.17s/it]\u001B[A\n",
      "films on page 10:   6%|▌         | 15/250 [03:55<1:03:09, 16.13s/it]\u001B[A\n",
      "films on page 10:   6%|▋         | 16/250 [04:08<1:00:14, 15.45s/it]\u001B[A\n",
      "films on page 10:   7%|▋         | 17/250 [04:26<1:02:17, 16.04s/it]\u001B[A\n",
      "films on page 10:   7%|▋         | 18/250 [04:44<1:04:00, 16.55s/it]\u001B[A\n",
      "films on page 10:   8%|▊         | 19/250 [04:56<59:17, 15.40s/it]  \u001B[A\n",
      "films on page 10:   8%|▊         | 20/250 [05:14<1:01:51, 16.14s/it]\u001B[A\n",
      "films on page 10:   8%|▊         | 21/250 [05:32<1:03:47, 16.71s/it]\u001B[A\n",
      "films on page 10:   9%|▉         | 22/250 [05:48<1:02:13, 16.38s/it]\u001B[A\n",
      "films on page 10:   9%|▉         | 23/250 [06:00<56:41, 14.99s/it]  \u001B[A\n",
      "films on page 10:  10%|▉         | 24/250 [06:15<56:52, 15.10s/it]\u001B[A\n",
      "films on page 10:  10%|█         | 25/250 [06:32<59:21, 15.83s/it]\u001B[A\n",
      "films on page 10:  10%|█         | 26/250 [06:49<59:37, 15.97s/it]\u001B[A\n",
      "films on page 10:  11%|█         | 27/250 [07:05<59:50, 16.10s/it]\u001B[A\n",
      "films on page 10:  11%|█         | 28/250 [07:21<59:34, 16.10s/it]\u001B[A\n",
      "films on page 10:  12%|█▏        | 29/250 [07:38<1:00:11, 16.34s/it]\u001B[A\n",
      "films on page 10:  12%|█▏        | 30/250 [07:56<1:01:52, 16.87s/it]\u001B[A\n",
      "films on page 10:  12%|█▏        | 31/250 [08:16<1:04:22, 17.64s/it]\u001B[A\n",
      "films on page 10:  13%|█▎        | 32/250 [08:33<1:03:30, 17.48s/it]\u001B[A\n",
      "films on page 10:  13%|█▎        | 33/250 [08:50<1:02:55, 17.40s/it]\u001B[A\n",
      "films on page 10:  14%|█▎        | 34/250 [09:09<1:04:02, 17.79s/it]\u001B[A\n",
      "films on page 10:  14%|█▍        | 35/250 [09:28<1:05:54, 18.39s/it]\u001B[A\n",
      "films on page 10:  14%|█▍        | 36/250 [09:46<1:04:55, 18.20s/it]\u001B[A\n",
      "films on page 10:  15%|█▍        | 37/250 [10:03<1:03:01, 17.75s/it]\u001B[A\n",
      "films on page 10:  15%|█▌        | 38/250 [10:17<59:15, 16.77s/it]  \u001B[A\n",
      "films on page 10:  16%|█▌        | 39/250 [10:32<56:59, 16.21s/it]\u001B[A\n",
      "films on page 10:  16%|█▌        | 40/250 [10:47<55:24, 15.83s/it]\u001B[A\n",
      "films on page 10:  16%|█▋        | 41/250 [11:02<53:30, 15.36s/it]\u001B[A\n",
      "films on page 10:  17%|█▋        | 42/250 [11:14<50:27, 14.56s/it]\u001B[A\n",
      "films on page 10:  17%|█▋        | 43/250 [11:30<51:24, 14.90s/it]\u001B[A\n",
      "films on page 10:  18%|█▊        | 44/250 [11:43<48:58, 14.26s/it]\u001B[A\n",
      "films on page 10:  18%|█▊        | 45/250 [12:00<51:30, 15.08s/it]\u001B[A\n",
      "films on page 10:  18%|█▊        | 46/250 [12:15<51:18, 15.09s/it]\u001B[A\n",
      "films on page 10:  19%|█▉        | 47/250 [12:30<51:19, 15.17s/it]\u001B[A\n",
      "films on page 10:  19%|█▉        | 48/250 [12:47<53:01, 15.75s/it]\u001B[A\n",
      "films on page 10:  20%|█▉        | 49/250 [13:03<52:39, 15.72s/it]\u001B[A\n",
      "films on page 10:  20%|██        | 50/250 [13:19<52:59, 15.90s/it]\u001B[A\n",
      "films on page 10:  20%|██        | 51/250 [13:32<49:47, 15.01s/it]\u001B[A\n",
      "films on page 10:  21%|██        | 52/250 [13:48<50:35, 15.33s/it]\u001B[A\n",
      "films on page 10:  21%|██        | 53/250 [14:04<50:48, 15.48s/it]\u001B[A\n",
      "films on page 10:  22%|██▏       | 54/250 [14:21<51:40, 15.82s/it]\u001B[A\n",
      "films on page 10:  22%|██▏       | 55/250 [14:37<51:50, 15.95s/it]\u001B[A\n",
      "films on page 10:  22%|██▏       | 56/250 [14:55<53:12, 16.46s/it]\u001B[A\n",
      "films on page 10:  23%|██▎       | 57/250 [15:11<53:10, 16.53s/it]\u001B[A\n",
      "films on page 10:  23%|██▎       | 58/250 [15:27<52:12, 16.31s/it]\u001B[A\n",
      "films on page 10:  24%|██▎       | 59/250 [15:44<52:26, 16.47s/it]\u001B[A\n",
      "films on page 10:  24%|██▍       | 60/250 [16:01<52:16, 16.51s/it]\u001B[A\n",
      "films on page 10:  24%|██▍       | 61/250 [16:14<48:51, 15.51s/it]\u001B[A\n",
      "films on page 10:  25%|██▍       | 62/250 [16:31<50:31, 16.12s/it]\u001B[A\n",
      "films on page 10:  25%|██▌       | 63/250 [16:46<49:02, 15.73s/it]\u001B[A\n",
      "films on page 10:  26%|██▌       | 64/250 [17:00<47:30, 15.33s/it]\u001B[A\n",
      "films on page 10:  26%|██▌       | 65/250 [17:19<49:51, 16.17s/it]\u001B[A\n",
      "films on page 10:  26%|██▋       | 66/250 [17:35<49:53, 16.27s/it]\u001B[A\n",
      "films on page 10:  27%|██▋       | 67/250 [17:51<49:41, 16.29s/it]\u001B[A\n",
      "films on page 10:  27%|██▋       | 68/250 [18:09<50:55, 16.79s/it]\u001B[A\n",
      "films on page 10:  28%|██▊       | 69/250 [18:25<49:33, 16.43s/it]\u001B[A\n",
      "films on page 10:  28%|██▊       | 70/250 [18:40<48:08, 16.05s/it]\u001B[A\n",
      "films on page 10:  28%|██▊       | 71/250 [18:53<45:05, 15.12s/it]\u001B[A\n",
      "films on page 10:  29%|██▉       | 72/250 [19:10<46:20, 15.62s/it]\u001B[A\n",
      "films on page 10:  29%|██▉       | 73/250 [19:25<45:56, 15.57s/it]\u001B[A\n",
      "films on page 10:  30%|██▉       | 74/250 [19:44<48:32, 16.55s/it]\u001B[A\n",
      "films on page 10:  30%|███       | 75/250 [20:00<47:46, 16.38s/it]\u001B[A\n",
      "films on page 10:  30%|███       | 76/250 [20:17<48:10, 16.61s/it]\u001B[A\n",
      "films on page 10:  31%|███       | 77/250 [20:34<47:53, 16.61s/it]\u001B[A\n",
      "films on page 10:  31%|███       | 78/250 [20:45<43:04, 15.02s/it]\u001B[AWARNING:root:film tt0815244\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D346F460>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143D346F460>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D346F460>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D346F460>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 10:  32%|███▏      | 79/250 [20:59<41:50, 14.68s/it]\u001B[A\n",
      "films on page 10:  32%|███▏      | 80/250 [21:19<46:17, 16.34s/it]\u001B[A\n",
      "films on page 10:  32%|███▏      | 81/250 [21:35<45:43, 16.23s/it]\u001B[A\n",
      "films on page 10:  33%|███▎      | 82/250 [21:51<45:13, 16.15s/it]\u001B[A\n",
      "films on page 10:  33%|███▎      | 83/250 [22:06<43:40, 15.69s/it]\u001B[A\n",
      "films on page 10:  34%|███▎      | 84/250 [22:21<42:37, 15.41s/it]\u001B[A\n",
      "films on page 10:  34%|███▍      | 85/250 [22:38<43:38, 15.87s/it]\u001B[A\n",
      "films on page 10:  34%|███▍      | 86/250 [22:57<46:17, 16.94s/it]\u001B[A\n",
      "films on page 10:  35%|███▍      | 87/250 [23:13<45:03, 16.58s/it]\u001B[A\n",
      "films on page 10:  35%|███▌      | 88/250 [23:27<43:06, 15.97s/it]\u001B[A\n",
      "films on page 10:  36%|███▌      | 89/250 [23:45<43:51, 16.35s/it]\u001B[A\n",
      "films on page 10:  36%|███▌      | 90/250 [24:01<43:32, 16.33s/it]\u001B[A\n",
      "films on page 10:  36%|███▋      | 91/250 [24:17<42:54, 16.19s/it]\u001B[A\n",
      "films on page 10:  37%|███▋      | 92/250 [24:30<40:05, 15.22s/it]\u001B[A\n",
      "films on page 10:  37%|███▋      | 93/250 [24:44<39:01, 14.91s/it]\u001B[A\n",
      "films on page 10:  38%|███▊      | 94/250 [25:00<40:06, 15.43s/it]\u001B[A\n",
      "films on page 10:  38%|███▊      | 95/250 [25:18<41:28, 16.05s/it]\u001B[A\n",
      "films on page 10:  38%|███▊      | 96/250 [25:33<40:42, 15.86s/it]\u001B[A\n",
      "films on page 10:  39%|███▉      | 97/250 [25:47<38:58, 15.28s/it]\u001B[A\n",
      "films on page 10:  39%|███▉      | 98/250 [26:02<38:17, 15.12s/it]\u001B[A\n",
      "films on page 10:  40%|███▉      | 99/250 [26:20<39:50, 15.83s/it]\u001B[A\n",
      "films on page 10:  40%|████      | 100/250 [26:47<48:23, 19.36s/it]\u001B[A\n",
      "films on page 10:  40%|████      | 101/250 [27:04<46:02, 18.54s/it]\u001B[A\n",
      "films on page 10:  41%|████      | 102/250 [27:16<41:07, 16.67s/it]\u001B[A\n",
      "films on page 10:  41%|████      | 103/250 [27:35<42:17, 17.26s/it]\u001B[A\n",
      "films on page 10:  42%|████▏     | 104/250 [27:50<40:55, 16.82s/it]\u001B[A\n",
      "films on page 10:  42%|████▏     | 105/250 [28:04<38:23, 15.89s/it]\u001B[A\n",
      "films on page 10:  42%|████▏     | 106/250 [28:23<40:28, 16.87s/it]\u001B[A\n",
      "films on page 10:  43%|████▎     | 107/250 [28:39<39:10, 16.43s/it]\u001B[A\n",
      "films on page 10:  43%|████▎     | 108/250 [28:55<38:53, 16.43s/it]\u001B[A\n",
      "films on page 10:  44%|████▎     | 109/250 [29:14<40:22, 17.18s/it]\u001B[A\n",
      "films on page 10:  44%|████▍     | 110/250 [29:35<42:20, 18.15s/it]\u001B[A\n",
      "films on page 10:  44%|████▍     | 111/250 [29:50<40:09, 17.33s/it]\u001B[A\n",
      "films on page 10:  45%|████▍     | 112/250 [30:09<40:45, 17.72s/it]\u001B[A\n",
      "films on page 10:  45%|████▌     | 113/250 [30:29<42:00, 18.40s/it]\u001B[A\n",
      "films on page 10:  46%|████▌     | 114/250 [30:44<39:31, 17.44s/it]\u001B[A\n",
      "films on page 10:  46%|████▌     | 115/250 [31:01<39:11, 17.42s/it]\u001B[A\n",
      "films on page 10:  46%|████▋     | 116/250 [31:18<38:43, 17.34s/it]\u001B[A\n",
      "films on page 10:  47%|████▋     | 117/250 [31:37<39:14, 17.70s/it]\u001B[A\n",
      "films on page 10:  47%|████▋     | 118/250 [31:52<37:34, 17.08s/it]\u001B[A\n",
      "films on page 10:  48%|████▊     | 119/250 [32:11<38:06, 17.46s/it]\u001B[A\n",
      "films on page 10:  48%|████▊     | 120/250 [32:26<36:23, 16.80s/it]\u001B[A\n",
      "films on page 10:  48%|████▊     | 121/250 [32:43<36:28, 16.96s/it]\u001B[A\n",
      "films on page 10:  49%|████▉     | 122/250 [33:05<39:11, 18.37s/it]\u001B[A\n",
      "films on page 10:  49%|████▉     | 123/250 [33:23<38:43, 18.30s/it]\u001B[A\n",
      "films on page 10:  50%|████▉     | 124/250 [33:39<36:50, 17.55s/it]\u001B[A\n",
      "films on page 10:  50%|█████     | 125/250 [33:57<37:07, 17.82s/it]\u001B[A\n",
      "films on page 10:  50%|█████     | 126/250 [34:20<39:33, 19.14s/it]\u001B[A\n",
      "films on page 10:  51%|█████     | 127/250 [34:41<40:29, 19.76s/it]\u001B[A\n",
      "films on page 10:  51%|█████     | 128/250 [35:04<41:59, 20.65s/it]\u001B[A\n",
      "films on page 10:  52%|█████▏    | 129/250 [35:23<40:54, 20.29s/it]\u001B[A\n",
      "films on page 10:  52%|█████▏    | 130/250 [35:43<40:35, 20.29s/it]\u001B[A\n",
      "films on page 10:  52%|█████▏    | 131/250 [36:02<39:25, 19.88s/it]\u001B[A\n",
      "films on page 10:  53%|█████▎    | 132/250 [36:21<38:15, 19.46s/it]\u001B[A\n",
      "films on page 10:  53%|█████▎    | 133/250 [36:37<35:53, 18.41s/it]\u001B[A\n",
      "films on page 10:  54%|█████▎    | 134/250 [36:56<36:06, 18.68s/it]\u001B[A\n",
      "films on page 10:  54%|█████▍    | 135/250 [37:14<35:31, 18.53s/it]\u001B[A\n",
      "films on page 10:  54%|█████▍    | 136/250 [37:31<34:07, 17.96s/it]\u001B[A\n",
      "films on page 10:  55%|█████▍    | 137/250 [37:51<34:53, 18.53s/it]\u001B[A\n",
      "films on page 10:  55%|█████▌    | 138/250 [38:08<34:00, 18.22s/it]\u001B[A\n",
      "films on page 10:  56%|█████▌    | 139/250 [38:30<35:56, 19.43s/it]\u001B[A\n",
      "films on page 10:  56%|█████▌    | 140/250 [38:52<36:50, 20.10s/it]\u001B[A\n",
      "films on page 10:  56%|█████▋    | 141/250 [39:12<36:17, 19.98s/it]\u001B[A\n",
      "films on page 10:  57%|█████▋    | 142/250 [39:27<33:14, 18.47s/it]\u001B[A\n",
      "films on page 10:  57%|█████▋    | 143/250 [39:44<32:02, 17.96s/it]\u001B[A\n",
      "films on page 10:  58%|█████▊    | 144/250 [39:58<29:55, 16.94s/it]\u001B[A\n",
      "films on page 10:  58%|█████▊    | 145/250 [40:15<29:33, 16.89s/it]\u001B[A\n",
      "films on page 10:  58%|█████▊    | 146/250 [40:33<29:48, 17.20s/it]\u001B[A\n",
      "films on page 10:  59%|█████▉    | 147/250 [40:54<31:22, 18.27s/it]\u001B[A\n",
      "films on page 10:  59%|█████▉    | 148/250 [41:13<31:55, 18.78s/it]\u001B[A\n",
      "films on page 10:  60%|█████▉    | 149/250 [41:31<30:47, 18.29s/it]\u001B[A\n",
      "films on page 10:  60%|██████    | 150/250 [41:49<30:42, 18.43s/it]\u001B[A\n",
      "films on page 10:  60%|██████    | 151/250 [42:08<30:34, 18.53s/it]\u001B[A\n",
      "films on page 10:  61%|██████    | 152/250 [42:27<30:14, 18.51s/it]\u001B[A\n",
      "films on page 10:  61%|██████    | 153/250 [42:45<30:02, 18.58s/it]\u001B[A\n",
      "films on page 10:  62%|██████▏   | 154/250 [43:05<30:15, 18.91s/it]\u001B[A\n",
      "films on page 10:  62%|██████▏   | 155/250 [43:23<29:24, 18.57s/it]\u001B[A\n",
      "films on page 10:  62%|██████▏   | 156/250 [43:43<29:49, 19.04s/it]\u001B[A\n",
      "films on page 10:  63%|██████▎   | 157/250 [44:03<29:47, 19.22s/it]\u001B[A\n",
      "films on page 10:  63%|██████▎   | 158/250 [44:18<27:53, 18.19s/it]\u001B[A\n",
      "films on page 10:  64%|██████▎   | 159/250 [44:34<26:32, 17.50s/it]\u001B[A\n",
      "films on page 10:  64%|██████▍   | 160/250 [44:55<27:35, 18.40s/it]\u001B[A\n",
      "films on page 10:  64%|██████▍   | 161/250 [45:11<26:25, 17.82s/it]\u001B[A\n",
      "films on page 10:  65%|██████▍   | 162/250 [45:28<25:52, 17.64s/it]\u001B[A\n",
      "films on page 10:  65%|██████▌   | 163/250 [45:45<24:56, 17.20s/it]\u001B[A\n",
      "films on page 10:  66%|██████▌   | 164/250 [46:03<25:10, 17.56s/it]\u001B[A\n",
      "films on page 10:  66%|██████▌   | 165/250 [46:17<23:23, 16.51s/it]\u001B[A\n",
      "films on page 10:  66%|██████▋   | 166/250 [46:36<24:15, 17.32s/it]\u001B[A\n",
      "films on page 10:  67%|██████▋   | 167/250 [46:54<24:03, 17.40s/it]\u001B[A\n",
      "films on page 10:  67%|██████▋   | 168/250 [47:12<24:14, 17.74s/it]\u001B[A\n",
      "films on page 10:  68%|██████▊   | 169/250 [47:32<24:35, 18.21s/it]\u001B[A\n",
      "films on page 10:  68%|██████▊   | 170/250 [47:53<25:24, 19.05s/it]\u001B[A\n",
      "films on page 10:  68%|██████▊   | 171/250 [48:07<23:16, 17.68s/it]\u001B[A\n",
      "films on page 10:  69%|██████▉   | 172/250 [48:28<24:00, 18.47s/it]\u001B[A\n",
      "films on page 10:  69%|██████▉   | 173/250 [48:43<22:42, 17.69s/it]\u001B[A\n",
      "films on page 10:  70%|██████▉   | 174/250 [49:02<22:54, 18.09s/it]\u001B[A\n",
      "films on page 10:  70%|███████   | 175/250 [49:22<23:16, 18.62s/it]\u001B[A\n",
      "films on page 10:  70%|███████   | 176/250 [49:42<23:19, 18.91s/it]\u001B[A\n",
      "films on page 10:  71%|███████   | 177/250 [49:57<21:47, 17.91s/it]\u001B[A\n",
      "films on page 10:  71%|███████   | 178/250 [50:18<22:17, 18.58s/it]\u001B[A\n",
      "films on page 10:  72%|███████▏  | 179/250 [50:34<21:07, 17.85s/it]\u001B[A\n",
      "films on page 10:  72%|███████▏  | 180/250 [50:53<21:20, 18.30s/it]\u001B[A\n",
      "films on page 10:  72%|███████▏  | 181/250 [51:10<20:30, 17.83s/it]\u001B[A\n",
      "films on page 10:  73%|███████▎  | 182/250 [51:27<19:57, 17.61s/it]\u001B[A\n",
      "films on page 10:  73%|███████▎  | 183/250 [51:46<20:06, 18.01s/it]\u001B[A\n",
      "films on page 10:  74%|███████▎  | 184/250 [52:04<19:42, 17.91s/it]\u001B[A\n",
      "films on page 10:  74%|███████▍  | 185/250 [52:28<21:25, 19.77s/it]\u001B[A\n",
      "films on page 10:  74%|███████▍  | 186/250 [52:51<22:21, 20.96s/it]\u001B[A\n",
      "films on page 10:  75%|███████▍  | 187/250 [53:10<21:24, 20.39s/it]\u001B[A\n",
      "films on page 10:  75%|███████▌  | 188/250 [53:25<19:21, 18.73s/it]\u001B[A\n",
      "films on page 10:  76%|███████▌  | 189/250 [53:41<18:13, 17.93s/it]\u001B[A\n",
      "films on page 10:  76%|███████▌  | 190/250 [54:02<18:37, 18.63s/it]\u001B[A\n",
      "films on page 10:  76%|███████▋  | 191/250 [54:21<18:24, 18.72s/it]\u001B[A\n",
      "films on page 10:  77%|███████▋  | 192/250 [54:38<17:41, 18.30s/it]\u001B[A\n",
      "films on page 10:  77%|███████▋  | 193/250 [54:55<17:01, 17.92s/it]\u001B[A\n",
      "films on page 10:  78%|███████▊  | 194/250 [55:14<16:59, 18.20s/it]\u001B[A\n",
      "films on page 10:  78%|███████▊  | 195/250 [55:31<16:31, 18.02s/it]\u001B[A\n",
      "films on page 10:  78%|███████▊  | 196/250 [55:49<16:14, 18.04s/it]\u001B[A\n",
      "films on page 10:  79%|███████▉  | 197/250 [56:03<14:37, 16.56s/it]\u001B[A\n",
      "films on page 10:  79%|███████▉  | 198/250 [56:21<14:49, 17.10s/it]\u001B[A\n",
      "films on page 10:  80%|███████▉  | 199/250 [56:37<14:22, 16.92s/it]\u001B[A\n",
      "films on page 10:  80%|████████  | 200/250 [57:00<15:37, 18.74s/it]\u001B[A\n",
      "films on page 10:  80%|████████  | 201/250 [57:18<14:59, 18.36s/it]\u001B[A\n",
      "films on page 10:  81%|████████  | 202/250 [57:34<14:07, 17.66s/it]\u001B[AWARNING:root:film tt1247640\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143BEE1FAC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143BEE1FAC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143BEE1FAC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143BEE1FAC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 10:  81%|████████  | 203/250 [57:46<12:29, 15.94s/it]\u001B[A\n",
      "films on page 10:  82%|████████▏ | 204/250 [58:01<11:56, 15.59s/it]\u001B[A\n",
      "films on page 10:  82%|████████▏ | 205/250 [58:18<12:06, 16.15s/it]\u001B[A\n",
      "films on page 10:  82%|████████▏ | 206/250 [58:36<12:18, 16.77s/it]\u001B[A\n",
      "films on page 10:  83%|████████▎ | 207/250 [58:48<10:55, 15.24s/it]\u001B[A\n",
      "films on page 10:  83%|████████▎ | 208/250 [59:06<11:20, 16.21s/it]\u001B[A\n",
      "films on page 10:  84%|████████▎ | 209/250 [59:21<10:44, 15.72s/it]\u001B[A\n",
      "films on page 10:  84%|████████▍ | 210/250 [59:41<11:15, 16.89s/it]\u001B[A\n",
      "films on page 10:  84%|████████▍ | 211/250 [1:00:00<11:32, 17.77s/it]\u001B[AWARNING:root:film tt2034031\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DED04490>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143DED04490>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DED04490>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DED04490>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 10:  85%|████████▍ | 212/250 [1:00:11<09:53, 15.62s/it]\u001B[A\n",
      "films on page 10:  85%|████████▌ | 213/250 [1:00:29<10:04, 16.35s/it]\u001B[A\n",
      "films on page 10:  86%|████████▌ | 214/250 [1:00:52<11:00, 18.34s/it]\u001B[A\n",
      "films on page 10:  86%|████████▌ | 215/250 [1:01:11<10:48, 18.54s/it]\u001B[A\n",
      "films on page 10:  86%|████████▋ | 216/250 [1:01:31<10:39, 18.82s/it]\u001B[A\n",
      "films on page 10:  87%|████████▋ | 217/250 [1:01:49<10:20, 18.80s/it]\u001B[A\n",
      "films on page 10:  87%|████████▋ | 218/250 [1:02:05<09:32, 17.89s/it]\u001B[A\n",
      "films on page 10:  88%|████████▊ | 219/250 [1:02:22<09:01, 17.46s/it]\u001B[A\n",
      "films on page 10:  88%|████████▊ | 220/250 [1:02:38<08:32, 17.08s/it]\u001B[A\n",
      "films on page 10:  88%|████████▊ | 221/250 [1:03:00<09:03, 18.74s/it]\u001B[A\n",
      "films on page 10:  89%|████████▉ | 222/250 [1:03:14<08:00, 17.17s/it]\u001B[A\n",
      "films on page 10:  89%|████████▉ | 223/250 [1:03:30<07:31, 16.73s/it]\u001B[A\n",
      "films on page 10:  90%|████████▉ | 224/250 [1:03:47<07:21, 16.97s/it]\u001B[A\n",
      "films on page 10:  90%|█████████ | 225/250 [1:04:03<06:56, 16.64s/it]\u001B[A\n",
      "films on page 10:  90%|█████████ | 226/250 [1:04:22<06:53, 17.23s/it]\u001B[A\n",
      "films on page 10:  91%|█████████ | 227/250 [1:04:41<06:48, 17.74s/it]\u001B[A\n",
      "films on page 10:  91%|█████████ | 228/250 [1:05:00<06:40, 18.20s/it]\u001B[A\n",
      "films on page 10:  92%|█████████▏| 229/250 [1:05:14<05:58, 17.05s/it]\u001B[A\n",
      "films on page 10:  92%|█████████▏| 230/250 [1:05:29<05:30, 16.50s/it]\u001B[A\n",
      "films on page 10:  92%|█████████▏| 231/250 [1:05:48<05:27, 17.25s/it]\u001B[A\n",
      "films on page 10:  93%|█████████▎| 232/250 [1:06:08<05:22, 17.91s/it]\u001B[A\n",
      "films on page 10:  93%|█████████▎| 233/250 [1:06:28<05:18, 18.74s/it]\u001B[A\n",
      "films on page 10:  94%|█████████▎| 234/250 [1:06:48<05:03, 18.97s/it]\u001B[A\n",
      "films on page 10:  94%|█████████▍| 235/250 [1:07:06<04:40, 18.73s/it]\u001B[A\n",
      "films on page 10:  94%|█████████▍| 236/250 [1:07:20<04:03, 17.39s/it]\u001B[A\n",
      "films on page 10:  95%|█████████▍| 237/250 [1:07:33<03:28, 16.01s/it]\u001B[AWARNING:root:film tt2800240\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143BEE1F820>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143BEE1F820>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143BEE1F820>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143BEE1F820>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 10:  95%|█████████▌| 238/250 [1:07:43<02:48, 14.07s/it]\u001B[A\n",
      "films on page 10:  96%|█████████▌| 239/250 [1:07:59<02:40, 14.61s/it]\u001B[A\n",
      "films on page 10:  96%|█████████▌| 240/250 [1:08:16<02:32, 15.29s/it]\u001B[A\n",
      "films on page 10:  96%|█████████▋| 241/250 [1:08:31<02:17, 15.29s/it]\u001B[A\n",
      "films on page 10:  97%|█████████▋| 242/250 [1:08:54<02:22, 17.76s/it]\u001B[A\n",
      "films on page 10:  97%|█████████▋| 243/250 [1:09:11<02:01, 17.37s/it]\u001B[A\n",
      "films on page 10:  98%|█████████▊| 244/250 [1:09:29<01:45, 17.59s/it]\u001B[A\n",
      "films on page 10:  98%|█████████▊| 245/250 [1:09:42<01:21, 16.32s/it]\u001B[A\n",
      "films on page 10:  98%|█████████▊| 246/250 [1:09:58<01:04, 16.15s/it]\u001B[A\n",
      "films on page 10:  99%|█████████▉| 247/250 [1:10:18<00:51, 17.17s/it]\u001B[AWARNING:root:film tt1064932\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DF198490>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143DF198490>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DF198490>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DF198490>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 10:  99%|█████████▉| 248/250 [1:10:34<00:34, 17.04s/it]\u001B[A\n",
      "films on page 10: 100%|█████████▉| 249/250 [1:10:59<00:19, 19.23s/it]\u001B[AWARNING:root:film tt1286126\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C3642850>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143C3642850>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C3642850>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C3642850>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 10: 100%|██████████| 250/250 [1:11:06<00:00, 17.07s/it]\u001B[A\n",
      "pages:  50%|█████     | 5/10 [5:41:49<5:45:07, 4141.41s/it]\n",
      "films on page 11:   0%|          | 0/250 [00:00<?, ?it/s]\u001B[A\n",
      "films on page 11:   0%|          | 1/250 [00:19<1:20:53, 19.49s/it]\u001B[A\n",
      "films on page 11:   1%|          | 2/250 [00:38<1:18:20, 18.95s/it]\u001B[A\n",
      "films on page 11:   1%|          | 3/250 [00:56<1:16:53, 18.68s/it]\u001B[A\n",
      "films on page 11:   2%|▏         | 4/250 [01:12<1:11:54, 17.54s/it]\u001B[A\n",
      "films on page 11:   2%|▏         | 5/250 [01:26<1:07:02, 16.42s/it]\u001B[A\n",
      "films on page 11:   2%|▏         | 6/250 [01:41<1:04:36, 15.89s/it]\u001B[A\n",
      "films on page 11:   3%|▎         | 7/250 [01:57<1:04:41, 15.97s/it]\u001B[A\n",
      "films on page 11:   3%|▎         | 8/250 [02:13<1:04:29, 15.99s/it]\u001B[A\n",
      "films on page 11:   4%|▎         | 9/250 [02:35<1:11:32, 17.81s/it]\u001B[A\n",
      "films on page 11:   4%|▍         | 10/250 [02:51<1:08:43, 17.18s/it]\u001B[A\n",
      "films on page 11:   4%|▍         | 11/250 [03:05<1:04:21, 16.16s/it]\u001B[A\n",
      "films on page 11:   5%|▍         | 12/250 [03:23<1:07:13, 16.95s/it]\u001B[A\n",
      "films on page 11:   5%|▌         | 13/250 [03:44<1:11:57, 18.22s/it]\u001B[A\n",
      "films on page 11:   6%|▌         | 14/250 [04:01<1:09:24, 17.65s/it]\u001B[A\n",
      "films on page 11:   6%|▌         | 15/250 [04:17<1:07:25, 17.21s/it]\u001B[A\n",
      "films on page 11:   6%|▋         | 16/250 [04:33<1:05:08, 16.70s/it]\u001B[A\n",
      "films on page 11:   7%|▋         | 17/250 [04:51<1:07:05, 17.28s/it]\u001B[A\n",
      "films on page 11:   7%|▋         | 18/250 [05:10<1:08:36, 17.74s/it]\u001B[A\n",
      "films on page 11:   8%|▊         | 19/250 [05:28<1:08:39, 17.83s/it]\u001B[A\n",
      "films on page 11:   8%|▊         | 20/250 [05:41<1:02:57, 16.42s/it]\u001B[A\n",
      "films on page 11:   8%|▊         | 21/250 [05:56<1:00:31, 15.86s/it]\u001B[A\n",
      "films on page 11:   9%|▉         | 22/250 [06:12<1:00:50, 16.01s/it]\u001B[A\n",
      "films on page 11:   9%|▉         | 23/250 [06:26<58:00, 15.33s/it]  \u001B[A\n",
      "films on page 11:  10%|▉         | 24/250 [06:43<1:00:23, 16.03s/it]\u001B[A\n",
      "films on page 11:  10%|█         | 25/250 [06:58<58:36, 15.63s/it]  \u001B[A\n",
      "films on page 11:  10%|█         | 26/250 [07:20<1:05:40, 17.59s/it]\u001B[A\n",
      "films on page 11:  11%|█         | 27/250 [07:39<1:06:33, 17.91s/it]\u001B[A\n",
      "films on page 11:  11%|█         | 28/250 [07:55<1:03:45, 17.23s/it]\u001B[A\n",
      "films on page 11:  12%|█▏        | 29/250 [08:12<1:03:48, 17.32s/it]\u001B[A\n",
      "films on page 11:  12%|█▏        | 30/250 [08:31<1:05:24, 17.84s/it]\u001B[A\n",
      "films on page 11:  12%|█▏        | 31/250 [08:49<1:05:14, 17.87s/it]\u001B[A\n",
      "films on page 11:  13%|█▎        | 32/250 [09:06<1:03:43, 17.54s/it]\u001B[A\n",
      "films on page 11:  13%|█▎        | 33/250 [09:24<1:04:01, 17.70s/it]\u001B[A\n",
      "films on page 11:  14%|█▎        | 34/250 [09:45<1:07:00, 18.61s/it]\u001B[A\n",
      "films on page 11:  14%|█▍        | 35/250 [10:04<1:06:53, 18.67s/it]\u001B[A\n",
      "films on page 11:  14%|█▍        | 36/250 [10:20<1:04:42, 18.14s/it]\u001B[A\n",
      "films on page 11:  15%|█▍        | 37/250 [10:39<1:04:30, 18.17s/it]\u001B[A\n",
      "films on page 11:  15%|█▌        | 38/250 [10:57<1:04:37, 18.29s/it]\u001B[A\n",
      "films on page 11:  16%|█▌        | 39/250 [11:13<1:02:06, 17.66s/it]\u001B[A\n",
      "films on page 11:  16%|█▌        | 40/250 [11:28<58:22, 16.68s/it]  \u001B[A\n",
      "films on page 11:  16%|█▋        | 41/250 [11:41<54:44, 15.71s/it]\u001B[A\n",
      "films on page 11:  17%|█▋        | 42/250 [11:58<55:30, 16.01s/it]\u001B[AWARNING:root:film tt0482527\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E0B1BD00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E0B1BD00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E0B1BD00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E0B1BD00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 11:  17%|█▋        | 43/250 [12:11<52:00, 15.07s/it]\u001B[A\n",
      "films on page 11:  18%|█▊        | 44/250 [12:30<55:59, 16.31s/it]\u001B[A\n",
      "films on page 11:  18%|█▊        | 45/250 [12:48<56:52, 16.64s/it]\u001B[A\n",
      "films on page 11:  18%|█▊        | 46/250 [13:06<58:18, 17.15s/it]\u001B[A\n",
      "films on page 11:  19%|█▉        | 47/250 [13:25<1:00:02, 17.75s/it]\u001B[A\n",
      "films on page 11:  19%|█▉        | 48/250 [13:52<1:08:55, 20.48s/it]\u001B[A\n",
      "films on page 11:  20%|█▉        | 49/250 [14:06<1:01:49, 18.46s/it]\u001B[A\n",
      "films on page 11:  20%|██        | 50/250 [14:29<1:06:10, 19.85s/it]\u001B[A\n",
      "films on page 11:  20%|██        | 51/250 [14:46<1:03:43, 19.22s/it]\u001B[A\n",
      "films on page 11:  21%|██        | 52/250 [15:10<1:07:26, 20.44s/it]\u001B[A\n",
      "films on page 11:  21%|██        | 53/250 [15:28<1:05:26, 19.93s/it]\u001B[A\n",
      "films on page 11:  22%|██▏       | 54/250 [15:45<1:02:06, 19.01s/it]\u001B[A\n",
      "films on page 11:  22%|██▏       | 55/250 [16:06<1:03:49, 19.64s/it]\u001B[A\n",
      "films on page 11:  22%|██▏       | 56/250 [16:28<1:05:07, 20.14s/it]\u001B[A\n",
      "films on page 11:  23%|██▎       | 57/250 [16:44<1:01:13, 19.03s/it]\u001B[A\n",
      "films on page 11:  23%|██▎       | 58/250 [16:57<54:52, 17.15s/it]  \u001B[A\n",
      "films on page 11:  24%|██▎       | 59/250 [17:12<52:40, 16.55s/it]\u001B[A\n",
      "films on page 11:  24%|██▍       | 60/250 [17:31<54:26, 17.19s/it]\u001B[A\n",
      "films on page 11:  24%|██▍       | 61/250 [17:51<57:28, 18.25s/it]\u001B[A\n",
      "films on page 11:  25%|██▍       | 62/250 [18:13<1:00:14, 19.23s/it]\u001B[A\n",
      "films on page 11:  25%|██▌       | 63/250 [18:32<59:25, 19.07s/it]  \u001B[A\n",
      "films on page 11:  26%|██▌       | 64/250 [18:49<57:31, 18.55s/it]\u001B[A\n",
      "films on page 11:  26%|██▌       | 65/250 [19:07<56:21, 18.28s/it]\u001B[A\n",
      "films on page 11:  26%|██▋       | 66/250 [19:24<54:50, 17.88s/it]\u001B[A\n",
      "films on page 11:  27%|██▋       | 67/250 [19:41<53:57, 17.69s/it]\u001B[AWARNING:root:film tt2113681\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E246CD00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E246CD00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E246CD00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E246CD00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 11:  27%|██▋       | 68/250 [20:01<56:01, 18.47s/it]\u001B[A\n",
      "films on page 11:  28%|██▊       | 69/250 [20:16<52:31, 17.41s/it]\u001B[A\n",
      "films on page 11:  28%|██▊       | 70/250 [20:33<52:11, 17.40s/it]\u001B[A\n",
      "films on page 11:  28%|██▊       | 71/250 [20:48<49:26, 16.57s/it]\u001B[A\n",
      "films on page 11:  29%|██▉       | 72/250 [21:05<49:10, 16.57s/it]\u001B[A\n",
      "films on page 11:  29%|██▉       | 73/250 [21:19<46:58, 15.92s/it]\u001B[A\n",
      "films on page 11:  30%|██▉       | 74/250 [21:33<44:39, 15.22s/it]\u001B[A\n",
      "films on page 11:  30%|███       | 75/250 [21:46<43:04, 14.77s/it]\u001B[A\n",
      "films on page 11:  30%|███       | 76/250 [22:01<43:03, 14.85s/it]\u001B[A\n",
      "films on page 11:  31%|███       | 77/250 [22:20<46:14, 16.04s/it]\u001B[A\n",
      "films on page 11:  31%|███       | 78/250 [22:36<45:54, 16.02s/it]\u001B[A\n",
      "films on page 11:  32%|███▏      | 79/250 [22:52<45:44, 16.05s/it]\u001B[A\n",
      "films on page 11:  32%|███▏      | 80/250 [23:21<55:55, 19.74s/it]\u001B[A\n",
      "films on page 11:  32%|███▏      | 81/250 [23:41<55:39, 19.76s/it]\u001B[A\n",
      "films on page 11:  33%|███▎      | 82/250 [23:56<51:58, 18.56s/it]\u001B[A\n",
      "films on page 11:  33%|███▎      | 83/250 [24:13<50:21, 18.09s/it]\u001B[A\n",
      "films on page 11:  34%|███▎      | 84/250 [24:31<49:21, 17.84s/it]\u001B[A\n",
      "films on page 11:  34%|███▍      | 85/250 [24:56<55:14, 20.08s/it]\u001B[A\n",
      "films on page 11:  34%|███▍      | 86/250 [25:17<55:52, 20.44s/it]\u001B[A\n",
      "films on page 11:  35%|███▍      | 87/250 [25:38<55:59, 20.61s/it]\u001B[A\n",
      "films on page 11:  35%|███▌      | 88/250 [26:00<56:38, 20.98s/it]\u001B[A\n",
      "films on page 11:  36%|███▌      | 89/250 [26:18<53:34, 19.96s/it]\u001B[A\n",
      "films on page 11:  36%|███▌      | 90/250 [26:38<53:51, 20.20s/it]\u001B[A\n",
      "films on page 11:  36%|███▋      | 91/250 [26:57<52:42, 19.89s/it]\u001B[A\n",
      "films on page 11:  37%|███▋      | 92/250 [27:17<51:48, 19.68s/it]\u001B[A\n",
      "films on page 11:  37%|███▋      | 93/250 [27:36<51:23, 19.64s/it]\u001B[A\n",
      "films on page 11:  38%|███▊      | 94/250 [27:58<52:21, 20.14s/it]\u001B[A\n",
      "films on page 11:  38%|███▊      | 95/250 [28:16<50:43, 19.63s/it]\u001B[A\n",
      "films on page 11:  38%|███▊      | 96/250 [28:33<48:03, 18.73s/it]\u001B[A\n",
      "films on page 11:  39%|███▉      | 97/250 [28:49<46:01, 18.05s/it]\u001B[A\n",
      "films on page 11:  39%|███▉      | 98/250 [29:11<49:04, 19.37s/it]\u001B[A\n",
      "films on page 11:  40%|███▉      | 99/250 [29:38<54:03, 21.48s/it]\u001B[A\n",
      "films on page 11:  40%|████      | 100/250 [29:55<50:23, 20.16s/it]\u001B[A\n",
      "films on page 11:  40%|████      | 101/250 [30:17<51:25, 20.71s/it]\u001B[A\n",
      "films on page 11:  41%|████      | 102/250 [30:34<48:04, 19.49s/it]\u001B[A\n",
      "films on page 11:  41%|████      | 103/250 [30:52<47:03, 19.20s/it]\u001B[A\n",
      "films on page 11:  42%|████▏     | 104/250 [31:11<46:40, 19.18s/it]\u001B[A\n",
      "films on page 11:  42%|████▏     | 105/250 [31:30<46:14, 19.14s/it]\u001B[A\n",
      "films on page 11:  42%|████▏     | 106/250 [31:47<43:58, 18.32s/it]\u001B[A\n",
      "films on page 11:  43%|████▎     | 107/250 [32:04<43:13, 18.14s/it]\u001B[A\n",
      "films on page 11:  43%|████▎     | 108/250 [32:20<41:17, 17.45s/it]\u001B[A\n",
      "films on page 11:  44%|████▎     | 109/250 [32:37<40:41, 17.31s/it]\u001B[A\n",
      "films on page 11:  44%|████▍     | 110/250 [33:02<45:15, 19.40s/it]\u001B[A\n",
      "films on page 11:  44%|████▍     | 111/250 [33:21<45:02, 19.44s/it]\u001B[A\n",
      "films on page 11:  45%|████▍     | 112/250 [33:41<45:09, 19.63s/it]\u001B[A\n",
      "films on page 11:  45%|████▌     | 113/250 [34:03<46:40, 20.44s/it]\u001B[A\n",
      "films on page 11:  46%|████▌     | 114/250 [34:18<42:17, 18.66s/it]\u001B[A\n",
      "films on page 11:  46%|████▌     | 115/250 [34:34<39:53, 17.73s/it]\u001B[A\n",
      "films on page 11:  46%|████▋     | 116/250 [34:47<36:53, 16.52s/it]\u001B[A\n",
      "films on page 11:  47%|████▋     | 117/250 [35:03<36:14, 16.35s/it]\u001B[A\n",
      "films on page 11:  47%|████▋     | 118/250 [35:27<40:44, 18.52s/it]\u001B[A\n",
      "films on page 11:  48%|████▊     | 119/250 [35:47<41:44, 19.12s/it]\u001B[A\n",
      "films on page 11:  48%|████▊     | 120/250 [36:05<40:25, 18.66s/it]\u001B[A\n",
      "films on page 11:  48%|████▊     | 121/250 [36:23<39:55, 18.57s/it]\u001B[A\n",
      "films on page 11:  49%|████▉     | 122/250 [36:37<36:30, 17.11s/it]\u001B[A\n",
      "films on page 11:  49%|████▉     | 123/250 [36:52<34:46, 16.43s/it]\u001B[A\n",
      "films on page 11:  50%|████▉     | 124/250 [37:18<40:30, 19.29s/it]\u001B[A\n",
      "films on page 11:  50%|█████     | 125/250 [37:33<37:56, 18.22s/it]\u001B[A\n",
      "films on page 11:  50%|█████     | 126/250 [37:56<40:26, 19.57s/it]\u001B[A\n",
      "films on page 11:  51%|█████     | 127/250 [38:17<40:49, 19.91s/it]\u001B[A\n",
      "films on page 11:  51%|█████     | 128/250 [38:33<38:27, 18.91s/it]\u001B[A\n",
      "films on page 11:  52%|█████▏    | 129/250 [38:52<37:49, 18.76s/it]\u001B[A\n",
      "films on page 11:  52%|█████▏    | 130/250 [39:10<36:55, 18.46s/it]\u001B[A\n",
      "films on page 11:  52%|█████▏    | 131/250 [39:26<35:28, 17.89s/it]\u001B[A\n",
      "films on page 11:  53%|█████▎    | 132/250 [39:47<36:43, 18.68s/it]\u001B[A\n",
      "films on page 11:  53%|█████▎    | 133/250 [40:01<33:54, 17.39s/it]\u001B[A\n",
      "films on page 11:  54%|█████▎    | 134/250 [40:22<35:27, 18.34s/it]\u001B[A\n",
      "films on page 11:  54%|█████▍    | 135/250 [40:39<34:20, 17.91s/it]\u001B[A\n",
      "films on page 11:  54%|█████▍    | 136/250 [40:57<34:07, 17.96s/it]\u001B[A\n",
      "films on page 11:  55%|█████▍    | 137/250 [41:12<32:31, 17.27s/it]\u001B[A\n",
      "films on page 11:  55%|█████▌    | 138/250 [41:29<31:43, 17.00s/it]\u001B[AWARNING:root:film tt1244668\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E2E0F250>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E2E0F250>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E2E0F250>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E2E0F250>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 11:  56%|█████▌    | 139/250 [41:40<28:27, 15.38s/it]\u001B[A\n",
      "films on page 11:  56%|█████▌    | 140/250 [42:01<31:07, 16.98s/it]\u001B[A\n",
      "films on page 11:  56%|█████▋    | 141/250 [42:23<33:35, 18.49s/it]\u001B[A\n",
      "films on page 11:  57%|█████▋    | 142/250 [42:43<33:59, 18.89s/it]\u001B[A\n",
      "films on page 11:  57%|█████▋    | 143/250 [43:00<32:40, 18.32s/it]\u001B[A\n",
      "films on page 11:  58%|█████▊    | 144/250 [43:19<33:04, 18.72s/it]\u001B[A\n",
      "films on page 11:  58%|█████▊    | 145/250 [43:34<30:24, 17.37s/it]\u001B[A\n",
      "films on page 11:  58%|█████▊    | 146/250 [43:54<31:28, 18.16s/it]\u001B[A\n",
      "films on page 11:  59%|█████▉    | 147/250 [44:13<31:36, 18.41s/it]\u001B[AWARNING:root:film tt0344854\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CF365DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143CF365DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CF365DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CF365DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 11:  59%|█████▉    | 148/250 [44:26<28:55, 17.02s/it]\u001B[A\n",
      "films on page 11:  60%|█████▉    | 149/250 [44:51<32:19, 19.20s/it]\u001B[A\n",
      "films on page 11:  60%|██████    | 150/250 [45:16<35:00, 21.01s/it]\u001B[A\n",
      "films on page 11:  60%|██████    | 151/250 [45:33<32:44, 19.84s/it]\u001B[A\n",
      "films on page 11:  61%|██████    | 152/250 [45:50<31:07, 19.06s/it]\u001B[A\n",
      "films on page 11:  61%|██████    | 153/250 [46:07<29:35, 18.30s/it]\u001B[A\n",
      "films on page 11:  62%|██████▏   | 154/250 [46:24<28:50, 18.03s/it]\u001B[A\n",
      "films on page 11:  62%|██████▏   | 155/250 [46:40<27:18, 17.24s/it]\u001B[A\n",
      "films on page 11:  62%|██████▏   | 156/250 [46:55<26:01, 16.61s/it]\u001B[A\n",
      "films on page 11:  63%|██████▎   | 157/250 [47:12<26:08, 16.86s/it]\u001B[A\n",
      "films on page 11:  63%|██████▎   | 158/250 [47:27<24:55, 16.26s/it]\u001B[A\n",
      "films on page 11:  64%|██████▎   | 159/250 [47:46<25:56, 17.11s/it]\u001B[A\n",
      "films on page 11:  64%|██████▍   | 160/250 [48:01<24:39, 16.44s/it]\u001B[A\n",
      "films on page 11:  64%|██████▍   | 161/250 [48:20<25:29, 17.18s/it]\u001B[A\n",
      "films on page 11:  65%|██████▍   | 162/250 [48:39<25:51, 17.63s/it]\u001B[A\n",
      "films on page 11:  65%|██████▌   | 163/250 [48:59<26:31, 18.29s/it]\u001B[A\n",
      "films on page 11:  66%|██████▌   | 164/250 [49:14<25:12, 17.59s/it]\u001B[A\n",
      "films on page 11:  66%|██████▌   | 165/250 [49:32<24:56, 17.61s/it]\u001B[A\n",
      "films on page 11:  66%|██████▋   | 166/250 [49:48<23:44, 16.96s/it]\u001B[A\n",
      "films on page 11:  67%|██████▋   | 167/250 [50:05<23:31, 17.01s/it]\u001B[A\n",
      "films on page 11:  67%|██████▋   | 168/250 [50:24<24:02, 17.59s/it]\u001B[A\n",
      "films on page 11:  68%|██████▊   | 169/250 [50:41<23:37, 17.50s/it]\u001B[A\n",
      "films on page 11:  68%|██████▊   | 170/250 [51:01<24:14, 18.18s/it]\u001B[A\n",
      "films on page 11:  68%|██████▊   | 171/250 [51:19<23:54, 18.15s/it]\u001B[A\n",
      "films on page 11:  69%|██████▉   | 172/250 [51:39<24:16, 18.67s/it]\u001B[A\n",
      "films on page 11:  69%|██████▉   | 173/250 [51:56<23:25, 18.25s/it]\u001B[A\n",
      "films on page 11:  70%|██████▉   | 174/250 [52:12<22:12, 17.53s/it]\u001B[A\n",
      "films on page 11:  70%|███████   | 175/250 [52:31<22:37, 18.10s/it]\u001B[A\n",
      "films on page 11:  70%|███████   | 176/250 [52:50<22:35, 18.32s/it]\u001B[AWARNING:root:film tt1251757\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C992FE20>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143C992FE20>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C992FE20>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C992FE20>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 11:  71%|███████   | 177/250 [53:03<20:20, 16.72s/it]\u001B[A\n",
      "films on page 11:  71%|███████   | 178/250 [53:19<19:57, 16.64s/it]\u001B[A\n",
      "films on page 11:  72%|███████▏  | 179/250 [53:36<19:40, 16.63s/it]\u001B[A\n",
      "films on page 11:  72%|███████▏  | 180/250 [53:53<19:29, 16.71s/it]\u001B[A\n",
      "films on page 11:  72%|███████▏  | 181/250 [54:09<18:52, 16.42s/it]\u001B[A\n",
      "films on page 11:  73%|███████▎  | 182/250 [54:25<18:43, 16.52s/it]\u001B[A\n",
      "films on page 11:  73%|███████▎  | 183/250 [54:41<18:07, 16.24s/it]\u001B[A\n",
      "films on page 11:  74%|███████▎  | 184/250 [54:58<18:09, 16.51s/it]\u001B[A\n",
      "films on page 11:  74%|███████▍  | 185/250 [55:16<18:09, 16.77s/it]\u001B[AWARNING:root:film tt6697582\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C992FD60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143C992FD60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C992FD60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C992FD60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 11:  74%|███████▍  | 186/250 [55:29<16:55, 15.87s/it]\u001B[AWARNING:root:film tt0270053\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CC31E2B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143CC31E2B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CC31E2B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CC31E2B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 11:  75%|███████▍  | 187/250 [55:36<13:47, 13.14s/it]\u001B[A\n",
      "films on page 11:  75%|███████▌  | 188/250 [55:55<15:22, 14.87s/it]\u001B[A\n",
      "films on page 11:  76%|███████▌  | 189/250 [56:14<16:30, 16.24s/it]\u001B[A\n",
      "films on page 11:  76%|███████▌  | 190/250 [56:34<17:18, 17.31s/it]\u001B[A\n",
      "films on page 11:  76%|███████▋  | 191/250 [56:51<16:55, 17.21s/it]\u001B[A\n",
      "films on page 11:  77%|███████▋  | 192/250 [57:05<15:36, 16.14s/it]\u001B[A\n",
      "films on page 11:  77%|███████▋  | 193/250 [57:24<16:13, 17.07s/it]\u001B[A\n",
      "films on page 11:  78%|███████▊  | 194/250 [57:41<15:58, 17.12s/it]\u001B[A\n",
      "films on page 11:  78%|███████▊  | 195/250 [57:59<15:50, 17.27s/it]\u001B[A\n",
      "films on page 11:  78%|███████▊  | 196/250 [58:17<15:49, 17.58s/it]\u001B[A\n",
      "films on page 11:  79%|███████▉  | 197/250 [58:36<15:52, 17.98s/it]\u001B[A\n",
      "films on page 11:  79%|███████▉  | 198/250 [59:04<18:04, 20.85s/it]\u001B[A\n",
      "films on page 11:  80%|███████▉  | 199/250 [59:19<16:23, 19.29s/it]\u001B[A\n",
      "films on page 11:  80%|████████  | 200/250 [59:38<15:48, 18.97s/it]\u001B[A\n",
      "films on page 11:  80%|████████  | 201/250 [59:54<14:48, 18.14s/it]\u001B[A\n",
      "films on page 11:  81%|████████  | 202/250 [1:00:09<13:46, 17.22s/it]\u001B[A\n",
      "films on page 11:  81%|████████  | 203/250 [1:00:29<14:05, 17.98s/it]\u001B[A\n",
      "films on page 11:  82%|████████▏ | 204/250 [1:00:46<13:32, 17.66s/it]\u001B[A\n",
      "films on page 11:  82%|████████▏ | 205/250 [1:01:02<12:53, 17.19s/it]\u001B[A\n",
      "films on page 11:  82%|████████▏ | 206/250 [1:01:18<12:19, 16.81s/it]\u001B[A\n",
      "films on page 11:  83%|████████▎ | 207/250 [1:01:31<11:15, 15.70s/it]\u001B[A\n",
      "films on page 11:  83%|████████▎ | 208/250 [1:01:47<11:04, 15.83s/it]\u001B[A\n",
      "films on page 11:  84%|████████▎ | 209/250 [1:02:04<11:05, 16.24s/it]\u001B[AWARNING:root:film tt1567448\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CC66B130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143CC66B130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CC66B130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CC66B130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 11:  84%|████████▍ | 210/250 [1:02:13<09:25, 14.13s/it]\u001B[A\n",
      "films on page 11:  84%|████████▍ | 211/250 [1:02:30<09:40, 14.88s/it]\u001B[A\n",
      "films on page 11:  85%|████████▍ | 212/250 [1:02:47<09:53, 15.61s/it]\u001B[A\n",
      "films on page 11:  85%|████████▌ | 213/250 [1:03:04<09:49, 15.94s/it]\u001B[A\n",
      "films on page 11:  86%|████████▌ | 214/250 [1:03:23<10:11, 16.99s/it]\u001B[A\n",
      "films on page 11:  86%|████████▌ | 215/250 [1:03:43<10:18, 17.67s/it]\u001B[A\n",
      "films on page 11:  86%|████████▋ | 216/250 [1:03:56<09:14, 16.30s/it]\u001B[AWARNING:root:film tt2752200\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C992FE50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143C992FE50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C992FE50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C992FE50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 11:  87%|████████▋ | 217/250 [1:04:06<08:02, 14.61s/it]\u001B[A\n",
      "films on page 11:  87%|████████▋ | 218/250 [1:04:24<08:12, 15.39s/it]\u001B[A\n",
      "films on page 11:  88%|████████▊ | 219/250 [1:04:42<08:21, 16.18s/it]\u001B[A\n",
      "films on page 11:  88%|████████▊ | 220/250 [1:04:57<07:56, 15.88s/it]\u001B[A\n",
      "films on page 11:  88%|████████▊ | 221/250 [1:05:13<07:42, 15.95s/it]\u001B[A\n",
      "films on page 11:  89%|████████▉ | 222/250 [1:05:30<07:36, 16.29s/it]\u001B[A\n",
      "films on page 11:  89%|████████▉ | 223/250 [1:05:45<07:13, 16.04s/it]\u001B[A\n",
      "films on page 11:  90%|████████▉ | 224/250 [1:06:06<07:28, 17.26s/it]\u001B[A\n",
      "films on page 11:  90%|█████████ | 225/250 [1:06:23<07:12, 17.29s/it]\u001B[A\n",
      "films on page 11:  90%|█████████ | 226/250 [1:06:43<07:14, 18.09s/it]\u001B[A\n",
      "films on page 11:  91%|█████████ | 227/250 [1:06:59<06:44, 17.57s/it]\u001B[A\n",
      "films on page 11:  91%|█████████ | 228/250 [1:07:25<07:18, 19.95s/it]\u001B[A\n",
      "films on page 11:  92%|█████████▏| 229/250 [1:07:43<06:48, 19.44s/it]\u001B[A\n",
      "films on page 11:  92%|█████████▏| 230/250 [1:08:00<06:17, 18.85s/it]\u001B[A\n",
      "films on page 11:  92%|█████████▏| 231/250 [1:08:16<05:42, 18.00s/it]\u001B[A\n",
      "films on page 11:  93%|█████████▎| 232/250 [1:08:34<05:23, 17.96s/it]\u001B[A\n",
      "films on page 11:  93%|█████████▎| 233/250 [1:08:51<04:58, 17.55s/it]\u001B[A\n",
      "films on page 11:  94%|█████████▎| 234/250 [1:09:10<04:49, 18.07s/it]\u001B[A\n",
      "films on page 11:  94%|█████████▍| 235/250 [1:09:29<04:35, 18.34s/it]\u001B[A\n",
      "films on page 11:  94%|█████████▍| 236/250 [1:09:48<04:17, 18.36s/it]\u001B[A\n",
      "films on page 11:  95%|█████████▍| 237/250 [1:10:04<03:52, 17.92s/it]\u001B[A\n",
      "films on page 11:  95%|█████████▌| 238/250 [1:10:26<03:46, 18.90s/it]\u001B[A\n",
      "films on page 11:  96%|█████████▌| 239/250 [1:10:47<03:36, 19.71s/it]\u001B[A\n",
      "films on page 11:  96%|█████████▌| 240/250 [1:11:06<03:14, 19.42s/it]\u001B[A\n",
      "films on page 11:  96%|█████████▋| 241/250 [1:11:24<02:50, 18.98s/it]\u001B[A\n",
      "films on page 11:  97%|█████████▋| 242/250 [1:11:43<02:32, 19.03s/it]\u001B[A\n",
      "films on page 11:  97%|█████████▋| 243/250 [1:11:57<02:03, 17.59s/it]\u001B[A\n",
      "films on page 11:  98%|█████████▊| 244/250 [1:12:13<01:41, 16.98s/it]\u001B[A\n",
      "films on page 11:  98%|█████████▊| 245/250 [1:12:32<01:27, 17.55s/it]\u001B[A\n",
      "films on page 11:  98%|█████████▊| 246/250 [1:12:48<01:08, 17.15s/it]\u001B[A\n",
      "films on page 11:  99%|█████████▉| 247/250 [1:13:07<00:52, 17.63s/it]\u001B[A\n",
      "films on page 11:  99%|█████████▉| 248/250 [1:13:28<00:37, 18.72s/it]\u001B[A\n",
      "films on page 11: 100%|█████████▉| 249/250 [1:13:48<00:19, 19.07s/it]\u001B[A\n",
      "films on page 11: 100%|██████████| 250/250 [1:14:15<00:00, 17.82s/it]\u001B[A\n",
      "pages:  60%|██████    | 6/10 [6:56:08<4:43:18, 4249.50s/it]\n",
      "films on page 12:   0%|          | 0/250 [00:00<?, ?it/s]\u001B[A\n",
      "films on page 12:   0%|          | 1/250 [00:19<1:20:11, 19.32s/it]\u001B[A\n",
      "films on page 12:   1%|          | 2/250 [00:44<1:35:09, 23.02s/it]\u001B[A\n",
      "films on page 12:   1%|          | 3/250 [01:00<1:19:53, 19.41s/it]\u001B[A\n",
      "films on page 12:   2%|▏         | 4/250 [01:19<1:19:16, 19.34s/it]\u001B[A\n",
      "films on page 12:   2%|▏         | 5/250 [01:45<1:29:13, 21.85s/it]\u001B[A\n",
      "films on page 12:   2%|▏         | 6/250 [02:01<1:20:10, 19.72s/it]\u001B[A\n",
      "films on page 12:   3%|▎         | 7/250 [02:17<1:15:16, 18.59s/it]\u001B[A\n",
      "films on page 12:   3%|▎         | 8/250 [02:39<1:19:15, 19.65s/it]\u001B[AWARNING:root:film tt1167638\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143B9319280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143B9319280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143B9319280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143B9319280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:   4%|▎         | 9/250 [02:52<1:10:51, 17.64s/it]\u001B[A\n",
      "films on page 12:   4%|▍         | 10/250 [03:09<1:09:24, 17.35s/it]\u001B[A\n",
      "films on page 12:   4%|▍         | 11/250 [03:26<1:09:23, 17.42s/it]\u001B[A\n",
      "films on page 12:   5%|▍         | 12/250 [03:46<1:11:48, 18.10s/it]\u001B[A\n",
      "films on page 12:   5%|▌         | 13/250 [04:06<1:13:46, 18.68s/it]\u001B[A\n",
      "films on page 12:   6%|▌         | 14/250 [04:29<1:18:15, 19.90s/it]\u001B[A\n",
      "films on page 12:   6%|▌         | 15/250 [04:53<1:22:42, 21.12s/it]\u001B[A\n",
      "films on page 12:   6%|▋         | 16/250 [05:07<1:14:56, 19.21s/it]\u001B[AWARNING:root:film tt1668191\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D7A72790>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143D7A72790>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D7A72790>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143D7A72790>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:   7%|▋         | 17/250 [05:19<1:05:13, 16.80s/it]\u001B[A\n",
      "films on page 12:   7%|▋         | 18/250 [05:37<1:07:16, 17.40s/it]\u001B[A\n",
      "films on page 12:   8%|▊         | 19/250 [06:05<1:18:35, 20.41s/it]\u001B[A\n",
      "films on page 12:   8%|▊         | 20/250 [06:24<1:16:36, 19.98s/it]\u001B[A\n",
      "films on page 12:   8%|▊         | 21/250 [06:52<1:25:20, 22.36s/it]\u001B[AWARNING:root:film tt1112782\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DBB1C280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143DBB1C280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DBB1C280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DBB1C280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:   9%|▉         | 22/250 [07:03<1:11:55, 18.93s/it]\u001B[A\n",
      "films on page 12:   9%|▉         | 23/250 [07:19<1:09:03, 18.25s/it]\u001B[A\n",
      "films on page 12:  10%|▉         | 24/250 [07:38<1:09:01, 18.32s/it]\u001B[A\n",
      "films on page 12:  10%|█         | 25/250 [07:54<1:06:33, 17.75s/it]\u001B[A\n",
      "films on page 12:  10%|█         | 26/250 [08:14<1:08:40, 18.39s/it]\u001B[A\n",
      "films on page 12:  11%|█         | 27/250 [08:29<1:04:22, 17.32s/it]\u001B[A\n",
      "films on page 12:  11%|█         | 28/250 [08:47<1:04:58, 17.56s/it]\u001B[A\n",
      "films on page 12:  12%|█▏        | 29/250 [09:07<1:06:53, 18.16s/it]\u001B[A\n",
      "films on page 12:  12%|█▏        | 30/250 [09:24<1:05:10, 17.77s/it]\u001B[A\n",
      "films on page 12:  12%|█▏        | 31/250 [09:41<1:04:52, 17.78s/it]\u001B[A\n",
      "films on page 12:  13%|█▎        | 32/250 [09:59<1:04:06, 17.65s/it]\u001B[A\n",
      "films on page 12:  13%|█▎        | 33/250 [10:16<1:03:33, 17.57s/it]\u001B[A\n",
      "films on page 12:  14%|█▎        | 34/250 [10:35<1:04:33, 17.93s/it]\u001B[AWARNING:root:film tt3253930\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DBCB5280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143DBCB5280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DBCB5280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DBCB5280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  14%|█▍        | 35/250 [10:49<1:00:39, 16.93s/it]\u001B[A\n",
      "films on page 12:  14%|█▍        | 36/250 [11:07<1:01:30, 17.24s/it]\u001B[A\n",
      "films on page 12:  15%|█▍        | 37/250 [11:23<59:04, 16.64s/it]  \u001B[A\n",
      "films on page 12:  15%|█▌        | 38/250 [11:40<59:43, 16.90s/it]\u001B[A\n",
      "films on page 12:  16%|█▌        | 39/250 [11:56<58:12, 16.55s/it]\u001B[A\n",
      "films on page 12:  16%|█▌        | 40/250 [12:15<1:00:50, 17.38s/it]\u001B[A\n",
      "films on page 12:  16%|█▋        | 41/250 [12:31<59:16, 17.02s/it]  \u001B[AWARNING:root:film tt3698408\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143B4B52520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143B4B52520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143B4B52520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143B4B52520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  17%|█▋        | 42/250 [12:39<49:21, 14.24s/it]\u001B[A\n",
      "films on page 12:  17%|█▋        | 43/250 [12:58<54:13, 15.72s/it]\u001B[A\n",
      "films on page 12:  18%|█▊        | 44/250 [13:14<54:14, 15.80s/it]\u001B[A\n",
      "films on page 12:  18%|█▊        | 45/250 [13:31<54:48, 16.04s/it]\u001B[A\n",
      "films on page 12:  18%|█▊        | 46/250 [13:49<57:05, 16.79s/it]\u001B[A\n",
      "films on page 12:  19%|█▉        | 47/250 [14:10<1:00:56, 18.01s/it]\u001B[A\n",
      "films on page 12:  19%|█▉        | 48/250 [14:23<54:59, 16.33s/it]  \u001B[A\n",
      "films on page 12:  20%|█▉        | 49/250 [14:45<1:01:06, 18.24s/it]\u001B[A\n",
      "films on page 12:  20%|██        | 50/250 [15:05<1:02:21, 18.71s/it]\u001B[A\n",
      "films on page 12:  20%|██        | 51/250 [15:24<1:02:36, 18.88s/it]\u001B[AWARNING:root:film tt1937264\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DC27B280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143DC27B280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DC27B280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DC27B280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  21%|██        | 52/250 [15:37<56:03, 16.99s/it]  \u001B[A\n",
      "films on page 12:  21%|██        | 53/250 [15:54<55:16, 16.84s/it]\u001B[A\n",
      "films on page 12:  22%|██▏       | 54/250 [16:13<57:31, 17.61s/it]\u001B[A\n",
      "films on page 12:  22%|██▏       | 55/250 [16:28<54:27, 16.75s/it]\u001B[A\n",
      "films on page 12:  22%|██▏       | 56/250 [16:43<52:19, 16.18s/it]\u001B[A\n",
      "films on page 12:  23%|██▎       | 57/250 [16:57<50:42, 15.77s/it]\u001B[AWARNING:root:film tt1259014\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DC331280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143DC331280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DC331280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DC331280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  23%|██▎       | 58/250 [17:11<48:18, 15.10s/it]\u001B[A\n",
      "films on page 12:  24%|██▎       | 59/250 [17:28<50:01, 15.71s/it]\u001B[A\n",
      "films on page 12:  24%|██▍       | 60/250 [17:46<52:19, 16.52s/it]\u001B[A\n",
      "films on page 12:  24%|██▍       | 61/250 [18:03<52:01, 16.52s/it]\u001B[A\n",
      "films on page 12:  25%|██▍       | 62/250 [18:20<52:13, 16.67s/it]\u001B[A\n",
      "films on page 12:  25%|██▌       | 63/250 [18:40<54:51, 17.60s/it]\u001B[A\n",
      "films on page 12:  26%|██▌       | 64/250 [18:55<52:09, 16.83s/it]\u001B[A\n",
      "films on page 12:  26%|██▌       | 65/250 [19:15<54:46, 17.77s/it]\u001B[A\n",
      "films on page 12:  26%|██▋       | 66/250 [19:32<53:54, 17.58s/it]\u001B[A\n",
      "films on page 12:  27%|██▋       | 67/250 [19:46<50:51, 16.68s/it]\u001B[AWARNING:root:film tt1171701\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DC4BA280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143DC4BA280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DC4BA280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DC4BA280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  27%|██▋       | 68/250 [19:59<47:04, 15.52s/it]\u001B[AWARNING:root:film tt0374569\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DC4AC520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143DC4AC520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DC4AC520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DC4AC520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  28%|██▊       | 69/250 [20:16<48:13, 15.99s/it]\u001B[AWARNING:root:film tt0470883\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143BD91F4C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143BD91F4C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143BD91F4C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143BD91F4C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  28%|██▊       | 70/250 [20:24<40:23, 13.46s/it]\u001B[A\n",
      "films on page 12:  28%|██▊       | 71/250 [20:43<45:08, 15.13s/it]\u001B[A\n",
      "films on page 12:  29%|██▉       | 72/250 [20:59<45:33, 15.36s/it]\u001B[A\n",
      "films on page 12:  29%|██▉       | 73/250 [21:15<46:10, 15.65s/it]\u001B[A\n",
      "films on page 12:  30%|██▉       | 74/250 [21:33<47:51, 16.32s/it]\u001B[A\n",
      "films on page 12:  30%|███       | 75/250 [21:50<47:46, 16.38s/it]\u001B[A\n",
      "films on page 12:  30%|███       | 76/250 [22:08<49:39, 17.12s/it]\u001B[A\n",
      "films on page 12:  31%|███       | 77/250 [22:26<49:37, 17.21s/it]\u001B[A\n",
      "films on page 12:  31%|███       | 78/250 [22:43<49:40, 17.33s/it]\u001B[A\n",
      "films on page 12:  32%|███▏      | 79/250 [23:01<49:59, 17.54s/it]\u001B[A\n",
      "films on page 12:  32%|███▏      | 80/250 [23:17<47:47, 16.87s/it]\u001B[A\n",
      "films on page 12:  32%|███▏      | 81/250 [23:30<44:36, 15.84s/it]\u001B[A\n",
      "films on page 12:  33%|███▎      | 82/250 [23:45<43:14, 15.44s/it]\u001B[A\n",
      "films on page 12:  33%|███▎      | 83/250 [24:02<44:40, 16.05s/it]\u001B[A\n",
      "films on page 12:  34%|███▎      | 84/250 [24:21<46:44, 16.90s/it]\u001B[A\n",
      "films on page 12:  34%|███▍      | 85/250 [24:37<46:02, 16.74s/it]\u001B[A\n",
      "films on page 12:  34%|███▍      | 86/250 [24:54<45:43, 16.73s/it]\u001B[A\n",
      "films on page 12:  35%|███▍      | 87/250 [25:11<45:51, 16.88s/it]\u001B[A\n",
      "films on page 12:  35%|███▌      | 88/250 [25:29<45:55, 17.01s/it]\u001B[A\n",
      "films on page 12:  36%|███▌      | 89/250 [25:47<46:54, 17.48s/it]\u001B[A\n",
      "films on page 12:  36%|███▌      | 90/250 [26:03<45:35, 17.10s/it]\u001B[A\n",
      "films on page 12:  36%|███▋      | 91/250 [26:17<42:42, 16.12s/it]\u001B[A\n",
      "films on page 12:  37%|███▋      | 92/250 [26:33<42:03, 15.97s/it]\u001B[A\n",
      "films on page 12:  37%|███▋      | 93/250 [26:48<40:56, 15.65s/it]\u001B[A\n",
      "films on page 12:  38%|███▊      | 94/250 [27:07<43:41, 16.81s/it]\u001B[A\n",
      "films on page 12:  38%|███▊      | 95/250 [27:25<44:27, 17.21s/it]\u001B[A\n",
      "films on page 12:  38%|███▊      | 96/250 [27:40<41:48, 16.29s/it]\u001B[A\n",
      "films on page 12:  39%|███▉      | 97/250 [28:05<48:39, 19.08s/it]\u001B[A\n",
      "films on page 12:  39%|███▉      | 98/250 [28:21<46:07, 18.21s/it]\u001B[AWARNING:root:film tt3792960\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E4E9F280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E4E9F280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E4E9F280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E4E9F280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  40%|███▉      | 99/250 [28:35<42:31, 16.90s/it]\u001B[A\n",
      "films on page 12:  40%|████      | 100/250 [28:59<47:12, 18.88s/it]\u001B[A\n",
      "films on page 12:  40%|████      | 101/250 [29:15<45:04, 18.15s/it]\u001B[A\n",
      "films on page 12:  41%|████      | 102/250 [29:37<47:38, 19.32s/it]\u001B[A\n",
      "films on page 12:  41%|████      | 103/250 [30:00<49:59, 20.41s/it]\u001B[A\n",
      "films on page 12:  42%|████▏     | 104/250 [30:21<49:56, 20.52s/it]\u001B[A\n",
      "films on page 12:  42%|████▏     | 105/250 [30:40<48:49, 20.20s/it]\u001B[A\n",
      "films on page 12:  42%|████▏     | 106/250 [30:58<46:58, 19.57s/it]\u001B[A\n",
      "films on page 12:  43%|████▎     | 107/250 [31:19<47:26, 19.91s/it]\u001B[A\n",
      "films on page 12:  43%|████▎     | 108/250 [31:36<44:54, 18.97s/it]\u001B[A\n",
      "films on page 12:  44%|████▎     | 109/250 [31:53<43:30, 18.52s/it]\u001B[AWARNING:root:film tt0827503\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143B4B52B20>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143B4B52B20>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143B4B52B20>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143B4B52B20>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  44%|████▍     | 110/250 [32:02<36:22, 15.59s/it]\u001B[AWARNING:root:film tt1541160\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E4FF2280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E4FF2280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E4FF2280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E4FF2280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  44%|████▍     | 111/250 [32:16<35:04, 15.14s/it]\u001B[A\n",
      "films on page 12:  45%|████▍     | 112/250 [32:34<36:40, 15.95s/it]\u001B[A\n",
      "films on page 12:  45%|████▌     | 113/250 [32:51<37:01, 16.22s/it]\u001B[A\n",
      "films on page 12:  46%|████▌     | 114/250 [33:11<39:36, 17.47s/it]\u001B[A\n",
      "films on page 12:  46%|████▌     | 115/250 [33:28<38:55, 17.30s/it]\u001B[A\n",
      "films on page 12:  46%|████▋     | 116/250 [33:47<39:31, 17.70s/it]\u001B[AWARNING:root:film tt0281322\n",
      "ERROR:root:('Connection aborted.', ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 419, in connect\n",
      "    self.sock = ssl_wrap_socket(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 449, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 493, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 501, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 1041, in _create\n",
      "    self.do_handshake()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 1310, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ConnectionResetError: [WinError 10054] Удаленный хост принудительно разорвал существующее подключение\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\packages\\six.py\", line 769, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 419, in connect\n",
      "    self.sock = ssl_wrap_socket(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 449, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 493, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 501, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 1041, in _create\n",
      "    self.do_handshake()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\ssl.py\", line 1310, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 67, in <module>\n",
      "    result_film = requests.get(API_URL + f'/title/{title_id}')\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 502, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(10054, 'Удаленный хост принудительно разорвал существующее подключение', None, 10054, None))\n",
      "\n",
      "films on page 12:  47%|████▋     | 117/250 [34:03<38:23, 17.32s/it]\u001B[A\n",
      "films on page 12:  47%|████▋     | 118/250 [34:23<39:23, 17.91s/it]\u001B[A\n",
      "films on page 12:  48%|████▊     | 119/250 [34:39<38:03, 17.43s/it]\u001B[A\n",
      "films on page 12:  48%|████▊     | 120/250 [34:56<37:51, 17.48s/it]\u001B[A\n",
      "films on page 12:  48%|████▊     | 121/250 [35:16<38:55, 18.10s/it]\u001B[A\n",
      "films on page 12:  49%|████▉     | 122/250 [35:33<37:47, 17.72s/it]\u001B[A\n",
      "films on page 12:  49%|████▉     | 123/250 [35:50<36:56, 17.45s/it]\u001B[AWARNING:root:film tt1964624\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E5172280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E5172280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E5172280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E5172280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  50%|████▉     | 124/250 [36:03<33:55, 16.15s/it]\u001B[A\n",
      "films on page 12:  50%|█████     | 125/250 [36:21<34:48, 16.71s/it]\u001B[A\n",
      "films on page 12:  50%|█████     | 126/250 [36:37<34:30, 16.69s/it]\u001B[A\n",
      "films on page 12:  51%|█████     | 127/250 [36:56<35:16, 17.21s/it]\u001B[A\n",
      "films on page 12:  51%|█████     | 128/250 [37:12<34:12, 16.82s/it]\u001B[A\n",
      "films on page 12:  52%|█████▏    | 129/250 [37:31<35:13, 17.47s/it]\u001B[A\n",
      "films on page 12:  52%|█████▏    | 130/250 [37:49<35:07, 17.56s/it]\u001B[AWARNING:root:film tt0460745\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E523E280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E523E280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E523E280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E523E280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  52%|█████▏    | 131/250 [38:02<32:12, 16.24s/it]\u001B[A\n",
      "films on page 12:  53%|█████▎    | 132/250 [38:21<33:29, 17.03s/it]\u001B[A\n",
      "films on page 12:  53%|█████▎    | 133/250 [38:38<33:42, 17.29s/it]\u001B[A\n",
      "films on page 12:  54%|█████▎    | 134/250 [38:58<34:57, 18.08s/it]\u001B[A\n",
      "films on page 12:  54%|█████▍    | 135/250 [39:14<33:02, 17.24s/it]\u001B[AWARNING:root:film tt4326444\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E52CC280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E52CC280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E52CC280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E52CC280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  54%|█████▍    | 136/250 [39:26<29:54, 15.74s/it]\u001B[A\n",
      "films on page 12:  55%|█████▍    | 137/250 [39:43<30:39, 16.28s/it]\u001B[A\n",
      "films on page 12:  55%|█████▌    | 138/250 [39:57<29:02, 15.56s/it]\u001B[A\n",
      "films on page 12:  56%|█████▌    | 139/250 [40:14<29:11, 15.78s/it]\u001B[A\n",
      "films on page 12:  56%|█████▌    | 140/250 [40:29<28:42, 15.66s/it]\u001B[A\n",
      "films on page 12:  56%|█████▋    | 141/250 [40:44<28:14, 15.55s/it]\u001B[A\n",
      "films on page 12:  57%|█████▋    | 142/250 [41:02<28:59, 16.10s/it]\u001B[A\n",
      "films on page 12:  57%|█████▋    | 143/250 [41:24<31:49, 17.85s/it]\u001B[A\n",
      "films on page 12:  58%|█████▊    | 144/250 [41:43<32:12, 18.23s/it]\u001B[A\n",
      "films on page 12:  58%|█████▊    | 145/250 [42:01<31:41, 18.11s/it]\u001B[AWARNING:root:film tt0880502\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E5403280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E5403280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E5403280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E5403280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  58%|█████▊    | 146/250 [42:14<28:52, 16.66s/it]\u001B[A\n",
      "films on page 12:  59%|█████▉    | 147/250 [42:31<28:39, 16.69s/it]\u001B[A\n",
      "films on page 12:  59%|█████▉    | 148/250 [42:48<28:57, 17.04s/it]\u001B[A\n",
      "films on page 12:  60%|█████▉    | 149/250 [43:05<28:14, 16.78s/it]\u001B[AWARNING:root:film tt0482088\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E546A280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E546A280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E546A280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E546A280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  60%|██████    | 150/250 [43:19<26:55, 16.16s/it]\u001B[A\n",
      "films on page 12:  60%|██████    | 151/250 [43:34<25:41, 15.57s/it]\u001B[A\n",
      "films on page 12:  61%|██████    | 152/250 [43:51<26:33, 16.26s/it]\u001B[A\n",
      "films on page 12:  61%|██████    | 153/250 [44:08<26:20, 16.30s/it]\u001B[A\n",
      "films on page 12:  62%|██████▏   | 154/250 [44:27<27:23, 17.12s/it]\u001B[AWARNING:root:film tt1641638\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E54E5790>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E54E5790>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E54E5790>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E54E5790>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  62%|██████▏   | 155/250 [44:39<24:52, 15.71s/it]\u001B[A\n",
      "films on page 12:  62%|██████▏   | 156/250 [44:56<24:57, 15.93s/it]\u001B[A\n",
      "films on page 12:  63%|██████▎   | 157/250 [45:13<25:08, 16.22s/it]\u001B[A\n",
      "films on page 12:  63%|██████▎   | 158/250 [45:29<25:06, 16.37s/it]\u001B[AWARNING:root:film tt1435513\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E5557280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E5557280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E5557280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E5557280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  64%|██████▎   | 159/250 [45:39<21:56, 14.47s/it]\u001B[A\n",
      "films on page 12:  64%|██████▍   | 160/250 [45:53<21:23, 14.26s/it]\u001B[A\n",
      "films on page 12:  64%|██████▍   | 161/250 [46:08<21:29, 14.49s/it]\u001B[A\n",
      "films on page 12:  65%|██████▍   | 162/250 [46:20<20:12, 13.78s/it]\u001B[A\n",
      "films on page 12:  65%|██████▌   | 163/250 [46:35<20:32, 14.16s/it]\u001B[A\n",
      "films on page 12:  66%|██████▌   | 164/250 [46:51<20:55, 14.60s/it]\u001B[A\n",
      "films on page 12:  66%|██████▌   | 165/250 [47:07<21:10, 14.95s/it]\u001B[A\n",
      "films on page 12:  66%|██████▋   | 166/250 [47:18<19:20, 13.82s/it]\u001B[A\n",
      "films on page 12:  67%|██████▋   | 167/250 [47:35<20:37, 14.91s/it]\u001B[A\n",
      "films on page 12:  67%|██████▋   | 168/250 [47:54<22:01, 16.12s/it]\u001B[A\n",
      "films on page 12:  68%|██████▊   | 169/250 [48:12<22:29, 16.66s/it]\u001B[A\n",
      "films on page 12:  68%|██████▊   | 170/250 [48:27<21:22, 16.03s/it]\u001B[A\n",
      "films on page 12:  68%|██████▊   | 171/250 [48:41<20:28, 15.55s/it]\u001B[A\n",
      "films on page 12:  69%|██████▉   | 172/250 [48:59<21:12, 16.31s/it]\u001B[A\n",
      "films on page 12:  69%|██████▉   | 173/250 [49:19<22:09, 17.27s/it]\u001B[A\n",
      "films on page 12:  70%|██████▉   | 174/250 [49:32<20:15, 15.99s/it]\u001B[A\n",
      "films on page 12:  70%|███████   | 175/250 [49:50<20:51, 16.68s/it]\u001B[A\n",
      "films on page 12:  70%|███████   | 176/250 [50:09<21:18, 17.28s/it]\u001B[A\n",
      "films on page 12:  71%|███████   | 177/250 [50:24<20:27, 16.81s/it]\u001B[A\n",
      "films on page 12:  71%|███████   | 178/250 [50:39<19:16, 16.06s/it]\u001B[A\n",
      "films on page 12:  72%|███████▏  | 179/250 [50:55<19:02, 16.09s/it]\u001B[A\n",
      "films on page 12:  72%|███████▏  | 180/250 [51:12<19:01, 16.30s/it]\u001B[A\n",
      "films on page 12:  72%|███████▏  | 181/250 [51:31<19:37, 17.06s/it]\u001B[A\n",
      "films on page 12:  73%|███████▎  | 182/250 [51:48<19:22, 17.09s/it]\u001B[A\n",
      "films on page 12:  73%|███████▎  | 183/250 [52:06<19:19, 17.30s/it]\u001B[A\n",
      "films on page 12:  74%|███████▎  | 184/250 [52:24<19:16, 17.52s/it]\u001B[A\n",
      "films on page 12:  74%|███████▍  | 185/250 [52:39<18:08, 16.75s/it]\u001B[A\n",
      "films on page 12:  74%|███████▍  | 186/250 [52:53<17:14, 16.16s/it]\u001B[A\n",
      "films on page 12:  75%|███████▍  | 187/250 [53:10<17:07, 16.31s/it]\u001B[A\n",
      "films on page 12:  75%|███████▌  | 188/250 [53:28<17:15, 16.69s/it]\u001B[A\n",
      "films on page 12:  76%|███████▌  | 189/250 [53:42<16:19, 16.06s/it]\u001B[A\n",
      "films on page 12:  76%|███████▌  | 190/250 [53:58<15:55, 15.92s/it]\u001B[A\n",
      "films on page 12:  76%|███████▋  | 191/250 [54:15<16:08, 16.41s/it]\u001B[A\n",
      "films on page 12:  77%|███████▋  | 192/250 [54:33<16:22, 16.94s/it]\u001B[A\n",
      "films on page 12:  77%|███████▋  | 193/250 [54:45<14:24, 15.17s/it]\u001B[A\n",
      "films on page 12:  78%|███████▊  | 194/250 [55:01<14:38, 15.69s/it]\u001B[A\n",
      "films on page 12:  78%|███████▊  | 195/250 [55:19<14:57, 16.32s/it]\u001B[A\n",
      "films on page 12:  78%|███████▊  | 196/250 [55:36<14:47, 16.44s/it]\u001B[A\n",
      "films on page 12:  79%|███████▉  | 197/250 [55:51<14:06, 15.98s/it]\u001B[A\n",
      "films on page 12:  79%|███████▉  | 198/250 [56:08<14:15, 16.45s/it]\u001B[A\n",
      "films on page 12:  80%|███████▉  | 199/250 [56:23<13:26, 15.81s/it]\u001B[A\n",
      "films on page 12:  80%|████████  | 200/250 [56:47<15:21, 18.43s/it]\u001B[A\n",
      "films on page 12:  80%|████████  | 201/250 [57:05<14:49, 18.16s/it]\u001B[A\n",
      "films on page 12:  81%|████████  | 202/250 [57:20<13:45, 17.20s/it]\u001B[A\n",
      "films on page 12:  81%|████████  | 203/250 [57:36<13:19, 17.01s/it]\u001B[A\n",
      "films on page 12:  82%|████████▏ | 204/250 [57:52<12:44, 16.63s/it]\u001B[A\n",
      "films on page 12:  82%|████████▏ | 205/250 [58:10<12:50, 17.12s/it]\u001B[AWARNING:root:film tt2438644\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6B57280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E6B57280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6B57280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6B57280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  82%|████████▏ | 206/250 [58:23<11:35, 15.80s/it]\u001B[A\n",
      "films on page 12:  83%|████████▎ | 207/250 [58:44<12:22, 17.27s/it]\u001B[AWARNING:root:film tt0213890\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6B7D2B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E6B7D2B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6B7D2B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6B7D2B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  83%|████████▎ | 208/250 [58:58<11:23, 16.26s/it]\u001B[A\n",
      "films on page 12:  84%|████████▎ | 209/250 [59:13<10:56, 16.00s/it]\u001B[A\n",
      "films on page 12:  84%|████████▍ | 210/250 [59:33<11:30, 17.26s/it]\u001B[A\n",
      "films on page 12:  84%|████████▍ | 211/250 [59:52<11:25, 17.59s/it]\u001B[A\n",
      "films on page 12:  85%|████████▍ | 212/250 [1:00:09<11:01, 17.42s/it]\u001B[A\n",
      "films on page 12:  85%|████████▌ | 213/250 [1:00:29<11:16, 18.27s/it]\u001B[A\n",
      "films on page 12:  86%|████████▌ | 214/250 [1:00:43<10:09, 16.94s/it]\u001B[A\n",
      "films on page 12:  86%|████████▌ | 215/250 [1:01:01<10:12, 17.50s/it]\u001B[A\n",
      "films on page 12:  86%|████████▋ | 216/250 [1:01:18<09:40, 17.06s/it]\u001B[A\n",
      "films on page 12:  87%|████████▋ | 217/250 [1:01:30<08:39, 15.73s/it]\u001B[A\n",
      "films on page 12:  87%|████████▋ | 218/250 [1:01:46<08:19, 15.62s/it]\u001B[A\n",
      "films on page 12:  88%|████████▊ | 219/250 [1:02:01<08:04, 15.63s/it]\u001B[A\n",
      "films on page 12:  88%|████████▊ | 220/250 [1:02:16<07:43, 15.43s/it]\u001B[A\n",
      "films on page 12:  88%|████████▊ | 221/250 [1:02:31<07:20, 15.19s/it]\u001B[A\n",
      "films on page 12:  89%|████████▉ | 222/250 [1:02:50<07:36, 16.31s/it]\u001B[A\n",
      "films on page 12:  89%|████████▉ | 223/250 [1:03:05<07:09, 15.89s/it]\u001B[A\n",
      "films on page 12:  90%|████████▉ | 224/250 [1:03:22<07:06, 16.41s/it]\u001B[A\n",
      "films on page 12:  90%|█████████ | 225/250 [1:03:40<07:00, 16.81s/it]\u001B[A\n",
      "films on page 12:  90%|█████████ | 226/250 [1:03:56<06:36, 16.52s/it]\u001B[A\n",
      "films on page 12:  91%|█████████ | 227/250 [1:04:13<06:21, 16.60s/it]\u001B[A\n",
      "films on page 12:  91%|█████████ | 228/250 [1:04:28<05:56, 16.19s/it]\u001B[A\n",
      "films on page 12:  92%|█████████▏| 229/250 [1:04:42<05:27, 15.61s/it]\u001B[A\n",
      "films on page 12:  92%|█████████▏| 230/250 [1:05:01<05:29, 16.47s/it]\u001B[A\n",
      "films on page 12:  92%|█████████▏| 231/250 [1:05:18<05:16, 16.63s/it]\u001B[A\n",
      "films on page 12:  93%|█████████▎| 232/250 [1:05:35<05:01, 16.74s/it]\u001B[A\n",
      "films on page 12:  93%|█████████▎| 233/250 [1:05:51<04:44, 16.74s/it]\u001B[A\n",
      "films on page 12:  94%|█████████▎| 234/250 [1:06:06<04:19, 16.23s/it]\u001B[A\n",
      "films on page 12:  94%|█████████▍| 235/250 [1:06:24<04:11, 16.76s/it]\u001B[A\n",
      "films on page 12:  94%|█████████▍| 236/250 [1:06:41<03:52, 16.59s/it]\u001B[AWARNING:root:film tt1474276\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6F752B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E6F752B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6F752B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6F752B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  95%|█████████▍| 237/250 [1:06:52<03:13, 14.90s/it]\u001B[A\n",
      "films on page 12:  95%|█████████▌| 238/250 [1:07:08<03:03, 15.29s/it]\u001B[A\n",
      "films on page 12:  96%|█████████▌| 239/250 [1:07:25<02:54, 15.88s/it]\u001B[A\n",
      "films on page 12:  96%|█████████▌| 240/250 [1:07:41<02:38, 15.87s/it]\u001B[AWARNING:root:film tt1424797\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6B57700>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E6B57700>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6B57700>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6B57700>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  96%|█████████▋| 241/250 [1:07:50<02:05, 13.92s/it]\u001B[A\n",
      "films on page 12:  97%|█████████▋| 242/250 [1:08:05<01:52, 14.11s/it]\u001B[AWARNING:root:film tt0411272\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6B57C70>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E6B57C70>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6B57C70>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6B57C70>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 12:  97%|█████████▋| 243/250 [1:08:15<01:30, 12.90s/it]\u001B[A\n",
      "films on page 12:  98%|█████████▊| 244/250 [1:08:32<01:25, 14.19s/it]\u001B[A\n",
      "films on page 12:  98%|█████████▊| 245/250 [1:08:46<01:11, 14.25s/it]\u001B[A\n",
      "films on page 12:  98%|█████████▊| 246/250 [1:09:04<01:01, 15.40s/it]\u001B[A\n",
      "films on page 12:  99%|█████████▉| 247/250 [1:09:25<00:51, 17.05s/it]\u001B[A\n",
      "films on page 12:  99%|█████████▉| 248/250 [1:09:42<00:33, 16.91s/it]\u001B[A\n",
      "films on page 12: 100%|█████████▉| 249/250 [1:09:58<00:16, 16.78s/it]\u001B[A\n",
      "films on page 12: 100%|██████████| 250/250 [1:10:18<00:00, 16.88s/it]\u001B[A\n",
      "pages:  70%|███████   | 7/10 [8:06:33<3:32:03, 4241.22s/it]\n",
      "films on page 13:   0%|          | 0/250 [00:00<?, ?it/s]\u001B[A\n",
      "films on page 13:   0%|          | 1/250 [00:17<1:10:49, 17.07s/it]\u001B[AWARNING:root:film tt2987732\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DC1553D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143DC1553D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DC1553D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DC1553D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:   1%|          | 2/250 [00:24<47:56, 11.60s/it]  \u001B[A\n",
      "films on page 13:   1%|          | 3/250 [00:41<58:07, 14.12s/it]\u001B[A\n",
      "films on page 13:   2%|▏         | 4/250 [00:58<1:01:59, 15.12s/it]\u001B[AWARNING:root:film tt1481572\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E04F7070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E04F7070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E04F7070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E04F7070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:   2%|▏         | 5/250 [01:11<58:37, 14.36s/it]  \u001B[A\n",
      "films on page 13:   2%|▏         | 6/250 [01:28<1:02:21, 15.33s/it]\u001B[A\n",
      "films on page 13:   3%|▎         | 7/250 [01:45<1:04:18, 15.88s/it]\u001B[A\n",
      "films on page 13:   3%|▎         | 8/250 [02:03<1:05:59, 16.36s/it]\u001B[A\n",
      "films on page 13:   4%|▎         | 9/250 [02:20<1:06:14, 16.49s/it]\u001B[A\n",
      "films on page 13:   4%|▍         | 10/250 [02:38<1:08:27, 17.11s/it]\u001B[A\n",
      "films on page 13:   4%|▍         | 11/250 [02:53<1:05:39, 16.48s/it]\u001B[AWARNING:root:film tt1018785\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E05AB580>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E05AB580>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E05AB580>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E05AB580>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:   5%|▍         | 12/250 [03:05<1:00:14, 15.19s/it]\u001B[A\n",
      "films on page 13:   5%|▌         | 13/250 [03:25<1:04:48, 16.41s/it]\u001B[A\n",
      "films on page 13:   6%|▌         | 14/250 [03:42<1:05:34, 16.67s/it]\u001B[A\n",
      "films on page 13:   6%|▌         | 15/250 [03:58<1:05:03, 16.61s/it]\u001B[A\n",
      "films on page 13:   6%|▋         | 16/250 [04:14<1:03:35, 16.30s/it]\u001B[A\n",
      "films on page 13:   7%|▋         | 17/250 [04:32<1:05:43, 16.92s/it]\u001B[A\n",
      "films on page 13:   7%|▋         | 18/250 [04:49<1:05:22, 16.91s/it]\u001B[AWARNING:root:film tt0310203\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E067F580>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E067F580>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E067F580>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E067F580>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:   8%|▊         | 19/250 [04:57<54:41, 14.20s/it]  \u001B[A\n",
      "films on page 13:   8%|▊         | 20/250 [05:13<56:59, 14.87s/it]\u001B[A\n",
      "films on page 13:   8%|▊         | 21/250 [05:31<59:52, 15.69s/it]\u001B[A\n",
      "films on page 13:   9%|▉         | 22/250 [05:47<1:00:17, 15.87s/it]\u001B[A\n",
      "films on page 13:   9%|▉         | 23/250 [06:07<1:04:57, 17.17s/it]\u001B[A\n",
      "films on page 13:  10%|▉         | 24/250 [06:21<1:00:52, 16.16s/it]\u001B[A\n",
      "films on page 13:  10%|█         | 25/250 [06:37<1:00:33, 16.15s/it]\u001B[A\n",
      "films on page 13:  10%|█         | 26/250 [06:54<1:00:40, 16.25s/it]\u001B[A\n",
      "films on page 13:  11%|█         | 27/250 [07:09<58:44, 15.81s/it]  \u001B[A\n",
      "films on page 13:  11%|█         | 28/250 [07:25<58:53, 15.92s/it]\u001B[AWARNING:root:film tt1600524\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E0827070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E0827070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E0827070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E0827070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  12%|█▏        | 29/250 [07:37<54:15, 14.73s/it]\u001B[A\n",
      "films on page 13:  12%|█▏        | 30/250 [07:53<55:41, 15.19s/it]\u001B[A\n",
      "films on page 13:  12%|█▏        | 31/250 [08:11<58:02, 15.90s/it]\u001B[A\n",
      "films on page 13:  13%|█▎        | 32/250 [08:27<57:54, 15.94s/it]\u001B[A\n",
      "films on page 13:  13%|█▎        | 33/250 [08:47<1:02:33, 17.30s/it]\u001B[A\n",
      "films on page 13:  14%|█▎        | 34/250 [09:02<59:28, 16.52s/it]  \u001B[A\n",
      "films on page 13:  14%|█▍        | 35/250 [09:18<58:58, 16.46s/it]\u001B[A\n",
      "films on page 13:  14%|█▍        | 36/250 [09:32<56:14, 15.77s/it]\u001B[A\n",
      "films on page 13:  15%|█▍        | 37/250 [09:49<56:46, 15.99s/it]\u001B[A\n",
      "films on page 13:  15%|█▌        | 38/250 [10:07<59:16, 16.78s/it]\u001B[A\n",
      "films on page 13:  16%|█▌        | 39/250 [10:23<58:03, 16.51s/it]\u001B[A\n",
      "films on page 13:  16%|█▌        | 40/250 [10:37<54:52, 15.68s/it]\u001B[A\n",
      "films on page 13:  16%|█▋        | 41/250 [10:53<55:00, 15.79s/it]\u001B[A\n",
      "films on page 13:  17%|█▋        | 42/250 [11:12<57:28, 16.58s/it]\u001B[A\n",
      "films on page 13:  17%|█▋        | 43/250 [11:26<55:19, 16.04s/it]\u001B[A\n",
      "films on page 13:  18%|█▊        | 44/250 [11:45<58:08, 16.93s/it]\u001B[A\n",
      "films on page 13:  18%|█▊        | 45/250 [11:58<53:50, 15.76s/it]\u001B[A\n",
      "films on page 13:  18%|█▊        | 46/250 [12:15<54:51, 16.14s/it]\u001B[A\n",
      "films on page 13:  19%|█▉        | 47/250 [12:31<54:00, 15.96s/it]\u001B[A\n",
      "films on page 13:  19%|█▉        | 48/250 [12:46<53:14, 15.81s/it]\u001B[A\n",
      "films on page 13:  20%|█▉        | 49/250 [13:06<56:23, 16.83s/it]\u001B[A\n",
      "films on page 13:  20%|██        | 50/250 [13:28<1:01:30, 18.45s/it]\u001B[A\n",
      "films on page 13:  20%|██        | 51/250 [13:45<59:33, 17.96s/it]  \u001B[A\n",
      "films on page 13:  21%|██        | 52/250 [13:59<55:29, 16.82s/it]\u001B[A\n",
      "films on page 13:  21%|██        | 53/250 [14:18<57:23, 17.48s/it]\u001B[A\n",
      "films on page 13:  22%|██▏       | 54/250 [14:35<56:33, 17.32s/it]\u001B[A\n",
      "films on page 13:  22%|██▏       | 55/250 [14:53<57:36, 17.73s/it]\u001B[A\n",
      "films on page 13:  22%|██▏       | 56/250 [15:11<57:04, 17.65s/it]\u001B[A\n",
      "films on page 13:  23%|██▎       | 57/250 [15:28<56:01, 17.41s/it]\u001B[A\n",
      "films on page 13:  23%|██▎       | 58/250 [15:44<54:22, 16.99s/it]\u001B[A\n",
      "films on page 13:  24%|██▎       | 59/250 [15:59<52:20, 16.44s/it]\u001B[A\n",
      "films on page 13:  24%|██▍       | 60/250 [16:12<49:07, 15.51s/it]\u001B[A\n",
      "films on page 13:  24%|██▍       | 61/250 [16:29<50:01, 15.88s/it]\u001B[A\n",
      "films on page 13:  25%|██▍       | 62/250 [16:45<50:10, 16.01s/it]\u001B[A\n",
      "films on page 13:  25%|██▌       | 63/250 [17:02<50:18, 16.14s/it]\u001B[A\n",
      "films on page 13:  26%|██▌       | 64/250 [17:17<49:09, 15.86s/it]\u001B[A\n",
      "films on page 13:  26%|██▌       | 65/250 [17:36<51:21, 16.66s/it]\u001B[A\n",
      "films on page 13:  26%|██▋       | 66/250 [17:53<51:35, 16.82s/it]\u001B[AWARNING:root:film tt11497716\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E0466A60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E0466A60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E0466A60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E0466A60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  27%|██▋       | 67/250 [18:03<44:55, 14.73s/it]\u001B[A\n",
      "films on page 13:  27%|██▋       | 68/250 [18:20<46:58, 15.49s/it]\u001B[A\n",
      "films on page 13:  28%|██▊       | 69/250 [18:36<47:05, 15.61s/it]\u001B[A\n",
      "films on page 13:  28%|██▊       | 70/250 [18:48<44:05, 14.70s/it]\u001B[A\n",
      "films on page 13:  28%|██▊       | 71/250 [19:08<48:08, 16.14s/it]\u001B[A\n",
      "films on page 13:  29%|██▉       | 72/250 [19:24<48:07, 16.22s/it]\u001B[A\n",
      "films on page 13:  29%|██▉       | 73/250 [19:42<49:26, 16.76s/it]\u001B[A\n",
      "films on page 13:  30%|██▉       | 74/250 [19:58<47:57, 16.35s/it]\u001B[A\n",
      "films on page 13:  30%|███       | 75/250 [20:17<49:59, 17.14s/it]\u001B[A\n",
      "films on page 13:  30%|███       | 76/250 [20:32<48:24, 16.69s/it]\u001B[A\n",
      "films on page 13:  31%|███       | 77/250 [20:49<48:36, 16.86s/it]\u001B[A\n",
      "films on page 13:  31%|███       | 78/250 [21:07<48:43, 17.00s/it]\u001B[A\n",
      "films on page 13:  32%|███▏      | 79/250 [21:22<47:09, 16.54s/it]\u001B[A\n",
      "films on page 13:  32%|███▏      | 80/250 [21:37<44:58, 15.87s/it]\u001B[A\n",
      "films on page 13:  32%|███▏      | 81/250 [21:55<46:53, 16.65s/it]\u001B[A\n",
      "films on page 13:  33%|███▎      | 82/250 [22:14<48:21, 17.27s/it]\u001B[AWARNING:root:film tt3547740\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E110C070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E110C070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E110C070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E110C070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  33%|███▎      | 83/250 [22:27<45:04, 16.19s/it]\u001B[A\n",
      "films on page 13:  34%|███▎      | 84/250 [22:45<45:58, 16.62s/it]\u001B[A\n",
      "films on page 13:  34%|███▍      | 85/250 [22:59<43:31, 15.83s/it]\u001B[AWARNING:root:film tt1029235\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E1154070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E1154070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E1154070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E1154070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  34%|███▍      | 86/250 [23:11<40:03, 14.66s/it]\u001B[A\n",
      "films on page 13:  35%|███▍      | 87/250 [23:24<38:24, 14.14s/it]\u001B[A\n",
      "films on page 13:  35%|███▌      | 88/250 [23:41<40:42, 15.08s/it]\u001B[A\n",
      "films on page 13:  36%|███▌      | 89/250 [23:57<41:20, 15.41s/it]\u001B[AWARNING:root:film tt2452200\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E11FD070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E11FD070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E11FD070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E11FD070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  36%|███▌      | 90/250 [24:09<38:19, 14.37s/it]\u001B[A\n",
      "films on page 13:  36%|███▋      | 91/250 [24:25<39:21, 14.85s/it]\u001B[A\n",
      "films on page 13:  37%|███▋      | 92/250 [24:41<40:06, 15.23s/it]\u001B[A\n",
      "films on page 13:  37%|███▋      | 93/250 [24:57<39:50, 15.23s/it]\u001B[A\n",
      "films on page 13:  38%|███▊      | 94/250 [25:15<42:14, 16.25s/it]\u001B[A\n",
      "films on page 13:  38%|███▊      | 95/250 [25:34<43:50, 16.97s/it]\u001B[AWARNING:root:film tt5607714\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E71E0070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E71E0070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E71E0070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E71E0070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  38%|███▊      | 96/250 [25:47<40:33, 15.80s/it]\u001B[A\n",
      "films on page 13:  39%|███▉      | 97/250 [26:04<40:55, 16.05s/it]\u001B[A\n",
      "films on page 13:  39%|███▉      | 98/250 [26:21<41:27, 16.36s/it]\u001B[A\n",
      "films on page 13:  40%|███▉      | 99/250 [26:34<39:07, 15.55s/it]\u001B[A\n",
      "films on page 13:  40%|████      | 100/250 [26:48<37:06, 14.85s/it]\u001B[A\n",
      "films on page 13:  40%|████      | 101/250 [27:02<36:13, 14.58s/it]\u001B[A\n",
      "films on page 13:  41%|████      | 102/250 [27:19<37:46, 15.31s/it]\u001B[A\n",
      "films on page 13:  41%|████      | 103/250 [27:31<35:15, 14.39s/it]\u001B[A\n",
      "films on page 13:  42%|████▏     | 104/250 [27:47<36:11, 14.87s/it]\u001B[A\n",
      "films on page 13:  42%|████▏     | 105/250 [28:08<40:34, 16.79s/it]\u001B[A\n",
      "films on page 13:  42%|████▏     | 106/250 [28:24<39:53, 16.62s/it]\u001B[A\n",
      "films on page 13:  43%|████▎     | 107/250 [28:43<41:12, 17.29s/it]\u001B[A\n",
      "films on page 13:  43%|████▎     | 108/250 [28:59<39:36, 16.74s/it]\u001B[A\n",
      "films on page 13:  44%|████▎     | 109/250 [29:17<40:20, 17.17s/it]\u001B[A\n",
      "films on page 13:  44%|████▍     | 110/250 [29:35<40:33, 17.39s/it]\u001B[A\n",
      "films on page 13:  44%|████▍     | 111/250 [29:50<38:40, 16.70s/it]\u001B[A\n",
      "films on page 13:  45%|████▍     | 112/250 [30:04<36:56, 16.06s/it]\u001B[A\n",
      "films on page 13:  45%|████▌     | 113/250 [30:22<37:42, 16.52s/it]\u001B[AWARNING:root:film tt7748244\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E0466370>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E0466370>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E0466370>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E0466370>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  46%|████▌     | 114/250 [30:34<34:37, 15.28s/it]\u001B[A\n",
      "films on page 13:  46%|████▌     | 115/250 [30:53<36:32, 16.24s/it]\u001B[A\n",
      "films on page 13:  46%|████▋     | 116/250 [31:09<36:06, 16.17s/it]\u001B[A\n",
      "films on page 13:  47%|████▋     | 117/250 [31:25<35:58, 16.23s/it]\u001B[A\n",
      "films on page 13:  47%|████▋     | 118/250 [31:41<35:13, 16.01s/it]\u001B[AWARNING:root:film tt6768578\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E778B070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E778B070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E778B070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E778B070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  48%|████▊     | 119/250 [31:55<33:43, 15.45s/it]\u001B[A\n",
      "films on page 13:  48%|████▊     | 120/250 [32:07<31:39, 14.61s/it]\u001B[A\n",
      "films on page 13:  48%|████▊     | 121/250 [32:24<32:50, 15.28s/it]\u001B[A\n",
      "films on page 13:  49%|████▉     | 122/250 [32:43<34:33, 16.20s/it]\u001B[A\n",
      "films on page 13:  49%|████▉     | 123/250 [32:56<32:32, 15.38s/it]\u001B[A\n",
      "films on page 13:  50%|████▉     | 124/250 [33:15<34:13, 16.30s/it]\u001B[A\n",
      "films on page 13:  50%|█████     | 125/250 [33:30<33:21, 16.02s/it]\u001B[A\n",
      "films on page 13:  50%|█████     | 126/250 [33:43<31:13, 15.11s/it]\u001B[A\n",
      "films on page 13:  51%|█████     | 127/250 [33:58<31:07, 15.18s/it]\u001B[A\n",
      "films on page 13:  51%|█████     | 128/250 [34:14<31:18, 15.40s/it]\u001B[A\n",
      "films on page 13:  52%|█████▏    | 129/250 [34:28<29:55, 14.84s/it]\u001B[A\n",
      "films on page 13:  52%|█████▏    | 130/250 [34:46<31:29, 15.75s/it]\u001B[A\n",
      "films on page 13:  52%|█████▏    | 131/250 [35:06<34:17, 17.29s/it]\u001B[A\n",
      "films on page 13:  53%|█████▎    | 132/250 [35:21<32:41, 16.63s/it]\u001B[A\n",
      "films on page 13:  53%|█████▎    | 133/250 [35:38<32:05, 16.46s/it]\u001B[AWARNING:root:film tt2872462\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E80C1070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E80C1070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E80C1070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E80C1070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  54%|█████▎    | 134/250 [35:48<28:13, 14.60s/it]\u001B[A\n",
      "films on page 13:  54%|█████▍    | 135/250 [36:06<30:07, 15.72s/it]\u001B[A\n",
      "films on page 13:  54%|█████▍    | 136/250 [36:24<31:02, 16.34s/it]\u001B[A\n",
      "films on page 13:  55%|█████▍    | 137/250 [36:37<29:08, 15.47s/it]\u001B[A\n",
      "films on page 13:  55%|█████▌    | 138/250 [36:54<29:24, 15.75s/it]\u001B[A\n",
      "films on page 13:  56%|█████▌    | 139/250 [37:06<27:14, 14.73s/it]\u001B[A\n",
      "films on page 13:  56%|█████▌    | 140/250 [37:25<29:12, 15.93s/it]\u001B[A\n",
      "films on page 13:  56%|█████▋    | 141/250 [37:42<29:49, 16.41s/it]\u001B[A\n",
      "films on page 13:  57%|█████▋    | 142/250 [37:57<28:47, 15.99s/it]\u001B[A\n",
      "films on page 13:  57%|█████▋    | 143/250 [38:13<28:09, 15.79s/it]\u001B[AWARNING:root:film tt2023453\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9232070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E9232070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9232070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9232070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  58%|█████▊    | 144/250 [38:28<27:28, 15.55s/it]\u001B[A\n",
      "films on page 13:  58%|█████▊    | 145/250 [38:42<26:37, 15.22s/it]\u001B[AWARNING:root:film tt5078204\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E0466F10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E0466F10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E0466F10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E0466F10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  58%|█████▊    | 146/250 [38:51<23:17, 13.44s/it]\u001B[A\n",
      "films on page 13:  59%|█████▉    | 147/250 [39:10<25:53, 15.08s/it]\u001B[A\n",
      "films on page 13:  59%|█████▉    | 148/250 [39:28<26:47, 15.76s/it]\u001B[AWARNING:root:film tt1151359\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E92A4070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E92A4070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E92A4070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E92A4070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  60%|█████▉    | 149/250 [39:41<25:28, 15.14s/it]\u001B[A\n",
      "films on page 13:  60%|██████    | 150/250 [40:09<31:36, 18.97s/it]\u001B[A\n",
      "films on page 13:  60%|██████    | 151/250 [40:24<29:03, 17.61s/it]\u001B[A\n",
      "films on page 13:  61%|██████    | 152/250 [40:41<28:23, 17.38s/it]\u001B[A\n",
      "films on page 13:  61%|██████    | 153/250 [40:58<28:09, 17.42s/it]\u001B[A\n",
      "films on page 13:  62%|██████▏   | 154/250 [41:11<25:48, 16.13s/it]\u001B[A\n",
      "films on page 13:  62%|██████▏   | 155/250 [41:32<27:36, 17.44s/it]\u001B[A\n",
      "films on page 13:  62%|██████▏   | 156/250 [41:47<26:23, 16.85s/it]\u001B[A\n",
      "films on page 13:  63%|██████▎   | 157/250 [42:01<24:36, 15.88s/it]\u001B[A\n",
      "films on page 13:  63%|██████▎   | 158/250 [42:14<22:58, 14.99s/it]\u001B[A\n",
      "films on page 13:  64%|██████▎   | 159/250 [42:34<24:54, 16.43s/it]\u001B[AWARNING:root:film tt0913968\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E93FD070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E93FD070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E93FD070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E93FD070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  64%|██████▍   | 160/250 [42:47<23:24, 15.61s/it]\u001B[A\n",
      "films on page 13:  64%|██████▍   | 161/250 [43:03<23:21, 15.75s/it]\u001B[A\n",
      "films on page 13:  65%|██████▍   | 162/250 [43:18<22:41, 15.47s/it]\u001B[AWARNING:root:film tt1736633\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9432580>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E9432580>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9432580>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9432580>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  65%|██████▌   | 163/250 [43:29<20:29, 14.13s/it]\u001B[AWARNING:root:film tt1648216\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9448070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E9448070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9448070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9448070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  66%|██████▌   | 164/250 [43:43<19:57, 13.92s/it]\u001B[A\n",
      "films on page 13:  66%|██████▌   | 165/250 [43:56<19:29, 13.76s/it]\u001B[A\n",
      "films on page 13:  66%|██████▋   | 166/250 [44:14<21:01, 15.02s/it]\u001B[A\n",
      "films on page 13:  67%|██████▋   | 167/250 [44:29<21:00, 15.19s/it]\u001B[A\n",
      "films on page 13:  67%|██████▋   | 168/250 [44:43<20:08, 14.74s/it]\u001B[A\n",
      "films on page 13:  68%|██████▊   | 169/250 [44:58<20:08, 14.91s/it]\u001B[A\n",
      "films on page 13:  68%|██████▊   | 170/250 [45:18<21:43, 16.29s/it]\u001B[AWARNING:root:film tt1465487\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9511070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E9511070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9511070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9511070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  68%|██████▊   | 171/250 [45:30<19:53, 15.11s/it]\u001B[A\n",
      "films on page 13:  69%|██████▉   | 172/250 [45:47<20:18, 15.63s/it]\u001B[A\n",
      "films on page 13:  69%|██████▉   | 173/250 [46:06<21:17, 16.59s/it]\u001B[A\n",
      "films on page 13:  70%|██████▉   | 174/250 [46:22<20:41, 16.34s/it]\u001B[A\n",
      "films on page 13:  70%|███████   | 175/250 [46:38<20:31, 16.43s/it]\u001B[A\n",
      "films on page 13:  70%|███████   | 176/250 [46:57<21:00, 17.03s/it]\u001B[A\n",
      "films on page 13:  71%|███████   | 177/250 [47:10<19:08, 15.73s/it]\u001B[A\n",
      "films on page 13:  71%|███████   | 178/250 [47:28<19:56, 16.62s/it]\u001B[AWARNING:root:film tt2321405\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9614070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E9614070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9614070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9614070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  72%|███████▏  | 179/250 [47:41<18:07, 15.32s/it]\u001B[A\n",
      "films on page 13:  72%|███████▏  | 180/250 [48:00<19:24, 16.63s/it]\u001B[A\n",
      "films on page 13:  72%|███████▏  | 181/250 [48:18<19:21, 16.84s/it]\u001B[A\n",
      "films on page 13:  73%|███████▎  | 182/250 [48:35<19:08, 16.88s/it]\u001B[A\n",
      "films on page 13:  73%|███████▎  | 183/250 [48:49<18:10, 16.28s/it]\u001B[A\n",
      "films on page 13:  74%|███████▎  | 184/250 [49:04<17:17, 15.72s/it]\u001B[A\n",
      "films on page 13:  74%|███████▍  | 185/250 [49:20<17:09, 15.83s/it]\u001B[A\n",
      "films on page 13:  74%|███████▍  | 186/250 [49:38<17:35, 16.49s/it]\u001B[A\n",
      "films on page 13:  75%|███████▍  | 187/250 [49:51<16:13, 15.46s/it]\u001B[A\n",
      "films on page 13:  75%|███████▌  | 188/250 [50:07<16:17, 15.76s/it]\u001B[A\n",
      "films on page 13:  76%|███████▌  | 189/250 [50:20<15:07, 14.89s/it]\u001B[A\n",
      "films on page 13:  76%|███████▌  | 190/250 [50:39<16:09, 16.16s/it]\u001B[A\n",
      "films on page 13:  76%|███████▋  | 191/250 [50:52<14:43, 14.98s/it]\u001B[A\n",
      "films on page 13:  77%|███████▋  | 192/250 [51:07<14:33, 15.06s/it]\u001B[A\n",
      "films on page 13:  77%|███████▋  | 193/250 [51:21<14:10, 14.92s/it]\u001B[A\n",
      "films on page 13:  78%|███████▊  | 194/250 [51:35<13:37, 14.61s/it]\u001B[A\n",
      "films on page 13:  78%|███████▊  | 195/250 [51:53<14:08, 15.43s/it]\u001B[A\n",
      "films on page 13:  78%|███████▊  | 196/250 [52:07<13:32, 15.04s/it]\u001B[A\n",
      "films on page 13:  79%|███████▉  | 197/250 [52:23<13:27, 15.24s/it]\u001B[A\n",
      "films on page 13:  79%|███████▉  | 198/250 [52:41<13:55, 16.06s/it]\u001B[A\n",
      "films on page 13:  80%|███████▉  | 199/250 [52:53<12:48, 15.07s/it]\u001B[AWARNING:root:film tt2331143\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E98BC070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E98BC070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E98BC070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E98BC070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  80%|████████  | 200/250 [53:09<12:41, 15.23s/it]\u001B[A\n",
      "films on page 13:  80%|████████  | 201/250 [53:25<12:34, 15.40s/it]\u001B[A\n",
      "films on page 13:  81%|████████  | 202/250 [53:41<12:34, 15.72s/it]\u001B[A\n",
      "films on page 13:  81%|████████  | 203/250 [53:58<12:38, 16.15s/it]\u001B[A\n",
      "films on page 13:  82%|████████▏ | 204/250 [54:16<12:46, 16.67s/it]\u001B[A\n",
      "films on page 13:  82%|████████▏ | 205/250 [54:31<12:06, 16.14s/it]\u001B[A\n",
      "films on page 13:  82%|████████▏ | 206/250 [54:43<11:00, 15.01s/it]\u001B[A\n",
      "films on page 13:  83%|████████▎ | 207/250 [55:00<11:11, 15.62s/it]\u001B[A\n",
      "films on page 13:  83%|████████▎ | 208/250 [55:16<10:53, 15.57s/it]\u001B[A\n",
      "films on page 13:  84%|████████▎ | 209/250 [55:32<10:38, 15.57s/it]\u001B[A\n",
      "films on page 13:  84%|████████▍ | 210/250 [55:47<10:16, 15.41s/it]\u001B[AWARNING:root:film tt1715873\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6D2F070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E6D2F070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6D2F070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6D2F070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  84%|████████▍ | 211/250 [56:00<09:34, 14.72s/it]\u001B[AWARNING:root:film tt1198156\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6D2E070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E6D2E070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6D2E070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6D2E070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  85%|████████▍ | 212/250 [56:10<08:31, 13.47s/it]\u001B[A\n",
      "films on page 13:  85%|████████▌ | 213/250 [56:28<09:08, 14.83s/it]\u001B[A\n",
      "films on page 13:  86%|████████▌ | 214/250 [56:42<08:44, 14.56s/it]\u001B[A\n",
      "films on page 13:  86%|████████▌ | 215/250 [56:58<08:39, 14.85s/it]\u001B[A\n",
      "films on page 13:  86%|████████▋ | 216/250 [57:13<08:29, 14.99s/it]\u001B[AWARNING:root:film tt1440232\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6DBA070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E6DBA070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6DBA070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6DBA070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  87%|████████▋ | 217/250 [57:26<07:56, 14.45s/it]\u001B[AWARNING:root:film tt1924394\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6DBE070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E6DBE070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6DBE070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6DBE070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  87%|████████▋ | 218/250 [57:40<07:31, 14.11s/it]\u001B[A\n",
      "films on page 13:  88%|████████▊ | 219/250 [57:55<07:26, 14.39s/it]\u001B[A\n",
      "films on page 13:  88%|████████▊ | 220/250 [58:12<07:39, 15.32s/it]\u001B[A\n",
      "films on page 13:  88%|████████▊ | 221/250 [58:26<07:08, 14.78s/it]\u001B[AWARNING:root:film tt1487118\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6E17580>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E6E17580>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6E17580>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E6E17580>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  89%|████████▉ | 222/250 [58:35<06:11, 13.27s/it]\u001B[A\n",
      "films on page 13:  89%|████████▉ | 223/250 [58:51<06:17, 13.97s/it]\u001B[A\n",
      "films on page 13:  90%|████████▉ | 224/250 [59:05<06:06, 14.08s/it]\u001B[A\n",
      "films on page 13:  90%|█████████ | 225/250 [59:24<06:27, 15.50s/it]\u001B[A\n",
      "films on page 13:  90%|█████████ | 226/250 [59:38<06:03, 15.15s/it]\u001B[A\n",
      "films on page 13:  91%|█████████ | 227/250 [59:55<05:56, 15.51s/it]\u001B[A\n",
      "films on page 13:  91%|█████████ | 228/250 [1:00:10<05:41, 15.54s/it]\u001B[A\n",
      "films on page 13:  92%|█████████▏| 229/250 [1:00:26<05:26, 15.55s/it]\u001B[A\n",
      "films on page 13:  92%|█████████▏| 230/250 [1:00:40<05:02, 15.13s/it]\u001B[A\n",
      "films on page 13:  92%|█████████▏| 231/250 [1:00:57<04:59, 15.77s/it]\u001B[A\n",
      "films on page 13:  93%|█████████▎| 232/250 [1:01:17<05:04, 16.92s/it]\u001B[A\n",
      "films on page 13:  93%|█████████▎| 233/250 [1:01:33<04:44, 16.75s/it]\u001B[A\n",
      "films on page 13:  94%|█████████▎| 234/250 [1:01:49<04:23, 16.45s/it]\u001B[A\n",
      "films on page 13:  94%|█████████▍| 235/250 [1:02:04<03:59, 15.97s/it]\u001B[A\n",
      "films on page 13:  94%|█████████▍| 236/250 [1:02:20<03:45, 16.10s/it]\u001B[A\n",
      "films on page 13:  95%|█████████▍| 237/250 [1:02:38<03:34, 16.54s/it]\u001B[AWARNING:root:film tt15301048\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EAD09070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EAD09070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EAD09070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EAD09070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  95%|█████████▌| 238/250 [1:02:51<03:04, 15.38s/it]\u001B[AWARNING:root:film tt2424988\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EAD0E070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EAD0E070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EAD0E070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EAD0E070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  96%|█████████▌| 239/250 [1:03:03<02:38, 14.38s/it]\u001B[A\n",
      "films on page 13:  96%|█████████▌| 240/250 [1:03:23<02:42, 16.21s/it]\u001B[A\n",
      "films on page 13:  96%|█████████▋| 241/250 [1:03:41<02:31, 16.78s/it]\u001B[A\n",
      "films on page 13:  97%|█████████▋| 242/250 [1:03:58<02:13, 16.71s/it]\u001B[A\n",
      "films on page 13:  97%|█████████▋| 243/250 [1:04:13<01:53, 16.20s/it]\u001B[AWARNING:root:film tt0923811\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EAD93070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EAD93070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EAD93070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EAD93070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13:  98%|█████████▊| 244/250 [1:04:25<01:30, 15.15s/it]\u001B[A\n",
      "films on page 13:  98%|█████████▊| 245/250 [1:04:43<01:19, 15.93s/it]\u001B[A\n",
      "films on page 13:  98%|█████████▊| 246/250 [1:04:58<01:02, 15.64s/it]\u001B[A\n",
      "films on page 13:  99%|█████████▉| 247/250 [1:05:17<00:50, 16.72s/it]\u001B[A\n",
      "films on page 13:  99%|█████████▉| 248/250 [1:05:31<00:31, 15.67s/it]\u001B[A\n",
      "films on page 13: 100%|█████████▉| 249/250 [1:05:47<00:15, 15.86s/it]\u001B[AWARNING:root:film tt0254481\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EAE3D070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EAE3D070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EAE3D070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EAE3D070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 13: 100%|██████████| 250/250 [1:06:09<00:00, 15.88s/it]\u001B[A\n",
      "pages:  80%|████████  | 8/10 [9:12:46<2:18:32, 4156.14s/it]\n",
      "films on page 14:   0%|          | 0/249 [00:00<?, ?it/s]\u001B[A\n",
      "films on page 14:   0%|          | 1/249 [00:14<59:45, 14.46s/it]\u001B[A\n",
      "films on page 14:   1%|          | 2/249 [00:31<1:05:00, 15.79s/it]\u001B[A\n",
      "films on page 14:   1%|          | 3/249 [00:50<1:10:50, 17.28s/it]\u001B[A\n",
      "films on page 14:   2%|▏         | 4/249 [01:02<1:01:42, 15.11s/it]\u001B[AWARNING:root:film tt6628102\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EAEE9DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EAEE9DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EAEE9DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EAEE9DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:   2%|▏         | 5/249 [01:16<59:53, 14.73s/it]  \u001B[AWARNING:root:film tt2059255\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EAEF4DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EAEF4DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EAEF4DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EAEF4DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:   2%|▏         | 6/249 [01:28<56:38, 13.99s/it]\u001B[A\n",
      "films on page 14:   3%|▎         | 7/249 [01:45<1:00:20, 14.96s/it]\u001B[A\n",
      "films on page 14:   3%|▎         | 8/249 [01:59<58:31, 14.57s/it]  \u001B[A\n",
      "films on page 14:   4%|▎         | 9/249 [02:12<56:23, 14.10s/it]\u001B[A\n",
      "films on page 14:   4%|▍         | 10/249 [02:27<56:58, 14.30s/it]\u001B[A\n",
      "films on page 14:   4%|▍         | 11/249 [02:45<1:01:32, 15.52s/it]\u001B[A\n",
      "films on page 14:   5%|▍         | 12/249 [03:03<1:03:49, 16.16s/it]\u001B[A\n",
      "films on page 14:   5%|▌         | 13/249 [03:20<1:05:24, 16.63s/it]\u001B[A\n",
      "films on page 14:   6%|▌         | 14/249 [03:38<1:07:01, 17.11s/it]\u001B[A\n",
      "films on page 14:   6%|▌         | 15/249 [03:53<1:04:06, 16.44s/it]\u001B[A\n",
      "films on page 14:   6%|▋         | 16/249 [04:12<1:06:59, 17.25s/it]\u001B[AWARNING:root:film tt4272866\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB043DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EB043DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB043DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB043DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:   7%|▋         | 17/249 [04:26<1:01:55, 16.01s/it]\u001B[A\n",
      "films on page 14:   7%|▋         | 18/249 [04:42<1:02:36, 16.26s/it]\u001B[A\n",
      "films on page 14:   8%|▊         | 19/249 [04:56<59:07, 15.42s/it]  \u001B[A\n",
      "films on page 14:   8%|▊         | 20/249 [05:10<57:10, 14.98s/it]\u001B[A\n",
      "films on page 14:   8%|▊         | 21/249 [05:30<1:02:39, 16.49s/it]\u001B[A\n",
      "films on page 14:   9%|▉         | 22/249 [05:44<59:15, 15.66s/it]  \u001B[A\n",
      "films on page 14:   9%|▉         | 23/249 [05:57<56:47, 15.08s/it]\u001B[A\n",
      "films on page 14:  10%|▉         | 24/249 [06:15<59:38, 15.90s/it]\u001B[A\n",
      "films on page 14:  10%|█         | 25/249 [06:32<1:00:15, 16.14s/it]\u001B[A\n",
      "films on page 14:  10%|█         | 26/249 [06:48<59:33, 16.03s/it]  \u001B[A\n",
      "films on page 14:  11%|█         | 27/249 [07:01<55:54, 15.11s/it]\u001B[A\n",
      "films on page 14:  11%|█         | 28/249 [07:15<55:17, 15.01s/it]\u001B[A\n",
      "films on page 14:  12%|█▏        | 29/249 [07:32<56:24, 15.39s/it]\u001B[A\n",
      "films on page 14:  12%|█▏        | 30/249 [07:48<56:55, 15.60s/it]\u001B[AWARNING:root:film tt0497137\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB3EDDC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EB3EDDC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB3EDDC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB3EDDC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  12%|█▏        | 31/249 [07:58<50:27, 13.89s/it]\u001B[A\n",
      "films on page 14:  13%|█▎        | 32/249 [08:13<51:29, 14.24s/it]\u001B[A\n",
      "films on page 14:  13%|█▎        | 33/249 [08:28<52:53, 14.69s/it]\u001B[AWARNING:root:film tt1084972\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB438DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EB438DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB438DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB438DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  14%|█▎        | 34/249 [08:42<51:26, 14.35s/it]\u001B[A\n",
      "films on page 14:  14%|█▍        | 35/249 [09:00<55:19, 15.51s/it]\u001B[A\n",
      "films on page 14:  14%|█▍        | 36/249 [09:19<58:39, 16.52s/it]\u001B[AWARNING:root:film tt2258345\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB483DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EB483DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB483DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB483DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  15%|█▍        | 37/249 [09:31<53:37, 15.18s/it]\u001B[A\n",
      "films on page 14:  15%|█▌        | 38/249 [09:50<57:32, 16.36s/it]\u001B[A\n",
      "films on page 14:  16%|█▌        | 39/249 [10:09<59:35, 17.03s/it]\u001B[A\n",
      "films on page 14:  16%|█▌        | 40/249 [10:25<58:54, 16.91s/it]\u001B[AWARNING:root:film tt2625030\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB4ECDC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EB4ECDC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB4ECDC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB4ECDC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  16%|█▋        | 41/249 [10:38<54:05, 15.60s/it]\u001B[A\n",
      "films on page 14:  17%|█▋        | 42/249 [10:53<53:31, 15.51s/it]\u001B[A\n",
      "films on page 14:  17%|█▋        | 43/249 [11:11<55:33, 16.18s/it]\u001B[A\n",
      "films on page 14:  18%|█▊        | 44/249 [11:30<58:05, 17.00s/it]\u001B[A\n",
      "films on page 14:  18%|█▊        | 45/249 [11:50<1:00:48, 17.89s/it]\u001B[A\n",
      "films on page 14:  18%|█▊        | 46/249 [12:04<56:49, 16.80s/it]  \u001B[AWARNING:root:film tt1183923\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB5A2DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EB5A2DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB5A2DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB5A2DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  19%|█▉        | 47/249 [12:16<51:29, 15.30s/it]\u001B[A\n",
      "films on page 14:  19%|█▉        | 48/249 [12:31<51:00, 15.23s/it]\u001B[A\n",
      "films on page 14:  20%|█▉        | 49/249 [12:48<52:48, 15.84s/it]\u001B[AWARNING:root:film tt7642818\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB4FEFA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EB4FEFA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB4FEFA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EB4FEFA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  20%|██        | 50/249 [12:57<45:02, 13.58s/it]\u001B[AWARNING:root:film tt3140100\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD7C310>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143ECD7C310>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD7C310>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD7C310>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  20%|██        | 51/249 [13:06<41:00, 12.43s/it]\u001B[AWARNING:root:film tt1326972\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD78310>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143ECD78310>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD78310>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD78310>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  21%|██        | 52/249 [13:15<36:58, 11.26s/it]\u001B[A\n",
      "films on page 14:  21%|██▏       | 53/249 [13:28<38:55, 11.92s/it]\u001B[A\n",
      "films on page 14:  22%|██▏       | 54/249 [13:45<43:23, 13.35s/it]\u001B[A\n",
      "films on page 14:  22%|██▏       | 55/249 [14:01<45:12, 13.98s/it]\u001B[A\n",
      "films on page 14:  22%|██▏       | 56/249 [14:13<43:14, 13.44s/it]\u001B[A\n",
      "films on page 14:  23%|██▎       | 57/249 [14:28<44:53, 14.03s/it]\u001B[A\n",
      "films on page 14:  23%|██▎       | 58/249 [14:46<47:55, 15.05s/it]\u001B[A\n",
      "films on page 14:  24%|██▎       | 59/249 [14:59<46:16, 14.61s/it]\u001B[A\n",
      "films on page 14:  24%|██▍       | 60/249 [15:14<45:49, 14.55s/it]\u001B[A\n",
      "films on page 14:  24%|██▍       | 61/249 [15:32<48:53, 15.60s/it]\u001B[A\n",
      "films on page 14:  25%|██▍       | 62/249 [15:45<46:13, 14.83s/it]\u001B[A\n",
      "films on page 14:  25%|██▌       | 63/249 [16:01<47:10, 15.22s/it]\u001B[A\n",
      "films on page 14:  26%|██▌       | 64/249 [16:17<47:59, 15.56s/it]\u001B[A\n",
      "films on page 14:  26%|██▌       | 65/249 [16:36<50:36, 16.50s/it]\u001B[A\n",
      "films on page 14:  27%|██▋       | 66/249 [16:52<49:37, 16.27s/it]\u001B[AWARNING:root:film tt1826590\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECF5FDC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143ECF5FDC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECF5FDC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECF5FDC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  27%|██▋       | 67/249 [17:04<45:33, 15.02s/it]\u001B[A\n",
      "films on page 14:  27%|██▋       | 68/249 [17:20<46:22, 15.37s/it]\u001B[A\n",
      "films on page 14:  28%|██▊       | 69/249 [17:35<45:49, 15.28s/it]\u001B[A\n",
      "films on page 14:  28%|██▊       | 70/249 [17:51<46:09, 15.47s/it]\u001B[AWARNING:root:film tt1582248\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECFCDDC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143ECFCDDC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECFCDDC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECFCDDC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  29%|██▊       | 71/249 [18:06<45:54, 15.47s/it]\u001B[A\n",
      "films on page 14:  29%|██▉       | 72/249 [18:23<46:43, 15.84s/it]\u001B[A\n",
      "films on page 14:  29%|██▉       | 73/249 [18:42<49:23, 16.84s/it]\u001B[AWARNING:root:film tt1808223\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECF39D60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143ECF39D60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECF39D60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECF39D60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  30%|██▉       | 74/249 [18:49<40:36, 13.92s/it]\u001B[A\n",
      "films on page 14:  30%|███       | 75/249 [19:03<40:26, 13.95s/it]\u001B[A\n",
      "films on page 14:  31%|███       | 76/249 [19:18<40:54, 14.19s/it]\u001B[AWARNING:root:film tt0466460\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C806B160>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143C806B160>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C806B160>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C806B160>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  31%|███       | 77/249 [19:31<39:53, 13.92s/it]\u001B[A\n",
      "films on page 14:  31%|███▏      | 78/249 [19:46<40:23, 14.17s/it]\u001B[A\n",
      "films on page 14:  32%|███▏      | 79/249 [20:08<46:34, 16.44s/it]\u001B[A\n",
      "films on page 14:  32%|███▏      | 80/249 [20:22<44:45, 15.89s/it]\u001B[AWARNING:root:film tt4645368\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C806B340>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143C806B340>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C806B340>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C806B340>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  33%|███▎      | 81/249 [20:34<40:39, 14.52s/it]\u001B[A\n",
      "films on page 14:  33%|███▎      | 82/249 [20:53<44:07, 15.85s/it]\u001B[A\n",
      "films on page 14:  33%|███▎      | 83/249 [21:11<45:48, 16.56s/it]\u001B[A\n",
      "films on page 14:  34%|███▎      | 84/249 [21:27<45:21, 16.49s/it]\u001B[A\n",
      "films on page 14:  34%|███▍      | 85/249 [21:44<45:09, 16.52s/it]\u001B[A\n",
      "films on page 14:  35%|███▍      | 86/249 [22:00<44:26, 16.36s/it]\u001B[A\n",
      "films on page 14:  35%|███▍      | 87/249 [22:18<45:37, 16.90s/it]\u001B[A\n",
      "films on page 14:  35%|███▌      | 88/249 [22:37<46:59, 17.51s/it]\u001B[A\n",
      "films on page 14:  36%|███▌      | 89/249 [22:52<45:02, 16.89s/it]\u001B[A\n",
      "films on page 14:  36%|███▌      | 90/249 [23:08<43:56, 16.58s/it]\u001B[A\n",
      "films on page 14:  37%|███▋      | 91/249 [23:23<42:31, 16.15s/it]\u001B[AWARNING:root:film tt0926762\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E730>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E045E730>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E730>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E730>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  37%|███▋      | 92/249 [23:32<36:42, 14.03s/it]\u001B[A\n",
      "films on page 14:  37%|███▋      | 93/249 [23:49<38:03, 14.64s/it]\u001B[A\n",
      "films on page 14:  38%|███▊      | 94/249 [24:03<38:02, 14.72s/it]\u001B[A\n",
      "films on page 14:  38%|███▊      | 95/249 [24:19<38:43, 15.09s/it]\u001B[A\n",
      "films on page 14:  39%|███▊      | 96/249 [24:36<39:54, 15.65s/it]\u001B[A\n",
      "films on page 14:  39%|███▉      | 97/249 [24:52<39:31, 15.60s/it]\u001B[A\n",
      "films on page 14:  39%|███▉      | 98/249 [25:10<41:16, 16.40s/it]\u001B[A\n",
      "films on page 14:  40%|███▉      | 99/249 [25:22<37:30, 15.01s/it]\u001B[A\n",
      "films on page 14:  40%|████      | 100/249 [25:48<45:41, 18.40s/it]\u001B[A\n",
      "films on page 14:  41%|████      | 101/249 [26:06<44:50, 18.18s/it]\u001B[AWARNING:root:film tt1447500\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E760>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E045E760>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E760>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E760>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  41%|████      | 102/249 [26:16<38:23, 15.67s/it]\u001B[AWARNING:root:film tt0488798\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E760>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E045E760>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E760>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E760>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  41%|████▏     | 103/249 [26:26<34:23, 14.14s/it]\u001B[AWARNING:root:film tt1821593\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E490>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E045E490>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E490>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E490>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  42%|████▏     | 104/249 [26:36<31:02, 12.84s/it]\u001B[A\n",
      "films on page 14:  42%|████▏     | 105/249 [26:55<35:16, 14.70s/it]\u001B[A\n",
      "films on page 14:  43%|████▎     | 106/249 [27:08<33:43, 14.15s/it]\u001B[A\n",
      "films on page 14:  43%|████▎     | 107/249 [27:26<36:26, 15.40s/it]\u001B[A\n",
      "films on page 14:  43%|████▎     | 108/249 [27:44<38:01, 16.18s/it]\u001B[A\n",
      "films on page 14:  44%|████▍     | 109/249 [28:02<38:40, 16.57s/it]\u001B[A\n",
      "films on page 14:  44%|████▍     | 110/249 [28:20<39:42, 17.14s/it]\u001B[AWARNING:root:film tt1424310\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF790>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143C94AF790>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF790>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF790>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  45%|████▍     | 111/249 [28:32<35:26, 15.41s/it]\u001B[A\n",
      "films on page 14:  45%|████▍     | 112/249 [28:47<35:01, 15.34s/it]\u001B[A\n",
      "films on page 14:  45%|████▌     | 113/249 [29:03<35:16, 15.56s/it]\u001B[A\n",
      "films on page 14:  46%|████▌     | 114/249 [29:18<34:29, 15.33s/it]\u001B[A\n",
      "films on page 14:  46%|████▌     | 115/249 [29:34<35:04, 15.70s/it]\u001B[A\n",
      "films on page 14:  47%|████▋     | 116/249 [29:52<36:10, 16.32s/it]\u001B[A\n",
      "films on page 14:  47%|████▋     | 117/249 [30:07<34:54, 15.87s/it]\u001B[A\n",
      "films on page 14:  47%|████▋     | 118/249 [30:24<35:48, 16.40s/it]\u001B[A\n",
      "films on page 14:  48%|████▊     | 119/249 [30:41<35:53, 16.57s/it]\u001B[A\n",
      "films on page 14:  48%|████▊     | 120/249 [30:59<36:36, 17.03s/it]\u001B[A\n",
      "films on page 14:  49%|████▊     | 121/249 [31:16<36:10, 16.95s/it]\u001B[A\n",
      "films on page 14:  49%|████▉     | 122/249 [31:33<35:57, 16.99s/it]\u001B[A\n",
      "films on page 14:  49%|████▉     | 123/249 [31:48<34:04, 16.22s/it]\u001B[A\n",
      "films on page 14:  50%|████▉     | 124/249 [32:04<33:42, 16.18s/it]\u001B[A\n",
      "films on page 14:  50%|█████     | 125/249 [32:21<33:55, 16.41s/it]\u001B[A\n",
      "films on page 14:  51%|█████     | 126/249 [32:37<33:29, 16.34s/it]\u001B[AWARNING:root:film tt0396184\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E370>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E045E370>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E370>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E370>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  51%|█████     | 127/249 [32:45<28:27, 14.00s/it]\u001B[A\n",
      "films on page 14:  51%|█████▏    | 128/249 [33:04<31:08, 15.44s/it]\u001B[A\n",
      "films on page 14:  52%|█████▏    | 129/249 [33:25<33:47, 16.89s/it]\u001B[A\n",
      "films on page 14:  52%|█████▏    | 130/249 [33:42<33:38, 16.96s/it]\u001B[A\n",
      "films on page 14:  53%|█████▎    | 131/249 [33:59<33:21, 16.96s/it]\u001B[AWARNING:root:film tt0938305\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143ECD3BFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  53%|█████▎    | 132/249 [34:06<27:19, 14.01s/it]\u001B[A\n",
      "films on page 14:  53%|█████▎    | 133/249 [34:19<26:52, 13.90s/it]\u001B[A\n",
      "films on page 14:  54%|█████▍    | 134/249 [34:33<26:44, 13.95s/it]\u001B[A\n",
      "films on page 14:  54%|█████▍    | 135/249 [34:52<28:53, 15.21s/it]\u001B[A\n",
      "films on page 14:  55%|█████▍    | 136/249 [35:08<29:30, 15.67s/it]\u001B[AWARNING:root:film tt1231586\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E045E280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  55%|█████▌    | 137/249 [35:19<26:26, 14.16s/it]\u001B[A\n",
      "films on page 14:  55%|█████▌    | 138/249 [35:38<29:09, 15.76s/it]\u001B[AWARNING:root:film tt0388182\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143C94AF130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  56%|█████▌    | 139/249 [35:53<28:22, 15.48s/it]\u001B[A\n",
      "films on page 14:  56%|█████▌    | 140/249 [36:10<28:45, 15.83s/it]\u001B[A\n",
      "films on page 14:  57%|█████▋    | 141/249 [36:31<31:03, 17.26s/it]\u001B[A\n",
      "films on page 14:  57%|█████▋    | 142/249 [36:44<28:55, 16.22s/it]\u001B[A\n",
      "films on page 14:  57%|█████▋    | 143/249 [37:02<29:24, 16.64s/it]\u001B[A\n",
      "films on page 14:  58%|█████▊    | 144/249 [37:17<28:12, 16.12s/it]\u001B[A\n",
      "films on page 14:  58%|█████▊    | 145/249 [37:30<26:28, 15.27s/it]\u001B[A\n",
      "films on page 14:  59%|█████▊    | 146/249 [37:49<28:04, 16.36s/it]\u001B[AWARNING:root:film tt0342492\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E250>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E045E250>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E250>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E250>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  59%|█████▉    | 147/249 [38:01<25:25, 14.96s/it]\u001B[A\n",
      "films on page 14:  59%|█████▉    | 148/249 [38:18<26:11, 15.56s/it]\u001B[A\n",
      "films on page 14:  60%|█████▉    | 149/249 [38:36<27:13, 16.33s/it]\u001B[A\n",
      "films on page 14:  60%|██████    | 150/249 [38:52<26:44, 16.21s/it]\u001B[A\n",
      "films on page 14:  61%|██████    | 151/249 [39:07<26:11, 16.04s/it]\u001B[A\n",
      "films on page 14:  61%|██████    | 152/249 [39:25<26:34, 16.43s/it]\u001B[A\n",
      "films on page 14:  61%|██████▏   | 153/249 [39:40<25:35, 15.99s/it]\u001B[A\n",
      "films on page 14:  62%|██████▏   | 154/249 [39:53<24:08, 15.24s/it]\u001B[A\n",
      "films on page 14:  62%|██████▏   | 155/249 [40:11<25:14, 16.11s/it]\u001B[AWARNING:root:film tt1051907\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CC324520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143CC324520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CC324520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CC324520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  63%|██████▎   | 156/249 [40:18<20:45, 13.39s/it]\u001B[A\n",
      "films on page 14:  63%|██████▎   | 157/249 [40:35<22:03, 14.38s/it]\u001B[A\n",
      "films on page 14:  63%|██████▎   | 158/249 [40:52<22:54, 15.10s/it]\u001B[AWARNING:root:film tt0860906\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E430>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E045E430>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E430>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E430>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  64%|██████▍   | 159/249 [40:59<18:59, 12.66s/it]\u001B[A\n",
      "films on page 14:  64%|██████▍   | 160/249 [41:14<19:45, 13.32s/it]\u001B[A\n",
      "films on page 14:  65%|██████▍   | 161/249 [41:27<19:21, 13.20s/it]\u001B[AWARNING:root:film tt3819668\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF4C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143C94AF4C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF4C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF4C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  65%|██████▌   | 162/249 [41:38<18:12, 12.56s/it]\u001B[AWARNING:root:film tt2608224\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BB20>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143ECD3BB20>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BB20>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BB20>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  65%|██████▌   | 163/249 [41:44<15:28, 10.80s/it]\u001B[AWARNING:root:film tt3347976\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BAF0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143ECD3BAF0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BAF0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BAF0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  66%|██████▌   | 164/249 [41:51<13:38,  9.63s/it]\u001B[A\n",
      "films on page 14:  66%|██████▋   | 165/249 [42:05<15:04, 10.77s/it]\u001B[A\n",
      "films on page 14:  67%|██████▋   | 166/249 [42:22<17:36, 12.72s/it]\u001B[AWARNING:root:film tt1740047\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF4F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143C94AF4F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF4F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF4F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  67%|██████▋   | 167/249 [42:34<17:05, 12.51s/it]\u001B[A\n",
      "films on page 14:  67%|██████▋   | 168/249 [42:47<17:02, 12.62s/it]\u001B[A\n",
      "films on page 14:  68%|██████▊   | 169/249 [43:02<17:48, 13.35s/it]\u001B[A\n",
      "films on page 14:  68%|██████▊   | 170/249 [43:16<17:49, 13.54s/it]\u001B[AWARNING:root:film tt0236027\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E7F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E045E7F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E7F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E7F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  69%|██████▊   | 171/249 [43:25<15:45, 12.13s/it]\u001B[A\n",
      "films on page 14:  69%|██████▉   | 172/249 [43:38<16:08, 12.57s/it]\u001B[A\n",
      "films on page 14:  69%|██████▉   | 173/249 [43:56<17:45, 14.02s/it]\u001B[A\n",
      "films on page 14:  70%|██████▉   | 174/249 [44:16<19:58, 15.97s/it]\u001B[A\n",
      "films on page 14:  70%|███████   | 175/249 [44:32<19:39, 15.93s/it]\u001B[A\n",
      "films on page 14:  71%|███████   | 176/249 [44:47<19:05, 15.69s/it]\u001B[A\n",
      "films on page 14:  71%|███████   | 177/249 [45:02<18:19, 15.27s/it]\u001B[A\n",
      "films on page 14:  71%|███████▏  | 178/249 [45:16<17:48, 15.05s/it]\u001B[A\n",
      "films on page 14:  72%|███████▏  | 179/249 [45:32<17:59, 15.43s/it]\u001B[A\n",
      "films on page 14:  72%|███████▏  | 180/249 [45:51<18:53, 16.43s/it]\u001B[A\n",
      "films on page 14:  73%|███████▎  | 181/249 [46:04<17:28, 15.42s/it]\u001B[A\n",
      "films on page 14:  73%|███████▎  | 182/249 [46:23<18:12, 16.30s/it]\u001B[A\n",
      "films on page 14:  73%|███████▎  | 183/249 [46:41<18:35, 16.90s/it]\u001B[A\n",
      "films on page 14:  74%|███████▍  | 184/249 [47:02<19:45, 18.23s/it]\u001B[A\n",
      "films on page 14:  74%|███████▍  | 185/249 [47:16<18:05, 16.96s/it]\u001B[A\n",
      "films on page 14:  75%|███████▍  | 186/249 [47:31<17:08, 16.32s/it]\u001B[AWARNING:root:film tt2515030\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF6D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143C94AF6D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF6D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF6D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  75%|███████▌  | 187/249 [47:44<15:44, 15.24s/it]\u001B[A\n",
      "films on page 14:  76%|███████▌  | 188/249 [48:01<16:14, 15.98s/it]\u001B[A\n",
      "films on page 14:  76%|███████▌  | 189/249 [48:19<16:35, 16.59s/it]\u001B[AWARNING:root:film tt10287954\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF790>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143C94AF790>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF790>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF790>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  76%|███████▋  | 190/249 [48:31<14:54, 15.16s/it]\u001B[A\n",
      "films on page 14:  77%|███████▋  | 191/249 [48:49<15:17, 15.81s/it]\u001B[A\n",
      "films on page 14:  77%|███████▋  | 192/249 [49:05<15:06, 15.90s/it]\u001B[A\n",
      "films on page 14:  78%|███████▊  | 193/249 [49:23<15:22, 16.47s/it]\u001B[A\n",
      "films on page 14:  78%|███████▊  | 194/249 [49:38<14:48, 16.16s/it]\u001B[A\n",
      "films on page 14:  78%|███████▊  | 195/249 [49:55<14:41, 16.32s/it]\u001B[AWARNING:root:film tt0837106\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143C94AF130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143C94AF130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  79%|███████▊  | 196/249 [50:10<14:01, 15.89s/it]\u001B[A\n",
      "films on page 14:  79%|███████▉  | 197/249 [50:25<13:41, 15.79s/it]\u001B[A\n",
      "films on page 14:  80%|███████▉  | 198/249 [50:41<13:27, 15.83s/it]\u001B[A\n",
      "films on page 14:  80%|███████▉  | 199/249 [50:55<12:49, 15.40s/it]\u001B[A\n",
      "films on page 14:  80%|████████  | 200/249 [51:20<14:45, 18.08s/it]\u001B[A\n",
      "films on page 14:  81%|████████  | 201/249 [51:35<13:49, 17.28s/it]\u001B[A\n",
      "films on page 14:  81%|████████  | 202/249 [51:50<12:57, 16.54s/it]\u001B[A\n",
      "films on page 14:  82%|████████▏ | 203/249 [52:07<12:45, 16.63s/it]\u001B[A\n",
      "films on page 14:  82%|████████▏ | 204/249 [52:27<13:20, 17.78s/it]\u001B[A\n",
      "films on page 14:  82%|████████▏ | 205/249 [52:43<12:40, 17.30s/it]\u001B[AWARNING:root:film tt2471640\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CC687CA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143CC687CA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CC687CA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CC687CA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  83%|████████▎ | 206/249 [52:50<10:04, 14.06s/it]\u001B[AWARNING:root:film tt1023500\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E220>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E045E220>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E220>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E220>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  83%|████████▎ | 207/249 [53:03<09:38, 13.77s/it]\u001B[AWARNING:root:film tt0469903\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E250>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E045E250>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E250>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E250>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  84%|████████▎ | 208/249 [53:17<09:25, 13.79s/it]\u001B[A\n",
      "films on page 14:  84%|████████▍ | 209/249 [53:34<09:46, 14.67s/it]\u001B[AWARNING:root:film tt0177858\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E045E130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  84%|████████▍ | 210/249 [53:48<09:26, 14.53s/it]\u001B[A\n",
      "films on page 14:  85%|████████▍ | 211/249 [54:03<09:24, 14.85s/it]\u001B[A\n",
      "films on page 14:  85%|████████▌ | 212/249 [54:23<10:00, 16.22s/it]\u001B[AWARNING:root:film tt0795434\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E5B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E045E5B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E5B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E5B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  86%|████████▌ | 213/249 [54:35<09:04, 15.13s/it]\u001B[A\n",
      "films on page 14:  86%|████████▌ | 214/249 [54:50<08:44, 14.99s/it]\u001B[A\n",
      "films on page 14:  86%|████████▋ | 215/249 [55:07<08:51, 15.62s/it]\u001B[A\n",
      "films on page 14:  87%|████████▋ | 216/249 [55:25<09:01, 16.41s/it]\u001B[A\n",
      "films on page 14:  87%|████████▋ | 217/249 [55:41<08:36, 16.14s/it]\u001B[A\n",
      "films on page 14:  88%|████████▊ | 218/249 [55:59<08:35, 16.64s/it]\u001B[A\n",
      "films on page 14:  88%|████████▊ | 219/249 [56:14<08:09, 16.31s/it]\u001B[AWARNING:root:film tt1921149\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BA90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143ECD3BA90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BA90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BA90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  88%|████████▊ | 220/249 [56:24<06:53, 14.27s/it]\u001B[AWARNING:root:film tt1508675\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BA90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143ECD3BA90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BA90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BA90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  89%|████████▉ | 221/249 [56:33<05:55, 12.68s/it]\u001B[A\n",
      "films on page 14:  89%|████████▉ | 222/249 [56:48<06:01, 13.39s/it]\u001B[A\n",
      "films on page 14:  90%|████████▉ | 223/249 [57:04<06:12, 14.33s/it]\u001B[AWARNING:root:film tt2388637\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E045E070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  90%|████████▉ | 224/249 [57:15<05:33, 13.34s/it]\u001B[A\n",
      "films on page 14:  90%|█████████ | 225/249 [57:29<05:19, 13.33s/it]\u001B[A\n",
      "films on page 14:  91%|█████████ | 226/249 [57:44<05:20, 13.92s/it]\u001B[AWARNING:root:film tt1926313\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E370>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E045E370>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E370>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E370>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  91%|█████████ | 227/249 [57:59<05:15, 14.34s/it]\u001B[A\n",
      "films on page 14:  92%|█████████▏| 228/249 [58:15<05:08, 14.68s/it]\u001B[A\n",
      "films on page 14:  92%|█████████▏| 229/249 [58:32<05:06, 15.31s/it]\u001B[A\n",
      "films on page 14:  92%|█████████▏| 230/249 [58:48<04:57, 15.64s/it]\u001B[A\n",
      "films on page 14:  93%|█████████▎| 231/249 [59:08<05:02, 16.83s/it]\u001B[A\n",
      "films on page 14:  93%|█████████▎| 232/249 [59:24<04:43, 16.66s/it]\u001B[A\n",
      "films on page 14:  94%|█████████▎| 233/249 [59:35<03:58, 14.91s/it]\u001B[AWARNING:root:film tt1233381\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143ECD3BFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  94%|█████████▍| 234/249 [59:45<03:23, 13.57s/it]\u001B[AWARNING:root:film tt1016301\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BFA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143ECD3BFA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BFA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BFA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  94%|█████████▍| 235/249 [59:55<02:53, 12.36s/it]\u001B[AWARNING:root:film tt0425333\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CC324FA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143CC324FA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CC324FA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CC324FA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  95%|█████████▍| 236/249 [59:59<02:10, 10.02s/it]\u001B[A\n",
      "films on page 14:  95%|█████████▌| 237/249 [1:00:18<02:31, 12.62s/it]\u001B[A\n",
      "films on page 14:  96%|█████████▌| 238/249 [1:00:33<02:28, 13.48s/it]\u001B[A\n",
      "films on page 14:  96%|█████████▌| 239/249 [1:00:52<02:28, 14.88s/it]\u001B[A\n",
      "films on page 14:  96%|█████████▋| 240/249 [1:01:11<02:26, 16.23s/it]\u001B[A\n",
      "films on page 14:  97%|█████████▋| 241/249 [1:01:28<02:11, 16.41s/it]\u001B[A\n",
      "films on page 14:  97%|█████████▋| 242/249 [1:01:48<02:03, 17.68s/it]\u001B[AWARNING:root:film tt1230385\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BE50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143ECD3BE50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BE50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143ECD3BE50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  98%|█████████▊| 243/249 [1:01:59<01:32, 15.46s/it]\u001B[A\n",
      "films on page 14:  98%|█████████▊| 244/249 [1:02:15<01:18, 15.63s/it]\u001B[AWARNING:root:film tt1349451\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E640>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E045E640>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E640>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E640>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  98%|█████████▊| 245/249 [1:02:25<00:56, 14.00s/it]\u001B[A\n",
      "films on page 14:  99%|█████████▉| 246/249 [1:02:46<00:48, 16.16s/it]\u001B[AWARNING:root:film tt0464913\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E045E130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E045E130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 14:  99%|█████████▉| 247/249 [1:02:59<00:30, 15.11s/it]\u001B[A\n",
      "films on page 14: 100%|█████████▉| 248/249 [1:03:11<00:14, 14.41s/it]\u001B[A\n",
      "films on page 14: 100%|██████████| 249/249 [1:03:27<00:00, 15.29s/it]\u001B[A\n",
      "pages:  90%|█████████ | 9/10 [10:16:18<1:07:28, 4048.39s/it]\n",
      "films on page 15:   0%|          | 0/250 [00:00<?, ?it/s]\u001B[A\n",
      "films on page 15:   0%|          | 1/250 [00:14<58:07, 14.01s/it]\u001B[AWARNING:root:film tt0339230\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DF9C7520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143DF9C7520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DF9C7520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DF9C7520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:   1%|          | 2/250 [00:24<49:28, 11.97s/it]\u001B[A\n",
      "films on page 15:   1%|          | 3/250 [00:44<1:03:20, 15.39s/it]\u001B[A\n",
      "films on page 15:   2%|▏         | 4/250 [01:02<1:08:01, 16.59s/it]\u001B[A\n",
      "films on page 15:   2%|▏         | 5/250 [01:16<1:03:51, 15.64s/it]\u001B[AWARNING:root:film tt1876277\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DFA30FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143DFA30FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DFA30FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DFA30FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:   2%|▏         | 6/250 [01:26<55:51, 13.74s/it]  \u001B[AWARNING:root:film tt0961097\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DFA2EFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143DFA2EFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DFA2EFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DFA2EFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:   3%|▎         | 7/250 [01:38<53:57, 13.32s/it]\u001B[A\n",
      "films on page 15:   3%|▎         | 8/250 [01:58<1:01:23, 15.22s/it]\u001B[AWARNING:root:film tt1179025\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DFA56FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143DFA56FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DFA56FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143DFA56FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:   4%|▎         | 9/250 [02:08<55:30, 13.82s/it]  \u001B[A\n",
      "films on page 15:   4%|▍         | 10/250 [02:23<56:38, 14.16s/it]\u001B[A\n",
      "films on page 15:   4%|▍         | 11/250 [02:39<57:41, 14.48s/it]\u001B[A\n",
      "films on page 15:   5%|▍         | 12/250 [02:51<54:56, 13.85s/it]\u001B[A\n",
      "films on page 15:   5%|▌         | 13/250 [03:08<58:39, 14.85s/it]\u001B[A\n",
      "films on page 15:   6%|▌         | 14/250 [03:24<59:07, 15.03s/it]\u001B[A\n",
      "films on page 15:   6%|▌         | 15/250 [03:39<59:51, 15.28s/it]\u001B[AWARNING:root:film tt2316801\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E7931FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E7931FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E7931FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E7931FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:   6%|▋         | 16/250 [03:54<58:24, 14.98s/it]\u001B[A\n",
      "films on page 15:   7%|▋         | 17/250 [04:10<1:00:01, 15.46s/it]\u001B[A\n",
      "films on page 15:   7%|▋         | 18/250 [04:33<1:07:37, 17.49s/it]\u001B[A\n",
      "films on page 15:   8%|▊         | 19/250 [04:48<1:05:20, 16.97s/it]\u001B[A\n",
      "films on page 15:   8%|▊         | 20/250 [05:03<1:02:54, 16.41s/it]\u001B[A\n",
      "films on page 15:   8%|▊         | 21/250 [05:19<1:02:04, 16.27s/it]\u001B[A\n",
      "films on page 15:   9%|▉         | 22/250 [05:34<1:00:05, 15.81s/it]\u001B[A\n",
      "films on page 15:   9%|▉         | 23/250 [05:54<1:04:15, 16.98s/it]\u001B[A\n",
      "films on page 15:  10%|▉         | 24/250 [06:07<1:00:15, 16.00s/it]\u001B[A\n",
      "films on page 15:  10%|█         | 25/250 [06:26<1:03:10, 16.84s/it]\u001B[A\n",
      "films on page 15:  10%|█         | 26/250 [06:46<1:06:29, 17.81s/it]\u001B[A\n",
      "films on page 15:  11%|█         | 27/250 [07:03<1:05:12, 17.55s/it]\u001B[A\n",
      "films on page 15:  11%|█         | 28/250 [07:18<1:01:19, 16.58s/it]\u001B[A\n",
      "films on page 15:  12%|█▏        | 29/250 [07:34<1:01:23, 16.67s/it]\u001B[A\n",
      "films on page 15:  12%|█▏        | 30/250 [07:52<1:02:25, 17.03s/it]\u001B[A\n",
      "films on page 15:  12%|█▏        | 31/250 [08:08<1:00:50, 16.67s/it]\u001B[AWARNING:root:film tt1847731\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E7B26FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E7B26FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E7B26FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E7B26FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  13%|█▎        | 32/250 [08:23<58:19, 16.05s/it]  \u001B[A\n",
      "films on page 15:  13%|█▎        | 33/250 [08:39<57:42, 15.95s/it]\u001B[A\n",
      "films on page 15:  14%|█▎        | 34/250 [08:53<55:36, 15.45s/it]\u001B[A\n",
      "films on page 15:  14%|█▍        | 35/250 [09:09<56:24, 15.74s/it]\u001B[A\n",
      "films on page 15:  14%|█▍        | 36/250 [09:26<56:58, 15.97s/it]\u001B[AWARNING:root:film tt1322930\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E7BAD520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E7BAD520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E7BAD520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E7BAD520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  15%|█▍        | 37/250 [09:35<49:12, 13.86s/it]\u001B[AWARNING:root:film tt1753496\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E7BB5FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E7BB5FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E7BB5FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E7BB5FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  15%|█▌        | 38/250 [09:46<45:49, 12.97s/it]\u001B[A\n",
      "films on page 15:  16%|█▌        | 39/250 [10:02<49:10, 13.98s/it]\u001B[A\n",
      "films on page 15:  16%|█▌        | 40/250 [10:21<54:14, 15.50s/it]\u001B[A\n",
      "films on page 15:  16%|█▋        | 41/250 [10:40<57:30, 16.51s/it]\u001B[AWARNING:root:film tt0422091\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E7C15FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E7C15FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E7C15FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E7C15FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  17%|█▋        | 42/250 [10:52<52:51, 15.25s/it]\u001B[A\n",
      "films on page 15:  17%|█▋        | 43/250 [11:11<56:32, 16.39s/it]\u001B[A\n",
      "films on page 15:  18%|█▊        | 44/250 [11:24<52:10, 15.20s/it]\u001B[A\n",
      "films on page 15:  18%|█▊        | 45/250 [11:42<54:55, 16.07s/it]\u001B[A\n",
      "films on page 15:  18%|█▊        | 46/250 [11:59<56:11, 16.52s/it]\u001B[AWARNING:root:film tt0416960\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9A5E520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E9A5E520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9A5E520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9A5E520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  19%|█▉        | 47/250 [12:11<51:06, 15.11s/it]\u001B[A\n",
      "films on page 15:  19%|█▉        | 48/250 [12:27<51:41, 15.35s/it]\u001B[A\n",
      "films on page 15:  20%|█▉        | 49/250 [12:43<52:09, 15.57s/it]\u001B[A\n",
      "films on page 15:  20%|██        | 50/250 [13:01<54:31, 16.36s/it]\u001B[AWARNING:root:film tt1816518\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9AC4FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E9AC4FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9AC4FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9AC4FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  20%|██        | 51/250 [13:25<1:01:09, 18.44s/it]\u001B[AWARNING:root:film tt1650048\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9ACF9D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E9ACF9D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9ACF9D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9ACF9D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  21%|██        | 52/250 [13:34<51:59, 15.76s/it]  \u001B[A\n",
      "films on page 15:  21%|██        | 53/250 [13:51<52:59, 16.14s/it]\u001B[AWARNING:root:film tt2917388\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9AEB520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E9AEB520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9AEB520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E9AEB520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  22%|██▏       | 54/250 [13:59<44:36, 13.65s/it]\u001B[A\n",
      "films on page 15:  22%|██▏       | 55/250 [14:20<51:36, 15.88s/it]\u001B[A\n",
      "films on page 15:  22%|██▏       | 56/250 [14:34<49:43, 15.38s/it]\u001B[A\n",
      "films on page 15:  23%|██▎       | 57/250 [14:55<54:23, 16.91s/it]\u001B[A\n",
      "films on page 15:  23%|██▎       | 58/250 [15:09<51:50, 16.20s/it]\u001B[A\n",
      "films on page 15:  24%|██▎       | 59/250 [15:25<51:15, 16.10s/it]\u001B[A\n",
      "films on page 15:  24%|██▍       | 60/250 [15:41<51:08, 16.15s/it]\u001B[AWARNING:root:film tt3668162\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EEFFBFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EEFFBFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EEFFBFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EEFFBFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  24%|██▍       | 61/250 [15:54<47:44, 15.16s/it]\u001B[A\n",
      "films on page 15:  25%|██▍       | 62/250 [16:10<47:42, 15.22s/it]\u001B[A\n",
      "films on page 15:  25%|██▌       | 63/250 [16:26<48:05, 15.43s/it]\u001B[AWARNING:root:film tt4088268\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF045520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF045520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF045520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF045520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  26%|██▌       | 64/250 [16:37<44:22, 14.31s/it]\u001B[A\n",
      "films on page 15:  26%|██▌       | 65/250 [16:57<49:04, 15.92s/it]\u001B[A\n",
      "films on page 15:  26%|██▋       | 66/250 [17:15<51:02, 16.65s/it]\u001B[A\n",
      "films on page 15:  27%|██▋       | 67/250 [17:34<53:07, 17.42s/it]\u001B[A\n",
      "films on page 15:  27%|██▋       | 68/250 [17:48<49:24, 16.29s/it]\u001B[A\n",
      "films on page 15:  28%|██▊       | 69/250 [18:05<49:43, 16.48s/it]\u001B[A\n",
      "films on page 15:  28%|██▊       | 70/250 [18:21<49:18, 16.44s/it]\u001B[AWARNING:root:film tt1434447\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF114FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF114FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF114FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF114FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  28%|██▊       | 71/250 [18:35<46:27, 15.57s/it]\u001B[A\n",
      "films on page 15:  29%|██▉       | 72/250 [18:52<47:34, 16.03s/it]\u001B[A\n",
      "films on page 15:  29%|██▉       | 73/250 [19:08<47:17, 16.03s/it]\u001B[A\n",
      "films on page 15:  30%|██▉       | 74/250 [19:23<46:08, 15.73s/it]\u001B[A\n",
      "films on page 15:  30%|███       | 75/250 [19:40<46:51, 16.06s/it]\u001B[A\n",
      "films on page 15:  30%|███       | 76/250 [19:58<48:27, 16.71s/it]\u001B[A\n",
      "films on page 15:  31%|███       | 77/250 [20:12<45:18, 15.72s/it]\u001B[A\n",
      "films on page 15:  31%|███       | 78/250 [20:30<47:13, 16.47s/it]\u001B[A\n",
      "films on page 15:  32%|███▏      | 79/250 [20:46<47:08, 16.54s/it]\u001B[A\n",
      "films on page 15:  32%|███▏      | 80/250 [21:01<44:45, 15.80s/it]\u001B[AWARNING:root:film tt0416185\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF252FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF252FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF252FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF252FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  32%|███▏      | 81/250 [21:13<41:37, 14.78s/it]\u001B[A\n",
      "films on page 15:  33%|███▎      | 82/250 [21:27<40:52, 14.60s/it]\u001B[A\n",
      "films on page 15:  33%|███▎      | 83/250 [21:45<43:34, 15.66s/it]\u001B[AWARNING:root:film tt1594918\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E8215BB0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143E8215BB0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E8215BB0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143E8215BB0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  34%|███▎      | 84/250 [21:51<35:03, 12.67s/it]\u001B[AWARNING:root:film tt4025514\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF298FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF298FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF298FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF298FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  34%|███▍      | 85/250 [22:05<35:46, 13.01s/it]\u001B[A\n",
      "films on page 15:  34%|███▍      | 86/250 [22:22<39:03, 14.29s/it]\u001B[AWARNING:root:film tt0409184\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF2BD520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF2BD520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF2BD520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF2BD520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  35%|███▍      | 87/250 [22:33<36:06, 13.29s/it]\u001B[A\n",
      "films on page 15:  35%|███▌      | 88/250 [22:51<40:00, 14.82s/it]\u001B[AWARNING:root:film tt1639084\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF2E3FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF2E3FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF2E3FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF2E3FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  36%|███▌      | 89/250 [23:03<37:34, 14.00s/it]\u001B[A\n",
      "films on page 15:  36%|███▌      | 90/250 [23:15<35:35, 13.35s/it]\u001B[AWARNING:root:film tt1320239\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF309FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF309FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF309FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF309FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  36%|███▋      | 91/250 [23:26<33:12, 12.53s/it]\u001B[A\n",
      "films on page 15:  37%|███▋      | 92/250 [23:42<35:36, 13.52s/it]\u001B[A\n",
      "films on page 15:  37%|███▋      | 93/250 [23:58<37:11, 14.21s/it]\u001B[A\n",
      "films on page 15:  38%|███▊      | 94/250 [24:15<39:11, 15.07s/it]\u001B[A\n",
      "films on page 15:  38%|███▊      | 95/250 [24:31<39:49, 15.41s/it]\u001B[AWARNING:root:film tt2011971\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF392FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF392FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF392FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF392FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  38%|███▊      | 96/250 [24:43<37:03, 14.44s/it]\u001B[A\n",
      "films on page 15:  39%|███▉      | 97/250 [24:56<35:27, 13.90s/it]\u001B[A\n",
      "films on page 15:  39%|███▉      | 98/250 [25:12<36:51, 14.55s/it]\u001B[A\n",
      "films on page 15:  40%|███▉      | 99/250 [25:29<38:44, 15.40s/it]\u001B[A\n",
      "films on page 15:  40%|████      | 100/250 [25:46<39:55, 15.97s/it]\u001B[A\n",
      "films on page 15:  40%|████      | 101/250 [26:04<40:46, 16.42s/it]\u001B[A\n",
      "films on page 15:  41%|████      | 102/250 [26:18<39:08, 15.87s/it]\u001B[A\n",
      "films on page 15:  41%|████      | 103/250 [26:37<40:40, 16.60s/it]\u001B[AWARNING:root:film tt4987556\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF479FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF479FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF479FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF479FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  42%|████▏     | 104/250 [26:52<39:14, 16.13s/it]\u001B[A\n",
      "films on page 15:  42%|████▏     | 105/250 [27:08<38:40, 16.00s/it]\u001B[AWARNING:root:film tt6060156\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF4AAFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF4AAFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF4AAFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF4AAFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  42%|████▏     | 106/250 [27:20<36:12, 15.09s/it]\u001B[A\n",
      "films on page 15:  43%|████▎     | 107/250 [27:39<38:15, 16.05s/it]\u001B[AWARNING:root:film tt0293007\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF4CF520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF4CF520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF4CF520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF4CF520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  43%|████▎     | 108/250 [27:48<32:50, 13.88s/it]\u001B[A\n",
      "films on page 15:  44%|████▎     | 109/250 [28:06<35:52, 15.26s/it]\u001B[AWARNING:root:film tt1068641\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF4E9FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF4E9FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF4E9FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF4E9FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  44%|████▍     | 110/250 [28:20<34:26, 14.76s/it]\u001B[A\n",
      "films on page 15:  44%|████▍     | 111/250 [28:35<34:29, 14.89s/it]\u001B[A\n",
      "films on page 15:  45%|████▍     | 112/250 [28:50<34:39, 15.07s/it]\u001B[AWARNING:root:film tt0790623\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF53BFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF53BFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF53BFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF53BFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  45%|████▌     | 113/250 [29:00<30:37, 13.41s/it]\u001B[A\n",
      "films on page 15:  46%|████▌     | 114/250 [29:19<34:28, 15.21s/it]\u001B[A\n",
      "films on page 15:  46%|████▌     | 115/250 [29:33<33:18, 14.81s/it]\u001B[A\n",
      "films on page 15:  46%|████▋     | 116/250 [29:50<34:32, 15.47s/it]\u001B[AWARNING:root:film tt1747958\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF5A9FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF5A9FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF5A9FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF5A9FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  47%|████▋     | 117/250 [30:01<31:25, 14.18s/it]\u001B[AWARNING:root:film tt2621000\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF5A9C70>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF5A9C70>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF5A9C70>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF5A9C70>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  47%|████▋     | 118/250 [30:12<29:05, 13.23s/it]\u001B[A\n",
      "films on page 15:  48%|████▊     | 119/250 [30:29<30:56, 14.17s/it]\u001B[A\n",
      "films on page 15:  48%|████▊     | 120/250 [30:46<32:57, 15.21s/it]\u001B[A\n",
      "films on page 15:  48%|████▊     | 121/250 [31:05<34:53, 16.23s/it]\u001B[AWARNING:root:film tt1183665\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF612FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF612FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF612FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF612FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  49%|████▉     | 122/250 [31:18<32:40, 15.32s/it]\u001B[A\n",
      "films on page 15:  49%|████▉     | 123/250 [31:31<30:42, 14.51s/it]\u001B[A\n",
      "films on page 15:  50%|████▉     | 124/250 [31:45<30:06, 14.34s/it]\u001B[A\n",
      "films on page 15:  50%|█████     | 125/250 [32:02<31:43, 15.23s/it]\u001B[A\n",
      "films on page 15:  50%|█████     | 126/250 [32:16<30:59, 15.00s/it]\u001B[A\n",
      "films on page 15:  51%|█████     | 127/250 [32:29<29:27, 14.37s/it]\u001B[AWARNING:root:film tt2195548\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF6BDFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF6BDFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF6BDFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF6BDFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  51%|█████     | 128/250 [32:41<27:14, 13.40s/it]\u001B[A\n",
      "films on page 15:  52%|█████▏    | 129/250 [32:57<29:11, 14.47s/it]\u001B[A\n",
      "films on page 15:  52%|█████▏    | 130/250 [33:12<29:09, 14.58s/it]\u001B[A\n",
      "films on page 15:  52%|█████▏    | 131/250 [33:27<29:02, 14.64s/it]\u001B[A\n",
      "films on page 15:  53%|█████▎    | 132/250 [33:42<28:57, 14.72s/it]\u001B[AWARNING:root:film tt0388139\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF746FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF746FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF746FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF746FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  53%|█████▎    | 133/250 [33:52<25:48, 13.24s/it]\u001B[A\n",
      "films on page 15:  54%|█████▎    | 134/250 [34:05<25:52, 13.38s/it]\u001B[A\n",
      "films on page 15:  54%|█████▍    | 135/250 [34:22<27:24, 14.30s/it]\u001B[AWARNING:root:film tt0875113\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF6C0940>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF6C0940>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF6C0940>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF6C0940>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  54%|█████▍    | 136/250 [34:27<21:39, 11.40s/it]\u001B[A\n",
      "films on page 15:  55%|█████▍    | 137/250 [34:44<24:58, 13.26s/it]\u001B[A\n",
      "films on page 15:  55%|█████▌    | 138/250 [35:03<27:45, 14.87s/it]\u001B[AWARNING:root:film tt5221584\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF7DB520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF7DB520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF7DB520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF7DB520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  56%|█████▌    | 139/250 [35:13<25:03, 13.55s/it]\u001B[A\n",
      "films on page 15:  56%|█████▌    | 140/250 [35:29<26:10, 14.28s/it]\u001B[AWARNING:root:film tt2406252\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF7FAFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF7FAFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF7FAFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF7FAFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  56%|█████▋    | 141/250 [35:42<25:22, 13.97s/it]\u001B[AWARNING:root:film tt8649186\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF7F3FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF7F3FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF7F3FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF7F3FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  57%|█████▋    | 142/250 [35:58<25:53, 14.39s/it]\u001B[A\n",
      "films on page 15:  57%|█████▋    | 143/250 [36:13<26:15, 14.72s/it]\u001B[A\n",
      "films on page 15:  58%|█████▊    | 144/250 [36:30<26:56, 15.25s/it]\u001B[A\n",
      "films on page 15:  58%|█████▊    | 145/250 [36:46<27:05, 15.48s/it]\u001B[A\n",
      "films on page 15:  58%|█████▊    | 146/250 [37:03<27:29, 15.86s/it]\u001B[A\n",
      "films on page 15:  59%|█████▉    | 147/250 [37:18<27:02, 15.76s/it]\u001B[A\n",
      "films on page 15:  59%|█████▉    | 148/250 [37:35<27:22, 16.11s/it]\u001B[A\n",
      "films on page 15:  60%|█████▉    | 149/250 [37:54<28:33, 16.96s/it]\u001B[A\n",
      "films on page 15:  60%|██████    | 150/250 [38:09<27:17, 16.38s/it]\u001B[A\n",
      "films on page 15:  60%|██████    | 151/250 [38:39<33:30, 20.31s/it]\u001B[AWARNING:root:film tt1654523\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF92D850>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EF92D850>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF92D850>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EF92D850>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  61%|██████    | 152/250 [38:55<31:06, 19.05s/it]\u001B[A\n",
      "films on page 15:  61%|██████    | 153/250 [39:12<29:50, 18.46s/it]\u001B[A\n",
      "films on page 15:  62%|██████▏   | 154/250 [39:27<27:47, 17.37s/it]\u001B[A\n",
      "films on page 15:  62%|██████▏   | 155/250 [39:41<26:12, 16.55s/it]\u001B[A\n",
      "films on page 15:  62%|██████▏   | 156/250 [39:56<25:08, 16.05s/it]\u001B[A\n",
      "films on page 15:  63%|██████▎   | 157/250 [40:11<24:23, 15.74s/it]\u001B[A\n",
      "films on page 15:  63%|██████▎   | 158/250 [40:23<22:12, 14.49s/it]\u001B[AWARNING:root:film tt0374271\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFA01FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EFA01FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFA01FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFA01FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  64%|██████▎   | 159/250 [40:33<19:57, 13.15s/it]\u001B[A\n",
      "films on page 15:  64%|██████▍   | 160/250 [40:50<21:29, 14.33s/it]\u001B[A\n",
      "films on page 15:  64%|██████▍   | 161/250 [41:05<21:38, 14.59s/it]\u001B[A\n",
      "films on page 15:  65%|██████▍   | 162/250 [41:22<22:16, 15.19s/it]\u001B[A\n",
      "films on page 15:  65%|██████▌   | 163/250 [41:35<21:13, 14.63s/it]\u001B[A\n",
      "films on page 15:  66%|██████▌   | 164/250 [41:50<21:04, 14.70s/it]\u001B[A\n",
      "films on page 15:  66%|██████▌   | 165/250 [42:05<21:12, 14.97s/it]\u001B[AWARNING:root:film tt2061702\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFACA520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EFACA520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFACA520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFACA520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  66%|██████▋   | 166/250 [42:17<19:42, 14.08s/it]\u001B[A\n",
      "films on page 15:  67%|██████▋   | 167/250 [42:36<21:22, 15.45s/it]\u001B[A\n",
      "films on page 15:  67%|██████▋   | 168/250 [42:54<22:20, 16.35s/it]\u001B[AWARNING:root:film tt0995868\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFB17FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EFB17FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFB17FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFB17FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  68%|██████▊   | 169/250 [43:08<20:46, 15.39s/it]\u001B[AWARNING:root:film tt1533749\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFB1B520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EFB1B520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFB1B520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFB1B520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  68%|██████▊   | 170/250 [43:15<17:31, 13.15s/it]\u001B[A\n",
      "films on page 15:  68%|██████▊   | 171/250 [43:28<17:14, 13.10s/it]\u001B[A\n",
      "films on page 15:  69%|██████▉   | 172/250 [43:41<16:43, 12.86s/it]\u001B[A\n",
      "films on page 15:  69%|██████▉   | 173/250 [44:02<19:32, 15.23s/it]\u001B[A\n",
      "films on page 15:  70%|██████▉   | 174/250 [44:19<19:58, 15.77s/it]\u001B[A\n",
      "films on page 15:  70%|███████   | 175/250 [44:35<20:00, 16.00s/it]\u001B[A\n",
      "films on page 15:  70%|███████   | 176/250 [44:50<19:09, 15.54s/it]\u001B[A\n",
      "films on page 15:  71%|███████   | 177/250 [45:08<20:04, 16.50s/it]\u001B[A\n",
      "films on page 15:  71%|███████   | 178/250 [45:23<19:17, 16.08s/it]\u001B[A\n",
      "films on page 15:  72%|███████▏  | 179/250 [45:41<19:38, 16.60s/it]\u001B[A\n",
      "films on page 15:  72%|███████▏  | 180/250 [45:59<19:54, 17.07s/it]\u001B[A\n",
      "films on page 15:  72%|███████▏  | 181/250 [46:15<19:14, 16.72s/it]\u001B[AWARNING:root:film tt0220099\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFC9AFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EFC9AFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFC9AFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFC9AFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  73%|███████▎  | 182/250 [46:28<17:35, 15.52s/it]\u001B[AWARNING:root:film tt0376127\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFC9EFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EFC9EFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFC9EFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFC9EFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  73%|███████▎  | 183/250 [46:39<15:44, 14.10s/it]\u001B[A\n",
      "films on page 15:  74%|███████▎  | 184/250 [46:54<16:01, 14.57s/it]\u001B[AWARNING:root:film tt2390237\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFCBB520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EFCBB520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFCBB520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFCBB520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  74%|███████▍  | 185/250 [47:04<14:00, 12.92s/it]\u001B[A\n",
      "films on page 15:  74%|███████▍  | 186/250 [47:19<14:35, 13.69s/it]\u001B[A\n",
      "films on page 15:  75%|███████▍  | 187/250 [47:34<14:52, 14.17s/it]\u001B[A\n",
      "films on page 15:  75%|███████▌  | 188/250 [47:51<15:29, 14.99s/it]\u001B[AWARNING:root:film tt0248408\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFD23FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EFD23FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFD23FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFD23FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  76%|███████▌  | 189/250 [48:00<13:27, 13.24s/it]\u001B[AWARNING:root:film tt0913445\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFD29520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EFD29520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFD29520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFD29520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  76%|███████▌  | 190/250 [48:11<12:19, 12.33s/it]\u001B[A\n",
      "films on page 15:  76%|███████▋  | 191/250 [48:25<12:35, 12.81s/it]\u001B[AWARNING:root:film tt1032825\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFD4BFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EFD4BFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFD4BFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFD4BFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  77%|███████▋  | 192/250 [48:37<12:20, 12.77s/it]\u001B[A\n",
      "films on page 15:  77%|███████▋  | 193/250 [48:55<13:39, 14.38s/it]\u001B[A\n",
      "films on page 15:  78%|███████▊  | 194/250 [49:12<14:01, 15.02s/it]\u001B[AWARNING:root:film tt1332134\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFD91520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EFD91520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFD91520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFD91520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  78%|███████▊  | 195/250 [49:21<12:05, 13.18s/it]\u001B[A\n",
      "films on page 15:  78%|███████▊  | 196/250 [49:37<12:39, 14.07s/it]\u001B[AWARNING:root:film tt0493949\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFDB0FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EFDB0FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFDB0FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFDB0FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  79%|███████▉  | 197/250 [49:48<11:39, 13.19s/it]\u001B[A\n",
      "films on page 15:  79%|███████▉  | 198/250 [50:04<12:10, 14.05s/it]\u001B[A\n",
      "films on page 15:  80%|███████▉  | 199/250 [50:20<12:25, 14.62s/it]\u001B[AWARNING:root:film tt2245003\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFE05FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EFE05FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFE05FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFE05FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  80%|████████  | 200/250 [50:33<11:44, 14.09s/it]\u001B[A\n",
      "films on page 15:  80%|████████  | 201/250 [50:51<12:31, 15.34s/it]\u001B[AWARNING:root:film tt2249221\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFE2BFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EFE2BFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFE2BFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFE2BFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  81%|████████  | 202/250 [51:04<11:38, 14.55s/it]\u001B[AWARNING:root:film tt2398149\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFE19FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EFE19FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFE19FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFE19FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  81%|████████  | 203/250 [51:16<10:52, 13.88s/it]\u001B[A\n",
      "films on page 15:  82%|████████▏ | 204/250 [51:32<11:02, 14.40s/it]\u001B[A\n",
      "films on page 15:  82%|████████▏ | 205/250 [51:48<11:07, 14.84s/it]\u001B[A\n",
      "films on page 15:  82%|████████▏ | 206/250 [52:05<11:20, 15.47s/it]\u001B[AWARNING:root:film tt0920458\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFE90520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EFE90520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFE90520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFE90520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  83%|████████▎ | 207/250 [52:12<09:26, 13.17s/it]\u001B[A\n",
      "films on page 15:  83%|████████▎ | 208/250 [52:28<09:39, 13.79s/it]\u001B[A\n",
      "films on page 15:  84%|████████▎ | 209/250 [52:44<09:59, 14.63s/it]\u001B[A\n",
      "films on page 15:  84%|████████▍ | 210/250 [53:04<10:41, 16.04s/it]\u001B[AWARNING:root:film tt7832124\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFF01520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EFF01520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFF01520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFF01520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  84%|████████▍ | 211/250 [53:13<09:06, 14.02s/it]\u001B[A\n",
      "films on page 15:  85%|████████▍ | 212/250 [53:29<09:18, 14.70s/it]\u001B[A\n",
      "films on page 15:  85%|████████▌ | 213/250 [53:49<09:56, 16.13s/it]\u001B[A\n",
      "films on page 15:  86%|████████▌ | 214/250 [54:06<09:58, 16.64s/it]\u001B[A\n",
      "films on page 15:  86%|████████▌ | 215/250 [54:24<09:53, 16.94s/it]\u001B[AWARNING:root:film tt1807950\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CC687F40>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143CC687F40>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CC687F40>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143CC687F40>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  86%|████████▋ | 216/250 [54:34<08:19, 14.69s/it]\u001B[A\n",
      "films on page 15:  87%|████████▋ | 217/250 [54:47<07:51, 14.30s/it]\u001B[A\n",
      "films on page 15:  87%|████████▋ | 218/250 [55:02<07:43, 14.48s/it]\u001B[AWARNING:root:film tt0953318\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFFDE520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143EFFDE520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFFDE520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143EFFDE520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  88%|████████▊ | 219/250 [55:14<07:04, 13.70s/it]\u001B[A\n",
      "films on page 15:  88%|████████▊ | 220/250 [55:29<07:06, 14.21s/it]\u001B[A\n",
      "films on page 15:  88%|████████▊ | 221/250 [55:45<07:05, 14.69s/it]\u001B[AWARNING:root:film tt1291652\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143F0022520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143F0022520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143F0022520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143F0022520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  89%|████████▉ | 222/250 [55:57<06:29, 13.91s/it]\u001B[A\n",
      "films on page 15:  89%|████████▉ | 223/250 [56:12<06:26, 14.33s/it]\u001B[A\n",
      "films on page 15:  90%|████████▉ | 224/250 [56:31<06:44, 15.57s/it]\u001B[A\n",
      "films on page 15:  90%|█████████ | 225/250 [56:47<06:33, 15.74s/it]\u001B[A\n",
      "films on page 15:  90%|█████████ | 226/250 [57:02<06:15, 15.67s/it]\u001B[A\n",
      "films on page 15:  91%|█████████ | 227/250 [57:19<06:04, 15.87s/it]\u001B[A\n",
      "films on page 15:  91%|█████████ | 228/250 [57:33<05:36, 15.28s/it]\u001B[A\n",
      "films on page 15:  92%|█████████▏| 229/250 [57:52<05:44, 16.41s/it]\u001B[A\n",
      "films on page 15:  92%|█████████▏| 230/250 [58:08<05:24, 16.25s/it]\u001B[A\n",
      "films on page 15:  92%|█████████▏| 231/250 [58:25<05:12, 16.46s/it]\u001B[A\n",
      "films on page 15:  93%|█████████▎| 232/250 [58:44<05:10, 17.24s/it]\u001B[AWARNING:root:film tt1247692\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143F1137520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143F1137520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143F1137520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143F1137520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  93%|█████████▎| 233/250 [58:54<04:16, 15.08s/it]\u001B[A\n",
      "films on page 15:  94%|█████████▎| 234/250 [59:13<04:21, 16.36s/it]\u001B[AWARNING:root:film tt1883180\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143F1161FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143F1161FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143F1161FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143F1161FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  94%|█████████▍| 235/250 [59:27<03:55, 15.69s/it]\u001B[A\n",
      "films on page 15:  94%|█████████▍| 236/250 [59:46<03:54, 16.74s/it]\u001B[A\n",
      "films on page 15:  95%|█████████▍| 237/250 [1:00:01<03:28, 16.06s/it]\u001B[A\n",
      "films on page 15:  95%|█████████▌| 238/250 [1:00:15<03:06, 15.58s/it]\u001B[A\n",
      "films on page 15:  96%|█████████▌| 239/250 [1:00:33<02:57, 16.10s/it]\u001B[AWARNING:root:film tt1981107\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143F11DCFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143F11DCFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143F11DCFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143F11DCFD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  96%|█████████▌| 240/250 [1:00:44<02:28, 14.85s/it]\u001B[A\n",
      "films on page 15:  96%|█████████▋| 241/250 [1:01:01<02:17, 15.26s/it]\u001B[AWARNING:root:film tt1705786\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143F1201520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143F1201520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143F1201520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143F1201520>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  97%|█████████▋| 242/250 [1:01:08<01:43, 12.92s/it]\u001B[A\n",
      "films on page 15:  97%|█████████▋| 243/250 [1:01:24<01:37, 13.92s/it]\u001B[A\n",
      "films on page 15:  98%|█████████▊| 244/250 [1:01:40<01:26, 14.47s/it]\u001B[AWARNING:root:film tt0369060\n",
      "ERROR:root:HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143F1249FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\socket.py\", line 954, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000143F1249FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143F1249FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 78, in <module>\n",
      "    result_reviews = get_reviews_results(title_id)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 47, in get_reviews_results\n",
      "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
      "  File \"C:\\Users\\petro\\AppData\\Local\\Temp\\ipykernel_13848\\2469378305.py\", line 39, in get_reviews_results\n",
      "    result = requests.get(url)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\petro\\miniconda3\\envs\\web_crawling\\lib\\site-packages\\requests\\adapters.py\", line 520, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='imdb-api.tprojects.workers.devnone', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000143F1249FD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "films on page 15:  98%|█████████▊| 245/250 [1:01:53<01:10, 14.02s/it]\u001B[A\n",
      "films on page 15:  98%|█████████▊| 246/250 [1:02:11<01:00, 15.25s/it]\u001B[A\n",
      "films on page 15:  99%|█████████▉| 247/250 [1:02:25<00:44, 14.81s/it]\u001B[A\n",
      "films on page 15:  99%|█████████▉| 248/250 [1:02:43<00:31, 15.76s/it]\u001B[A\n",
      "films on page 15: 100%|█████████▉| 249/250 [1:02:59<00:15, 15.81s/it]\u001B[A\n",
      "films on page 15: 100%|██████████| 250/250 [1:03:14<00:00, 15.18s/it]\u001B[A\n",
      "pages: 100%|██████████| 10/10 [11:19:37<00:00, 4077.75s/it] \n"
     ]
    }
   ],
   "source": [
    "def wait():\n",
    "    sleep(randint(1, 3))\n",
    "\n",
    "def get_page_results(page_num):\n",
    "    count_per_page = 250  # [50, 100, 250]\n",
    "    starting_position = 1 + (page_num - 1) * count_per_page\n",
    "    url = f'https://www.imdb.com/search/title/?title_type=feature&release_date=2000-01-01,&user_rating=6.0,&adult=include&start={starting_position}&ref_=adv_nxt&sort=num_votes,desc&count={count_per_page}'\n",
    "\n",
    "    result_page = requests.get(url, headers=HEADERS)\n",
    "    # wait()\n",
    "\n",
    "    if result_page.status_code != 200:\n",
    "        logging.warn(f'Results at {starting_position} (url: {url}): code {result_page.status_code}')\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(result_page.text)\n",
    "    page_results = soup.find_all(class_='lister-item mode-advanced')\n",
    "    return page_results\n",
    "\n",
    "def get_reviews_results(title_id, reviews_results=None, url=None, page_num=0):\n",
    "    if page_num > 3:\n",
    "        return reviews_results\n",
    "\n",
    "    if page_num == 0:\n",
    "        reviews_results = []\n",
    "        url = f'{API_URL}/reviews/{title_id}?option=helpfulness&sortOrder=desc'\n",
    "\n",
    "    result = requests.get(url)\n",
    "    if result.status_code != 200:\n",
    "        logging.warn(f'Results of the reviews page {page_num}, film {title_id}: code {result.status_code}')\n",
    "        return reviews_results\n",
    "    wait()\n",
    "    data = result.json()\n",
    "    reviews_results += data['reviews']\n",
    "    next_url = f'{API_URL}{data[\"next_api_path\"]}'\n",
    "    reviews_results = get_reviews_results(title_id, reviews_results, next_url, page_num + 1)\n",
    "    return reviews_results\n",
    "###\n",
    "\n",
    "films_data = []\n",
    "reviews_data = []\n",
    "films_list = []\n",
    "i = 0\n",
    "\n",
    "for page_num in tqdm(PAGES_RANGE, 'pages'):\n",
    "    page_results = get_page_results(page_num)\n",
    "    for page_result in tqdm(page_results, f'films on page {page_num}'):\n",
    "        i += 1\n",
    "        try:\n",
    "            # title_id = page_result.find('a').get('href')[7:-1]\n",
    "            is_match = re.search(IMDB_TITLE_PATTERN, page_result.find('a').get('href'))\n",
    "            title_id = is_match.group(1)\n",
    "\n",
    "            # film_url = f'https://www.imdb.com/title/{title_id}/'\n",
    "            # result_film = requests.get(film_url, headers=HEADERS)\n",
    "            result_film = requests.get(API_URL + f'/title/{title_id}')\n",
    "            if result_film.status_code != 200:\n",
    "                logging.warn(f'Results of the film {title_id}: code {result_film.status_code}')\n",
    "                continue\n",
    "            data = result_film.json()\n",
    "            if 'releaseDeatiled' in data:\n",
    "                del data['releaseDeatiled']\n",
    "            films_data.append(data)\n",
    "            # print(result_film.status_code)\n",
    "            # print(result_film.json())\n",
    "            wait()\n",
    "            result_reviews = get_reviews_results(title_id)\n",
    "            reviews_data += result_reviews\n",
    "        except Exception as e:\n",
    "            logging.warn(f'film {title_id}')\n",
    "            logging.exception(e)\n",
    "#         break\n",
    "#     break\n",
    "        if i % 100 == 0:\n",
    "            with open(SAVE_FILMS_JSON, 'w') as f:\n",
    "                json.dump(films_data, f)\n",
    "            with open(SAVE_REVIEWS_JSON, 'w') as f:\n",
    "                json.dump(reviews_data, f)\n",
    "    \n",
    "with open(SAVE_FILMS_JSON, 'w') as f:\n",
    "    json.dump(films_data, f)\n",
    "with open(SAVE_REVIEWS_JSON, 'w') as f:\n",
    "    json.dump(reviews_data, f)\n",
    "\n",
    "df_films = pd.json_normalize(films_data)\n",
    "df_reviews = pd.json_normalize(reviews_data)\n",
    "df_films.to_csv(SAVE_FILMS_CSV)\n",
    "df_reviews.to_csv(SAVE_REVIEWS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Dd2fcn4u6n1R"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review_api_path</th>\n",
       "      <th>imdb</th>\n",
       "      <th>contentType</th>\n",
       "      <th>productionStatus</th>\n",
       "      <th>title</th>\n",
       "      <th>image</th>\n",
       "      <th>images</th>\n",
       "      <th>plot</th>\n",
       "      <th>contentRating</th>\n",
       "      <th>...</th>\n",
       "      <th>rating.count</th>\n",
       "      <th>rating.star</th>\n",
       "      <th>award.wins</th>\n",
       "      <th>award.nominations</th>\n",
       "      <th>releaseDetailed.day</th>\n",
       "      <th>releaseDetailed.month</th>\n",
       "      <th>releaseDetailed.year</th>\n",
       "      <th>releaseDetailed.releaseLocation.country</th>\n",
       "      <th>releaseDetailed.releaseLocation.cca2</th>\n",
       "      <th>releaseDetailed.originLocations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0468569</td>\n",
       "      <td>/reviews/tt0468569</td>\n",
       "      <td>https://www.imdb.com/title/tt0468569</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Released</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BMTMxNT...</td>\n",
       "      <td>['https://m.media-amazon.com/images/M/MV5BOTAx...</td>\n",
       "      <td>When the menace known as the Joker wreaks havo...</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>...</td>\n",
       "      <td>2713440</td>\n",
       "      <td>9.0</td>\n",
       "      <td>162</td>\n",
       "      <td>163</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2008</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>[{'country': 'United States', 'cca2': 'US'}, {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt1375666</td>\n",
       "      <td>/reviews/tt1375666</td>\n",
       "      <td>https://www.imdb.com/title/tt1375666</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Released</td>\n",
       "      <td>Inception</td>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BMjAxMz...</td>\n",
       "      <td>['https://m.media-amazon.com/images/M/MV5BMjIy...</td>\n",
       "      <td>A thief who steals corporate secrets through t...</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>...</td>\n",
       "      <td>2408658</td>\n",
       "      <td>8.8</td>\n",
       "      <td>159</td>\n",
       "      <td>220</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2010</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>[{'country': 'United States', 'cca2': 'US'}, {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0120737</td>\n",
       "      <td>/reviews/tt0120737</td>\n",
       "      <td>https://www.imdb.com/title/tt0120737</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Released</td>\n",
       "      <td>The Lord of the Rings: The Fellowship of the Ring</td>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BN2EyZj...</td>\n",
       "      <td>['https://m.media-amazon.com/images/M/MV5BMjQ4...</td>\n",
       "      <td>A meek Hobbit from the Shire and eight compani...</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>...</td>\n",
       "      <td>1912955</td>\n",
       "      <td>8.8</td>\n",
       "      <td>123</td>\n",
       "      <td>127</td>\n",
       "      <td>19.0</td>\n",
       "      <td>12</td>\n",
       "      <td>2001</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>[{'country': 'New Zealand', 'cca2': 'NZ'}, {'c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     review_api_path                                  imdb  \\\n",
       "0  tt0468569  /reviews/tt0468569  https://www.imdb.com/title/tt0468569   \n",
       "1  tt1375666  /reviews/tt1375666  https://www.imdb.com/title/tt1375666   \n",
       "2  tt0120737  /reviews/tt0120737  https://www.imdb.com/title/tt0120737   \n",
       "\n",
       "  contentType productionStatus  \\\n",
       "0       Movie         Released   \n",
       "1       Movie         Released   \n",
       "2       Movie         Released   \n",
       "\n",
       "                                               title  \\\n",
       "0                                    The Dark Knight   \n",
       "1                                          Inception   \n",
       "2  The Lord of the Rings: The Fellowship of the Ring   \n",
       "\n",
       "                                               image  \\\n",
       "0  https://m.media-amazon.com/images/M/MV5BMTMxNT...   \n",
       "1  https://m.media-amazon.com/images/M/MV5BMjAxMz...   \n",
       "2  https://m.media-amazon.com/images/M/MV5BN2EyZj...   \n",
       "\n",
       "                                              images  \\\n",
       "0  ['https://m.media-amazon.com/images/M/MV5BOTAx...   \n",
       "1  ['https://m.media-amazon.com/images/M/MV5BMjIy...   \n",
       "2  ['https://m.media-amazon.com/images/M/MV5BMjQ4...   \n",
       "\n",
       "                                                plot contentRating  ...  \\\n",
       "0  When the menace known as the Joker wreaks havo...         PG-13  ...   \n",
       "1  A thief who steals corporate secrets through t...         PG-13  ...   \n",
       "2  A meek Hobbit from the Shire and eight compani...         PG-13  ...   \n",
       "\n",
       "  rating.count  rating.star award.wins award.nominations releaseDetailed.day  \\\n",
       "0      2713440          9.0        162               163                18.0   \n",
       "1      2408658          8.8        159               220                16.0   \n",
       "2      1912955          8.8        123               127                19.0   \n",
       "\n",
       "   releaseDetailed.month releaseDetailed.year  \\\n",
       "0                      7                 2008   \n",
       "1                      7                 2010   \n",
       "2                     12                 2001   \n",
       "\n",
       "  releaseDetailed.releaseLocation.country  \\\n",
       "0                           United States   \n",
       "1                           United States   \n",
       "2                           United States   \n",
       "\n",
       "  releaseDetailed.releaseLocation.cca2  \\\n",
       "0                                   US   \n",
       "1                                   US   \n",
       "2                                   US   \n",
       "\n",
       "                     releaseDetailed.originLocations  \n",
       "0  [{'country': 'United States', 'cca2': 'US'}, {...  \n",
       "1  [{'country': 'United States', 'cca2': 'US'}, {...  \n",
       "2  [{'country': 'New Zealand', 'cca2': 'NZ'}, {'c...  \n",
       "\n",
       "[3 rows x 29 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "films_files = [\n",
    "    'films_data.csv',\n",
    "    'films_data_2.csv'\n",
    "]\n",
    "films_files = [SAVE_DIRECTORY.joinpath(file) for file in films_files]\n",
    "\n",
    "df_films = pd.concat(map(pd.read_csv, films_files)).drop(columns='Unnamed: 0')\n",
    "df_films.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3744 entries, 0 to 2495\n",
      "Data columns (total 29 columns):\n",
      " #   Column                                   Non-Null Count  Dtype  \n",
      "---  ------                                   --------------  -----  \n",
      " 0   id                                       3744 non-null   object \n",
      " 1   review_api_path                          3744 non-null   object \n",
      " 2   imdb                                     3744 non-null   object \n",
      " 3   contentType                              3744 non-null   object \n",
      " 4   productionStatus                         3744 non-null   object \n",
      " 5   title                                    3744 non-null   object \n",
      " 6   image                                    3744 non-null   object \n",
      " 7   images                                   3744 non-null   object \n",
      " 8   plot                                     3744 non-null   object \n",
      " 9   contentRating                            3643 non-null   object \n",
      " 10  genre                                    3744 non-null   object \n",
      " 11  year                                     3744 non-null   int64  \n",
      " 12  spokenLanguages                          3744 non-null   object \n",
      " 13  filmingLocations                         3744 non-null   object \n",
      " 14  runtime                                  3744 non-null   object \n",
      " 15  runtimeSeconds                           3744 non-null   int64  \n",
      " 16  actors                                   3744 non-null   object \n",
      " 17  directors                                3744 non-null   object \n",
      " 18  top_credits                              3744 non-null   object \n",
      " 19  rating.count                             3744 non-null   int64  \n",
      " 20  rating.star                              3744 non-null   float64\n",
      " 21  award.wins                               3744 non-null   int64  \n",
      " 22  award.nominations                        3744 non-null   int64  \n",
      " 23  releaseDetailed.day                      3743 non-null   float64\n",
      " 24  releaseDetailed.month                    3744 non-null   int64  \n",
      " 25  releaseDetailed.year                     3744 non-null   int64  \n",
      " 26  releaseDetailed.releaseLocation.country  3744 non-null   object \n",
      " 27  releaseDetailed.releaseLocation.cca2     3744 non-null   object \n",
      " 28  releaseDetailed.originLocations          3744 non-null   object \n",
      "dtypes: float64(2), int64(7), object(20)\n",
      "memory usage: 877.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_films.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'PH', 'FR', 'MX', 'KR', 'IN', 'DK', 'HU', 'GB', 'IR', 'CN',\n",
       "       'AR', 'BR', 'JP', 'ES', 'KZ', 'DE', 'BE', 'GR', 'PL', 'CA', 'HK',\n",
       "       'TR', 'NO', 'HR', 'PT', 'IE', 'LV', 'LB', 'NL', 'IL', 'RU', 'AU',\n",
       "       'IS', 'SE', 'IT', 'RO', 'BG', 'AE', 'ZA', 'MY', 'ID', 'TH', 'FI',\n",
       "       'NZ', 'SG', 'TW', 'LT', 'AT', 'PK', 'EG', 'BD', 'CZ', 'CL', 'EE',\n",
       "       'PR', 'BH', 'CO', 'UA', 'BY'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_films[\"releaseDetailed.releaseLocation.cca2\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['United States', 'Philippines', 'France', 'Mexico', 'South Korea',\n",
       "       'India', 'Denmark', 'Hungary', 'United Kingdom', 'Iran', 'China',\n",
       "       'Argentina', 'Brazil', 'Japan', 'Spain', 'Kazakhstan', 'Germany',\n",
       "       'Belgium', 'Greece', 'Poland', 'Canada', 'Hong Kong', 'Turkey',\n",
       "       'Norway', 'Croatia', 'Portugal', 'Ireland', 'Latvia', 'Lebanon',\n",
       "       'Netherlands', 'Israel', 'Russia', 'Australia', 'Iceland',\n",
       "       'Sweden', 'Italy', 'Romania', 'Bulgaria', 'United Arab Emirates',\n",
       "       'South Africa', 'Malaysia', 'Indonesia', 'Thailand', 'Finland',\n",
       "       'New Zealand', 'Singapore', 'Taiwan', 'Lithuania', 'Austria',\n",
       "       'Pakistan', 'Egypt', 'Bangladesh', 'Czech Republic', 'Chile',\n",
       "       'Estonia', 'Puerto Rico', 'Bahrain', 'Colombia', 'Ukraine',\n",
       "       'Belarus'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_films[\"releaseDetailed.releaseLocation.country\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>authorUrl</th>\n",
       "      <th>user_api_path</th>\n",
       "      <th>date</th>\n",
       "      <th>stars</th>\n",
       "      <th>heading</th>\n",
       "      <th>content</th>\n",
       "      <th>reviewLink</th>\n",
       "      <th>helpfulNess.votes</th>\n",
       "      <th>helpfulNess.votedAsHelpful</th>\n",
       "      <th>helpfulNess.votedAsHelpfulPercentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rw5478826</td>\n",
       "      <td>MrHeraclius</td>\n",
       "      <td>https://www.imdb.com/user/ur87850731/?ref_=tt_urv</td>\n",
       "      <td>/user/ur87850731</td>\n",
       "      <td>2020-02-12T00:00:00.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>Confidently directed, dark, brooding, and pack...</td>\n",
       "      <td>https://www.imdb.com/review/rw5478826</td>\n",
       "      <td>585</td>\n",
       "      <td>537</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rw1914442</td>\n",
       "      <td>Smells_Like_Cheese</td>\n",
       "      <td>https://www.imdb.com/user/ur1293485/?ref_=tt_urv</td>\n",
       "      <td>/user/ur1293485</td>\n",
       "      <td>2008-07-20T00:00:00.000Z</td>\n",
       "      <td>10</td>\n",
       "      <td>The Batman of our dreams! So much more than a ...</td>\n",
       "      <td>I got to see The Dark Knight on Wednesday nigh...</td>\n",
       "      <td>https://www.imdb.com/review/rw1914442</td>\n",
       "      <td>1</td>\n",
       "      <td>933</td>\n",
       "      <td>93300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rw6606026</td>\n",
       "      <td>dseferaj</td>\n",
       "      <td>https://www.imdb.com/user/ur129557514/?ref_=tt...</td>\n",
       "      <td>/user/ur129557514</td>\n",
       "      <td>2021-02-17T00:00:00.000Z</td>\n",
       "      <td>10</td>\n",
       "      <td>This town deserves a better class of criminal!</td>\n",
       "      <td>This movie is a work of art. The finest sequel...</td>\n",
       "      <td>https://www.imdb.com/review/rw6606026</td>\n",
       "      <td>187</td>\n",
       "      <td>171</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id              author  \\\n",
       "0  rw5478826         MrHeraclius   \n",
       "1  rw1914442  Smells_Like_Cheese   \n",
       "2  rw6606026            dseferaj   \n",
       "\n",
       "                                           authorUrl      user_api_path  \\\n",
       "0  https://www.imdb.com/user/ur87850731/?ref_=tt_urv   /user/ur87850731   \n",
       "1   https://www.imdb.com/user/ur1293485/?ref_=tt_urv    /user/ur1293485   \n",
       "2  https://www.imdb.com/user/ur129557514/?ref_=tt...  /user/ur129557514   \n",
       "\n",
       "                       date  stars  \\\n",
       "0  2020-02-12T00:00:00.000Z      0   \n",
       "1  2008-07-20T00:00:00.000Z     10   \n",
       "2  2021-02-17T00:00:00.000Z     10   \n",
       "\n",
       "                                             heading  \\\n",
       "0                                    The Dark Knight   \n",
       "1  The Batman of our dreams! So much more than a ...   \n",
       "2     This town deserves a better class of criminal!   \n",
       "\n",
       "                                             content  \\\n",
       "0  Confidently directed, dark, brooding, and pack...   \n",
       "1  I got to see The Dark Knight on Wednesday nigh...   \n",
       "2  This movie is a work of art. The finest sequel...   \n",
       "\n",
       "                              reviewLink  helpfulNess.votes  \\\n",
       "0  https://www.imdb.com/review/rw5478826                585   \n",
       "1  https://www.imdb.com/review/rw1914442                  1   \n",
       "2  https://www.imdb.com/review/rw6606026                187   \n",
       "\n",
       "   helpfulNess.votedAsHelpful  helpfulNess.votedAsHelpfulPercentage  \n",
       "0                         537                                    92  \n",
       "1                         933                                 93300  \n",
       "2                         171                                    91  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_files = [\n",
    "    'reviews_data.csv',\n",
    "    'reviews_data_2.csv'\n",
    "]\n",
    "reviews_files = [SAVE_DIRECTORY.joinpath(file) for file in reviews_files]\n",
    "\n",
    "df_reviews = pd.concat(map(pd.read_csv, reviews_files)).drop(columns='Unnamed: 0')\n",
    "df_reviews.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 352164 entries, 0 to 227720\n",
      "Data columns (total 12 columns):\n",
      " #   Column                                Non-Null Count   Dtype \n",
      "---  ------                                --------------   ----- \n",
      " 0   id                                    352164 non-null  object\n",
      " 1   author                                352164 non-null  object\n",
      " 2   authorUrl                             352164 non-null  object\n",
      " 3   user_api_path                         352164 non-null  object\n",
      " 4   date                                  352164 non-null  object\n",
      " 5   stars                                 352164 non-null  int64 \n",
      " 6   heading                               352161 non-null  object\n",
      " 7   content                               352164 non-null  object\n",
      " 8   reviewLink                            352164 non-null  object\n",
      " 9   helpfulNess.votes                     352164 non-null  int64 \n",
      " 10  helpfulNess.votedAsHelpful            352164 non-null  int64 \n",
      " 11  helpfulNess.votedAsHelpfulPercentage  352164 non-null  int64 \n",
      "dtypes: int64(4), object(8)\n",
      "memory usage: 34.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_films.to_csv(SAVE_DIRECTORY.joinpath('imdb_films.csv'))\n",
    "df_reviews.to_csv(SAVE_DIRECTORY.joinpath('imdb_reviews.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
